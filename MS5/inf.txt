│   .env
│   [
│       # Django's main secret key
│       DJANGO_SECRET_KEY='django-insecure-m3x$8o#H45Ysdverg56564ldpcuck6bytc4h1*8v!=8(_wau6g8or'
│       JWT_SECRET_KEY ='jwt-secure-m3x$DFGRTJRTYNEHRETNEFDDHD43.m<?><DFGRTJYRJGc4h1*8v!=8(_wau6g8or'
│       # You can also add other environment-specific settings here
│       DJANGO_DEBUG='True'
│       DATABASE_URL='sqlite:///./db.sqlite3' # Example for database config
│       JWT_ISSUER="https://ms1.auth-service.com"
│       RABBITMQ_URL='amqp://guest:guest@localhost:5672/'
│       
│       
│       NODE_SERVICE_GRPC_URL=localhost:50051
│       
│       # The address for the Model Service's gRPC server
│       MODEL_SERVICE_GRPC_URL=localhost:50052
│   ]
├───MS5
│   __init__.py
│   [
│       
│   ]
│   asgi.py
│   [
│       """
│       ASGI config for MS5 project.
│       
│       It exposes the ASGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
│       """
│       
│       import os
│       
│       from django.core.asgi import get_asgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS5.settings')
│       
│       application = get_asgi_application()
│       
│   ]
│   settings.py
│   [
│       
│       from datetime import timedelta
│       import os
│       from pathlib import Path
│       from datetime import timedelta
│       # Build paths inside the project like this: BASE_DIR / 'subdir'.
│       BASE_DIR = Path(__file__).resolve().parent.parent
│       
│       from dotenv import load_dotenv
│       load_dotenv(BASE_DIR / '.env')
│       # Quick-start development settings - unsuitable for production
│       # See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/
│       
│       SECRET_KEY = os.getenv('DJANGO_SECRET_KEY')
│       if not SECRET_KEY:
│           # This fallback should ideally not be hit if .env is loaded correctly
│           # or if the environment variable is set directly in the deployment environment.
│           SECRET_KEY = 'django-insecure-fallback-dev-key-!!change-me!!'
│           print("WARNING: DJANGO_SECRET_KEY not found in environment or .env. Using fallback. THIS IS INSECURE FOR PRODUCTION.")
│       
│       DEBUG = os.getenv('DJANGO_DEBUG', 'True').lower() in ('true', '1', 't')
│       
│       ALLOWED_HOSTS = ['*']
│       
│       
│       # Application definition
│       
│       INSTALLED_APPS = [
│       
│           'django.contrib.admin',
│           'django.contrib.auth',
│           'django.contrib.contenttypes',
│           'django.contrib.sessions',
│           'django.contrib.messages',
│           'django.contrib.staticfiles',
│           'rest_framework',
│           'rest_framework_simplejwt',
│           'inference_engine',
│           'inference_internals',
│           'messaging',
│       
│       ]
│       
│       MIDDLEWARE = [
│           'django.middleware.security.SecurityMiddleware',
│           'django.contrib.sessions.middleware.SessionMiddleware',
│           'django.middleware.common.CommonMiddleware',
│           'django.middleware.csrf.CsrfViewMiddleware',
│           'django.contrib.auth.middleware.AuthenticationMiddleware',
│           'django.contrib.messages.middleware.MessageMiddleware',
│           'django.middleware.clickjacking.XFrameOptionsMiddleware',
│       ]
│       
│       STATIC_URL = '/static/'
│       STATIC_ROOT = BASE_DIR / 'staticfiles'
│       
│       
│       ROOT_URLCONF = 'MS5.urls'
│       
│       TEMPLATES = [
│           {
│               'BACKEND': 'django.template.backends.django.DjangoTemplates',
│               'DIRS': [],
│               'APP_DIRS': True,
│               'OPTIONS': {
│                   'context_processors': [
│                       'django.template.context_processors.request',
│                       'django.contrib.auth.context_processors.auth',
│                       'django.contrib.messages.context_processors.messages',
│                   ],
│               },
│           },
│       ]
│       
│       WSGI_APPLICATION = 'MS5.wsgi.application'
│       
│       
│       # Database
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#databases
│       
│       DATABASES = {
│           'default': {
│               'ENGINE': 'django.db.backends.sqlite3',
│               'NAME': BASE_DIR / 'db.sqlite3',
│           }
│       }
│       
│       
│       # Password validation
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators
│       
│       AUTH_PASSWORD_VALIDATORS = [
│           {
│               'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
│           },
│       ]
│       
│       
│       # Internationalization
│       # https://docs.djangoproject.com/en/5.2/topics/i18n/
│       
│       LANGUAGE_CODE = 'en-us'
│       
│       TIME_ZONE = 'UTC'
│       
│       USE_I18N = True
│       
│       USE_TZ = True
│       
│       
│       
│       DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
│       
│       
│       # Media files
│       MEDIA_URL = '/media/'
│       MEDIA_ROOT = BASE_DIR / 'media'
│       
│       
│       
│       
│       # REST Framework
│       JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY')
│       
│       REST_FRAMEWORK = {
│           "DEFAULT_PERMISSION_CLASSES": ["rest_framework.permissions.IsAuthenticated"],
│           "DEFAULT_AUTHENTICATION_CLASSES": (
│                    
│               "inference_engine.custom_auth.ForceTokenUserJWTAuthentication", # <<< YOUR CUSTOM AUTH CLASS
│           ),
│           'DEFAULT_THROTTLE_CLASSES': (
│               'rest_framework.throttling.AnonRateThrottle',
│               'rest_framework.throttling.UserRateThrottle'
│           ),
│           'DEFAULT_THROTTLE_RATES': {
│               'anon': '100/day',  # Adjust as needed for unauthenticated requests
│               'user': '20000/day' # Adjust as needed for authenticated requests
│           }
│       }
│       
│       SIMPLE_JWT = {
│       
│           "SIGNING_KEY": JWT_SECRET_KEY,  # <<< USE DJANGO'S SECRET_KEY LOADED FROM ENV
│           "VERIFYING_KEY": JWT_SECRET_KEY,
│           "ISSUER": os.getenv('JWT_ISSUER', "https://ms1.auth-service.com"), # MUST match MS1's issuer
│           "AUTH_HEADER_TYPES": ("Bearer",),
│           "ACCESS_TOKEN_LIFETIME": timedelta(minutes=60), # e.g., 1 hour
│           "REFRESH_TOKEN_LIFETIME": timedelta(days=1),    # e.g., 1 day
│           "LEEWAY": timedelta(seconds=10),
│           "ALGORITHM": "HS256",
│           
│           # --- Settings related to interpreting the token payload ---
│           """
│       "USER_ID_CLAIM": "user_id": (Your Specific Question)
│        This is a critical instruction. It tells simple-jwt:
│          "When you parse the token's payload (the data inside),
│            the claim that contains the user's primary identifier is named 'user_id'."
│              Your MS1's CustomTokenObtainPairSerializer probably adds a claim with this name.
│           """
│       
│           "USER_ID_CLAIM": "user_id",
│       
│           "USER_ID_FIELD": "id",
│           "TOKEN_USER_CLASS": "rest_framework_simplejwt.models.TokenUser", # Explicitly use TokenUse
│       
│           # --- Settings for features MS2 likely DOES NOT use ---
│           "UPDATE_LAST_LOGIN": False,
│           "ROTATE_REFRESH_TOKENS": False,
│           "BLACKLIST_AFTER_ROTATION": False, 
│       
│       }
│       
│       
│       RABBITMQ_URL = os.getenv('RABBITMQ_URL', 'amqp://guest:guest@localhost:5672/')
│       
│       
│       
│   ]
│   urls.py
│   [
│       """
│       Root URL configuration for the MS5 Inference Service project.
│       """
│       from django.contrib import admin
│       from django.urls import path, include
│       
│       urlpatterns = [
│           # Main entry point for all Inference Service API calls.
│           # It delegates all requests starting with '/ms5/api/v1/' to the 'inference_engine' app.
│           path('ms5/api/v1/', include('inference_engine.api_urls')),
│       
│           # Optional: Include the Django admin interface for debugging and management.
│           # This will be accessible at '/ms5/admin/' if you configure an admin user.
│           path('ms5/admin/', admin.site.urls),
│       ]
│       
│       
│       
│   ]
│   views.py
│   [
│       from django.contrib.auth.decorators import login_required
│       from django.http import HttpResponse
│       from django.views.static import serve
│       from django.conf import settings
│       
│       # Protected media view
│       def protected_media_view(request, path):
│           if not request.user.is_authenticated:
│               return HttpResponse('Unauthorized', status=401)
│           # Add additional permission checks here if needed
│           return serve(request, path, document_root=settings.MEDIA_ROOT)
│       
│   ]
│   wsgi.py
│   [
│       """
│       WSGI config for MS5 project.
│       
│       It exposes the WSGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
│       """
│       
│       import os
│       
│       from django.core.wsgi import get_wsgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS5.settings')
│       
│       application = get_wsgi_application()
│       
│   ]
│   db.sqlite3
│   [
│       
│   ]
├───inference_engine
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       
│       # Register your models here.
│       
│   ]
│   api_urls.py
│   [
│       from django.urls import path
│       from .views import InferenceAPIView
│       urlpatterns = [
│           path('nodes/<uuid:node_id>/infer/', InferenceAPIView.as_view(), name='node-infer'),
│       ]
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class InferenceEngineConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'inference_engine'
│       
│   ]
│   custom_auth.py
│   [
│       # MS2/products/custom_auth.py
│       from rest_framework_simplejwt.authentication import JWTAuthentication
│       from rest_framework_simplejwt.models import TokenUser # Import TokenUser
│       from rest_framework_simplejwt.settings import api_settings as simple_jwt_settings
│       from django.utils.translation import gettext_lazy as _
│       from rest_framework_simplejwt.exceptions import InvalidToken
│       
│       class ForceTokenUserJWTAuthentication(JWTAuthentication):
│           def get_user(self, validated_token):
│               """
│               Returns a TokenUser instance based on the validated token.
│               Bypasses any local database User lookup for JWT authentication.
│               """
│               try:
│                   # simple_jwt_settings.USER_ID_CLAIM refers to what you set in settings.py
│                   # e.g., "user_id"
│                   user_id = validated_token[simple_jwt_settings.USER_ID_CLAIM]
│               except KeyError:
│                   raise InvalidToken(_("Token contained no recognizable user identification"))
│       
│               # Correct way to instantiate TokenUser: pass the validated_token
│               # The TokenUser class will internally use USER_ID_CLAIM and USER_ID_FIELD
│               # from your SIMPLE_JWT settings to extract the user ID and set its 'id' or 'pk'.
│               token_user = TokenUser(validated_token)
│       
│               # The TokenUser's 'id' (and 'pk') attribute should now be populated correctly
│               # by its own __init__ method based on the validated_token and your SIMPLE_JWT settings
│               # for USER_ID_CLAIM and USER_ID_FIELD.
│       
│               # Example: If you wanted to verify or access it (not strictly necessary here)
│               # print(f"TokenUser ID: {token_user.id}, TokenUser PK: {token_user.pk}")
│       
│               return token_user
│   ]
│   models.py
│   [
│       from django.db import models
│       
│       # Create your models here.
│       
│   ]
│   serializers.py
│   [
│       from rest_framework import serializers
│       class InferenceRequestSerializer(serializers.Serializer):
│           prompt = serializers.CharField(required=True)
│   ]
│   services.py
│   [
│       from .strategies import get_strategy
│       from inference_internals.clients import NodeServiceClient
│       from messaging.event_publisher import inference_job_publisher
│       # Need to import uuid and datetime
│       import uuid
│       from datetime import datetime
│       
│       class InferenceOrchestrationService:
│           def process_inference_request(self, node_id: str, user_id: str, query_data: dict):
│               # 1. Authorize and fetch the node's configuration blueprint
│               node_client = NodeServiceClient()
│               node_details = node_client.get_node_details(node_id, user_id)
│       
│               # 2. Select the appropriate resource collection strategy
│               StrategyClass = get_strategy(node_details)
│               strategy = StrategyClass(user_id, node_details)
│       
│               # 3. Execute the strategy to gather all required resources in parallel
│               collected_resources = strategy.collect_resources()
│       
│               # 4. Assemble the final, self-contained job payload
│               job_payload = {
│                   "job_id": str(uuid.uuid4()), # Generate a unique ID for this job
│                   "user_id": user_id,
│                   "node": node_details,
│                   "query": query_data,
│                   "resources": collected_resources,
│                   "timestamp": datetime.utcnow().isoformat()
│               }
│               
│               # 5. Publish the job to the queue
│               inference_job_publisher.publish_job(job_payload)
│       
│               return {"job_id": job_payload["job_id"], "status": "Job submitted successfully."}
│       
│       
│   ]
│   ├───strategies
│   │   __init__.py
│   │   [
│   │       from .base_strategy import BaseCollectionStrategy
│   │       from .llm_strategy import LLMCollectionStrategy
│   │       
│   │       def get_strategy(node_config: dict) -> BaseCollectionStrategy:
│   │           """Factory function to select the appropriate collection strategy."""
│   │           node_type = node_config.get("configuration", {}).get("node_type")
│   │       
│   │           if node_type in ["ai_agent", "llm_chat"]:
│   │               return LLMCollectionStrategy
│   │           # elif node_type == "image_diffusion":
│   │           #     return DiffusionCollectionStrategy
│   │           else:
│   │               raise ValueError(f"No collection strategy found for node type: {node_type}")
│   │           
│   │       
│   │       
│   │   ]
│   │   base_strategy.py
│   │   [
│   │       from abc import ABC, abstractmethod
│   │       
│   │       class BaseCollectionStrategy(ABC):
│   │           """Abstract base class for a resource collection strategy."""
│   │           def __init__(self, user_id: str, node_config: dict):
│   │               self.user_id = user_id
│   │               self.node_config = node_config
│   │       
│   │           @abstractmethod
│   │           def collect_resources(self) -> dict:
│   │               """
│   │               Collects all necessary resources for an inference job.
│   │               This method must be implemented by all concrete strategies.
│   │               Returns a dictionary of the collected resources.
│   │               """
│   │               pass
│   │   ]
│   │   llm_strategy.py
│   │   [
│   │       import concurrent.futures
│   │       from .base_strategy import BaseCollectionStrategy
│   │       from inference_internals.clients import ModelServiceClient
│   │       # Import other clients as they are created (e.g., MemoryServiceClient)
│   │       
│   │       class LLMCollectionStrategy(BaseCollectionStrategy):
│   │           """
│   │           Strategy for collecting resources for a standard LLM inference job.
│   │           It gathers model configuration and, if enabled, memory and RAG context.
│   │           """
│   │           def collect_resources(self) -> dict:
│   │               model_config_data = {}
│   │               # Placeholders for other resources
│   │               memory_context = None
│   │               rag_documents = None
│   │       
│   │               with concurrent.futures.ThreadPoolExecutor() as executor:
│   │                   future_to_resource = {}
│   │                   
│   │                   # --- Submit Model Config Task ---
│   │                   model_id = self.node_config.get("configuration", {}).get("model_config", {}).get("model_id")
│   │                   if not model_id:
│   │                       raise ValueError("Node configuration is missing a model_id.")
│   │                   
│   │                   model_client = ModelServiceClient()
│   │                   future_model = executor.submit(model_client.get_model_configuration, model_id, self.user_id)
│   │                   future_to_resource[future_model] = "model_config"
│   │       
│   │                   # --- Submit Memory Task (if enabled) ---
│   │                   # memory_config = self.node_config.get("configuration", {}).get("memory_config", {})
│   │                   # if memory_config.get("is_enabled"):
│   │                   #     memory_client = MemoryServiceClient()
│   │                   #     future_memory = executor.submit(memory_client.get_history, memory_config['bucket_id'], self.user_id)
│   │                   #     future_to_resource[future_memory] = "memory"
│   │       
│   │                   # --- Process results as they complete ---
│   │                   for future in concurrent.futures.as_completed(future_to_resource):
│   │                       resource_name = future_to_resource[future]
│   │                       try:
│   │                           result = future.result()
│   │                           if resource_name == "model_config":
│   │                               model_config_data = result
│   │                           elif resource_name == "memory":
│   │                               memory_context = result
│   │                           # ... handle other resources
│   │                       except Exception as exc:
│   │                           print(f'Resource collection for "{resource_name}" failed: {exc}')
│   │                           raise # Re-raise the exception to fail the entire request
│   │       
│   │               return {
│   │                   "model_config": model_config_data,
│   │                   "memory_context": memory_context,
│   │                   "rag_documents": rag_documents,
│   │               }
│   │   ]
│   │   vision_strategy.py
│   │   [
│   │       
│   │   ]
│   tests.py
│   [
│       from django.test import TestCase
│       
│       # Create your tests here.
│       
│   ]
│   views.py
│   [
│       from django.shortcuts import render
│       
│       # Create your views here.
│       from rest_framework.views import APIView
│       from rest_framework.response import Response
│       from rest_framework import status
│       from .serializers import InferenceRequestSerializer
│       from .services import InferenceOrchestrationService
│       
│       class InferenceAPIView(APIView):
│           def post(self, request, node_id):
│               serializer = InferenceRequestSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│       
│               service = InferenceOrchestrationService()
│               try:
│                   result = service.process_inference_request(
│                       node_id=str(node_id),
│                       user_id=str(request.user.id),
│                       query_data=serializer.validated_data
│                   )
│                   return Response(result, status=status.HTTP_202_ACCEPTED)
│               except (FileNotFoundError, PermissionError) as e:
│                   return Response({"error": str(e)}, status=status.HTTP_403_FORBIDDEN)
│               except Exception as e:
│                   return Response({"error": f"An unexpected error occurred: {e}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
│   ]
├───inference_internals
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       
│       # Register your models here.
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class InferenceInternalsConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'inference_internals'
│       
│   ]
│   clients.py
│   [
│       # inference_internals/clients.py (Updated Version)
│       import grpc
│       from django.conf import settings
│       from google.protobuf.json_format import MessageToDict
│       
│       # --- THE FIX IS HERE: Import from the 'generated' sub-package ---
│       from .generated import node_pb2, node_pb2_grpc
│       from .generated import model_pb2, model_pb2_grpc
│       
│       class NodeServiceClient:
│           # ... (The rest of this file remains exactly the same)
│           def get_node_details(self, node_id: str, user_id: str) -> dict:
│               with grpc.insecure_channel(settings.NODE_SERVICE_GRPC_URL) as channel:
│                   stub = node_pb2_grpc.NodeServiceStub(channel)
│                   request = node_pb2.GetNodeDetailsRequest(node_id=node_id, user_id=user_id)
│                   try:
│                       response = stub.GetNodeDetails(request, timeout=10)
│                       return MessageToDict(response)
│                   except grpc.RpcError as e:
│                       # ... (error handling code is unchanged)
│                       if e.code() == grpc.StatusCode.NOT_FOUND:
│                           raise FileNotFoundError(f"Node {node_id} not found.")
│                       if e.code() == grpc.StatusCode.PERMISSION_DENIED:
│                           raise PermissionError(f"Permission denied for node {node_id}.")
│                       raise
│       
│       class ModelServiceClient:
│           # ... (The rest of this class remains exactly the same)
│           def get_model_configuration(self, model_id: str, user_id: str) -> dict:
│               with grpc.insecure_channel(settings.MODEL_SERVICE_GRPC_URL) as channel:
│                   stub = model_pb2_grpc.ModelServiceStub(channel)
│                   request = model_pb2.GetModelConfigurationRequest(model_id=model_id, user_id=user_id)
│                   try:
│                       response = stub.GetModelConfiguration(request, timeout=10)
│                       return MessageToDict(response)
│                   except grpc.RpcError as e:
│                       # ... (error handling code is unchanged)
│                       if e.code() == grpc.StatusCode.NOT_FOUND:
│                           raise FileNotFoundError(f"Model {model_id} not found.")
│                       if e.code() == grpc.StatusCode.PERMISSION_DENIED:
│                           raise PermissionError(f"Permission denied for model {model_id}.")
│                       raise
│   ]
│   ├───generated
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   model_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: model.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'model.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0bmodel.proto\x12\x05model\x1a\x1cgoogle/protobuf/struct.proto\"A\n\x1cGetModelConfigurationRequest\x12\x10\n\x08model_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"w\n\x1dGetModelConfigurationResponse\x12\x10\n\x08provider\x18\x01 \x01(\t\x12.\n\rconfiguration\x18\x02 \x01(\x0b\x32\x17.google.protobuf.Struct\x12\x14\n\x0c\x63\x61pabilities\x18\x03 \x03(\t2r\n\x0cModelService\x12\x62\n\x15GetModelConfiguration\x12#.model.GetModelConfigurationRequest\x1a$.model.GetModelConfigurationResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'model_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETMODELCONFIGURATIONREQUEST']._serialized_start=52
│   │         _globals['_GETMODELCONFIGURATIONREQUEST']._serialized_end=117
│   │         _globals['_GETMODELCONFIGURATIONRESPONSE']._serialized_start=119
│   │         _globals['_GETMODELCONFIGURATIONRESPONSE']._serialized_end=238
│   │         _globals['_MODELSERVICE']._serialized_start=240
│   │         _globals['_MODELSERVICE']._serialized_end=354
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   model_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import model_pb2 as model__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in model_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class ModelServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetModelConfiguration = channel.unary_unary(
│   │                       '/model.ModelService/GetModelConfiguration',
│   │                       request_serializer=model__pb2.GetModelConfigurationRequest.SerializeToString,
│   │                       response_deserializer=model__pb2.GetModelConfigurationResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class ModelServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetModelConfiguration(self, request, context):
│   │               """Authorizes and retrieves the full, decrypted configuration for a model.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_ModelServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetModelConfiguration': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetModelConfiguration,
│   │                           request_deserializer=model__pb2.GetModelConfigurationRequest.FromString,
│   │                           response_serializer=model__pb2.GetModelConfigurationResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'model.ModelService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('model.ModelService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class ModelService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetModelConfiguration(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/model.ModelService/GetModelConfiguration',
│   │                   model__pb2.GetModelConfigurationRequest.SerializeToString,
│   │                   model__pb2.GetModelConfigurationResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   │   node_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: node.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'node.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\nnode.proto\x12\x04node\x1a\x1cgoogle/protobuf/struct.proto\"9\n\x15GetNodeDetailsRequest\x12\x0f\n\x07node_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"\x88\x01\n\x16GetNodeDetailsResponse\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nproject_id\x18\x02 \x01(\t\x12\x10\n\x08owner_id\x18\x03 \x01(\t\x12\x0c\n\x04name\x18\x04 \x01(\t\x12.\n\rconfiguration\x18\x05 \x01(\x0b\x32\x17.google.protobuf.Struct2Z\n\x0bNodeService\x12K\n\x0eGetNodeDetails\x12\x1b.node.GetNodeDetailsRequest\x1a\x1c.node.GetNodeDetailsResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'node_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETNODEDETAILSREQUEST']._serialized_start=50
│   │         _globals['_GETNODEDETAILSREQUEST']._serialized_end=107
│   │         _globals['_GETNODEDETAILSRESPONSE']._serialized_start=110
│   │         _globals['_GETNODEDETAILSRESPONSE']._serialized_end=246
│   │         _globals['_NODESERVICE']._serialized_start=248
│   │         _globals['_NODESERVICE']._serialized_end=338
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   node_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import node_pb2 as node__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in node_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class NodeServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetNodeDetails = channel.unary_unary(
│   │                       '/node.NodeService/GetNodeDetails',
│   │                       request_serializer=node__pb2.GetNodeDetailsRequest.SerializeToString,
│   │                       response_deserializer=node__pb2.GetNodeDetailsResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class NodeServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetNodeDetails(self, request, context):
│   │               """Authorizes and retrieves the full details of a node.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_NodeServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetNodeDetails': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetNodeDetails,
│   │                           request_deserializer=node__pb2.GetNodeDetailsRequest.FromString,
│   │                           response_serializer=node__pb2.GetNodeDetailsResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'node.NodeService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('node.NodeService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class NodeService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetNodeDetails(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/node.NodeService/GetNodeDetails',
│   │                   node__pb2.GetNodeDetailsRequest.SerializeToString,
│   │                   node__pb2.GetNodeDetailsResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   ├───management
│   │   └───commands
│   │       generate_protos.py
│   │       [
│   │           # MS5/inference_internals/management/commands/generate_protos.py (Final Version)
│   │           import os
│   │           import subprocess
│   │           import fileinput
│   │           from django.core.management.base import BaseCommand
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Generates Python gRPC code from .proto files.'
│   │               
│   │               # --- THE FIX IS HERE ---
│   │               # This tells Django that this command does not need to check the entire
│   │               # application state (like models and URLs) before running. It can run in isolation.
│   │               requires_system_checks = []
│   │               # --- END OF FIX ---
│   │           
│   │               def handle(self, *args, **options):
│   │                   proto_path = 'inference_internals/protos'
│   │                   output_path = 'inference_internals/generated'
│   │                   
│   │                   if not os.path.exists(proto_path):
│   │                       self.stderr.write(self.style.ERROR(f"Proto path '{proto_path}' does not exist."))
│   │                       return
│   │                   
│   │                   os.makedirs(output_path, exist_ok=True)
│   │                   open(os.path.join(output_path, '__init__.py'), 'a').close()
│   │           
│   │                   proto_files = [f for f in os.listdir(proto_path) if f.endswith('.proto')]
│   │                   if not proto_files:
│   │                       self.stdout.write(self.style.WARNING('No .proto files found.'))
│   │                       return
│   │                       
│   │                   command = [
│   │                       'python', '-m', 'grpc_tools.protoc',
│   │                       f'--proto_path={proto_path}',
│   │                       f'--python_out={output_path}',
│   │                       f'--grpc_python_out={output_path}',
│   │                   ] + proto_files
│   │           
│   │                   self.stdout.write(f"Running command: {' '.join(command)}")
│   │                   try:
│   │                       subprocess.run(command, check=True, capture_output=True, text=True)
│   │                       self.stdout.write(self.style.SUCCESS('Successfully generated gRPC Python stubs.'))
│   │                       
│   │                       for proto_file in proto_files:
│   │                           base_name = proto_file.replace('.proto', '')
│   │                           grpc_file_path = os.path.join(output_path, f'{base_name}_pb2_grpc.py')
│   │                           
│   │                           self.stdout.write(f"Fixing imports in {grpc_file_path}...")
│   │                           with fileinput.FileInput(grpc_file_path, inplace=True) as file:
│   │                               for line in file:
│   │                                   if line.strip() == f'import {base_name}_pb2 as {base_name}__pb2':
│   │                                       print(f'from . import {base_name}_pb2 as {base_name}__pb2')
│   │                                   else:
│   │                                       print(line, end='')
│   │                           self.stdout.write(self.style.SUCCESS('Imports fixed.'))
│   │           
│   │                   except subprocess.CalledProcessError as e:
│   │                       self.stderr.write(self.style.ERROR('Failed to generate gRPC stubs.'))
│   │                       self.stderr.write(e.stderr)
│   │       ]
│   └───protos
│       model.proto
│       [
│           syntax = "proto3";
│           
│           import "google/protobuf/struct.proto";
│           
│           package model;
│           
│           service ModelService {
│             // Authorizes and retrieves the full, decrypted configuration for a model.
│             rpc GetModelConfiguration(GetModelConfigurationRequest) returns (GetModelConfigurationResponse);
│           }
│           
│           message GetModelConfigurationRequest {
│             string model_id = 1;
│             string user_id = 2; // User ID from the JWT, for authorization.
│           }
│           
│           message GetModelConfigurationResponse {
│             string provider = 1;
│             google.protobuf.Struct configuration = 2; // Full, decrypted config (including API keys).
│             repeated string capabilities = 3; // e.g., ["text", "vision"]
│           }
│       ]
│       node.proto
│       [
│           syntax = "proto3";
│           
│           import "google/protobuf/struct.proto";
│           
│           package node;
│           
│           service NodeService {
│             // Authorizes and retrieves the full details of a node.
│             rpc GetNodeDetails(GetNodeDetailsRequest) returns (GetNodeDetailsResponse);
│           }
│           
│           message GetNodeDetailsRequest {
│             string node_id = 1;
│             string user_id = 2; // User ID from the JWT, used for authorization.
│           }
│           
│           message GetNodeDetailsResponse {
│             string id = 1;
│             string project_id = 2;
│             string owner_id = 3;
│             string name = 4;
│             google.protobuf.Struct configuration = 5; // The node's JSON configuration.
│           }
│       ]
│   manage.py
│   [
│       #!/usr/bin/env python
│       """Django's command-line utility for administrative tasks."""
│       import os
│       import sys
│       
│       
│       def main():
│           """Run administrative tasks."""
│           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS5.settings')
│           try:
│               from django.core.management import execute_from_command_line
│           except ImportError as exc:
│               raise ImportError(
│                   "Couldn't import Django. Are you sure it's installed and "
│                   "available on your PYTHONPATH environment variable? Did you "
│                   "forget to activate a virtual environment?"
│               ) from exc
│           execute_from_command_line(sys.argv)
│       
│       
│       if __name__ == '__main__':
│           main()
│       
│   ]
├───messaging
│   __init__.py
│   [
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MessagingConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'messaging'
│       
│   ]
│   event_publisher.py
│   [
│       
│       
│       # messaging/event_publisher.py
│       from .rabbitmq_client import rabbitmq_client
│       
│       class InferenceJobPublisher:
│           def publish_job(self, job_payload: dict):
│               rabbitmq_client.publish(
│                   exchange_name='inference_exchange', # Use a dedicated exchange
│                   routing_key='inference.job.start',  # A specific routing key
│                   body=job_payload
│               )
│       
│       inference_job_publisher = InferenceJobPublisher()
│   ]
│   rabbitmq_client.py
│   [
│       # project service/messaging/rabbitmq_client.py
│       
│       import pika
│       import json
│       import time
│       from django.conf import settings
│       import threading
│       
│       class RabbitMQClient:
│           """
│           A thread-safe RabbitMQ client that ensures one connection per thread.
│           This prevents connection sharing issues between the main web server thread
│           and background worker threads.
│           """
│           _thread_local = threading.local()
│       
│           def _get_connection(self):
│               """Gets or creates a connection for the current thread."""
│               if not hasattr(self._thread_local, 'connection') or self._thread_local.connection.is_closed:
│                   print(f"Thread {threading.get_ident()}: No active connection. Connecting to RabbitMQ...")
│                   try:
│                       params = pika.URLParameters('amqp://guest:guest@localhost:5672/') # In prod, use settings.RABBITMQ_URL
│                       self._thread_local.connection = pika.BlockingConnection(params)
│                       print(f"Thread {threading.get_ident()}: Connection successful.")
│                   except pika.exceptions.AMQPConnectionError as e:
│                       print(f"CRITICAL: Failed to connect to RabbitMQ: {e}")
│                       # In a real app, this should raise an exception or have a retry mechanism.
│                       raise
│               return self._thread_local.connection
│       
│           def publish(self, exchange_name, routing_key, body):
│               """Publishes a message, ensuring a valid connection and channel."""
│               try:
│                   connection = self._get_connection()
│                   channel = connection.channel()
│                   
│                   # Declare exchanges to ensure they exist. This is idempotent.
│                   channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│       
│                   message_body = json.dumps(body, default=str)
│                   
│                   channel.basic_publish(
│                       exchange=exchange_name,
│                       routing_key=routing_key,
│                       body=message_body,
│                       properties=pika.BasicProperties(
│                           content_type='application/json',
│                           delivery_mode=pika.DeliveryMode.Persistent,
│                       )
│                   )
│                   print(f" [x] Sent '{routing_key}':'{message_body}'")
│               except pika.exceptions.AMQPError as e:
│                   print(f"Error publishing message: {e}. Connection may be closed. It will be reopened on next call.")
│                   # Invalidate the connection so it's recreated on the next call
│                   if hasattr(self._thread_local, 'connection'):
│                       self._thread_local.connection.close()
│                   raise # Re-raise the exception so the caller knows the publish failed
│       
│       # Create a single, globally accessible instance.
│       # The instance itself is shared, but the connection it manages is thread-local.
│       rabbitmq_client = RabbitMQClient()
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       Django
│       djangorestframework
│       djangorestframework-simplejwt
│       python-dotenv
│       pika
│       
│       PyJWT
│       grpcio
│       grpcio-tools
│       protobuf
│       google-api-python-client
│       dotenv
│   ]
