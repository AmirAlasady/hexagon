│   .env
│   [
│       # Django's main secret key
│       DJANGO_SECRET_KEY='django-insecure-m3x$8o#H45Ysdverg56564ldpcuck6bytc4h1*8v!=8(_wau6g8or'
│       JWT_SECRET_KEY ='jwt-secure-m3x$DFGRTJRTYNEHRETNEFDDHD43.m<?><DFGRTJYRJGc4h1*8v!=8(_wau6g8or'
│       # You can also add other environment-specific settings here
│       DJANGO_DEBUG='True'
│       DATABASE_URL='sqlite:///./db.sqlite3' # Example for database config
│       JWT_ISSUER="https://ms1.auth-service.com"
│       RABBITMQ_URL='amqp://guest:guest@localhost:5672/'
│       
│       
│       NODE_SERVICE_GRPC_URL=localhost:50051
│       
│       # The address for the Model Service's gRPC server
│       MODEL_SERVICE_GRPC_URL=localhost:50052
│       
│       
│       TOOL_SERVICE_GRPC_URL=localhost:50057
│       
│       MEMORY_SERVICE_GRPC_URL=localhost:50059
│       
│       REDIS_URL="redis://localhost:6379/0"
│   ]
├───MS5
│   __init__.py
│   [
│       
│   ]
│   asgi.py
│   [
│       """
│       ASGI config for MS5 project.
│       
│       It exposes the ASGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
│       """
│       
│       import os
│       
│       from django.core.asgi import get_asgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS5.settings')
│       
│       application = get_asgi_application()
│       
│   ]
│   settings.py
│   [
│       
│       from datetime import timedelta
│       import os
│       from pathlib import Path
│       from datetime import timedelta
│       # Build paths inside the project like this: BASE_DIR / 'subdir'.
│       BASE_DIR = Path(__file__).resolve().parent.parent
│       
│       from dotenv import load_dotenv
│       load_dotenv(BASE_DIR / '.env')
│       # Quick-start development settings - unsuitable for production
│       # See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/
│       MEMORY_SERVICE_GRPC_URL = os.getenv('MEMORY_SERVICE_GRPC_URL')
│       SECRET_KEY = os.getenv('DJANGO_SECRET_KEY')
│       if not SECRET_KEY:
│           # This fallback should ideally not be hit if .env is loaded correctly
│           # or if the environment variable is set directly in the deployment environment.
│           SECRET_KEY = 'django-insecure-fallback-dev-key-!!change-me!!'
│           print("WARNING: DJANGO_SECRET_KEY not found in environment or .env. Using fallback. THIS IS INSECURE FOR PRODUCTION.")
│       
│       DEBUG = os.getenv('DJANGO_DEBUG', 'True').lower() in ('true', '1', 't')
│       import redis
│       
│       REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
│       REDIS_CLIENT = redis.from_url(REDIS_URL)
│       ALLOWED_HOSTS = ['*']
│       
│       
│       # Application definition
│       
│       INSTALLED_APPS = [
│       
│           'django.contrib.admin',
│           'django.contrib.auth',
│           'django.contrib.contenttypes',
│           'django.contrib.sessions',
│           'django.contrib.messages',
│           'django.contrib.staticfiles',
│           'rest_framework',
│           'rest_framework_simplejwt',
│           'inference_engine',
│           'inference_internals',
│           'messaging',
│           
│       ]
│       
│       MIDDLEWARE = [
│           'django.middleware.security.SecurityMiddleware',
│           'django.contrib.sessions.middleware.SessionMiddleware',
│           'django.middleware.common.CommonMiddleware',
│           'django.middleware.csrf.CsrfViewMiddleware',
│           'django.contrib.auth.middleware.AuthenticationMiddleware',
│           'django.contrib.messages.middleware.MessageMiddleware',
│           'django.middleware.clickjacking.XFrameOptionsMiddleware',
│       ]
│       
│       STATIC_URL = '/static/'
│       STATIC_ROOT = BASE_DIR / 'staticfiles'
│       
│       
│       ROOT_URLCONF = 'MS5.urls'
│       
│       TEMPLATES = [
│           {
│               'BACKEND': 'django.template.backends.django.DjangoTemplates',
│               'DIRS': [],
│               'APP_DIRS': True,
│               'OPTIONS': {
│                   'context_processors': [
│                       'django.template.context_processors.request',
│                       'django.contrib.auth.context_processors.auth',
│                       'django.contrib.messages.context_processors.messages',
│                   ],
│               },
│           },
│       ]
│       
│       WSGI_APPLICATION = 'MS5.wsgi.application'
│       
│       
│       # Database
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#databases
│       
│       DATABASES = {
│           'default': {
│               'ENGINE': 'django.db.backends.sqlite3',
│               'NAME': BASE_DIR / 'db.sqlite3',
│           }
│       }
│       
│       
│       # Password validation
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators
│       
│       AUTH_PASSWORD_VALIDATORS = [
│           {
│               'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
│           },
│       ]
│       
│       
│       # Internationalization
│       # https://docs.djangoproject.com/en/5.2/topics/i18n/
│       
│       LANGUAGE_CODE = 'en-us'
│       
│       TIME_ZONE = 'UTC'
│       
│       USE_I18N = True
│       
│       USE_TZ = True
│       
│       
│       
│       DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
│       
│       
│       # Media files
│       MEDIA_URL = '/media/'
│       MEDIA_ROOT = BASE_DIR / 'media'
│       
│       
│       
│       
│       # REST Framework
│       JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY')
│       
│       REST_FRAMEWORK = {
│           "DEFAULT_PERMISSION_CLASSES": ["rest_framework.permissions.IsAuthenticated"],
│           "DEFAULT_AUTHENTICATION_CLASSES": (
│                    
│               "inference_engine.custom_auth.ForceTokenUserJWTAuthentication", # <<< YOUR CUSTOM AUTH CLASS
│           ),
│           'DEFAULT_THROTTLE_CLASSES': (
│               'rest_framework.throttling.AnonRateThrottle',
│               'rest_framework.throttling.UserRateThrottle'
│           ),
│           'DEFAULT_THROTTLE_RATES': {
│               'anon': '100/day',  # Adjust as needed for unauthenticated requests
│               'user': '20000/day' # Adjust as needed for authenticated requests
│           }
│       }
│       
│       SIMPLE_JWT = {
│       
│           "SIGNING_KEY": JWT_SECRET_KEY,  # <<< USE DJANGO'S SECRET_KEY LOADED FROM ENV
│           "VERIFYING_KEY": JWT_SECRET_KEY,
│           "ISSUER": os.getenv('JWT_ISSUER', "https://ms1.auth-service.com"), # MUST match MS1's issuer
│           "AUTH_HEADER_TYPES": ("Bearer",),
│           "ACCESS_TOKEN_LIFETIME": timedelta(minutes=60), # e.g., 1 hour
│           "REFRESH_TOKEN_LIFETIME": timedelta(days=1),    # e.g., 1 day
│           "LEEWAY": timedelta(seconds=10),
│           "ALGORITHM": "HS256",
│           
│           # --- Settings related to interpreting the token payload ---
│           """
│       "USER_ID_CLAIM": "user_id": (Your Specific Question)
│        This is a critical instruction. It tells simple-jwt:
│          "When you parse the token's payload (the data inside),
│            the claim that contains the user's primary identifier is named 'user_id'."
│              Your MS1's CustomTokenObtainPairSerializer probably adds a claim with this name.
│           """
│       
│           "USER_ID_CLAIM": "user_id",
│       
│           "USER_ID_FIELD": "id",
│           "TOKEN_USER_CLASS": "rest_framework_simplejwt.models.TokenUser", # Explicitly use TokenUse
│       
│           # --- Settings for features MS2 likely DOES NOT use ---
│           "UPDATE_LAST_LOGIN": False,
│           "ROTATE_REFRESH_TOKENS": False,
│           "BLACKLIST_AFTER_ROTATION": False, 
│       
│       }
│       
│       
│       RABBITMQ_URL = os.getenv('RABBITMQ_URL', 'amqp://guest:guest@localhost:5672/')
│       
│       # gRPC Client URLs for inter-service communication
│       NODE_SERVICE_GRPC_URL = os.getenv('NODE_SERVICE_GRPC_URL')
│       MODEL_SERVICE_GRPC_URL = os.getenv('MODEL_SERVICE_GRPC_URL')
│       TOOL_SERVICE_GRPC_URL = os.getenv('TOOL_SERVICE_GRPC_URL')
│       
│       
│       
│       
│       # MS5/MS5/settings.py
│       
│       # ... (all your existing settings like DATABASES, SIMPLE_JWT, etc.)
│       
│       # --- ADD THIS ENTIRE LOGGING CONFIGURATION BLOCK ---
│       
│       LOGGING = {
│           "version": 1,
│           "disable_existing_loggers": False,
│           "formatters": {
│               "verbose": {
│                   "format": "%(asctime)s - MS5-Orchestrator - [%(levelname)s] - %(name)s - %(message)s"
│               },
│               "simple": {
│                   "format": "%(levelname)s %(message)s"
│               },
│           },
│           "handlers": {
│               "console": {
│                   "level": "INFO", # Set to DEBUG for more verbose output
│                   "class": "logging.StreamHandler",
│                   "formatter": "verbose",
│               },
│           },
│           "loggers": {
│               # This configures the root logger.
│               # All of your app's loggers will inherit from this.
│               "": {
│                   "handlers": ["console"],
│                   "level": "INFO",
│               },
│               # You can optionally silence noisy libraries here
│               "django.server": {
│                   "handlers": ["console"],
│                   "level": "INFO",
│                   "propagate": False,
│               },
│               "django.request": {
│                   "handlers": ["console"],
│                   "level": "WARNING",
│                   "propagate": False,
│               },
│           },
│       }
│       
│       # --- END OF LOGGING CONFIGURATION BLOCK ---
│   ]
│   urls.py
│   [
│       """
│       Root URL configuration for the MS5 Inference Service project.
│       """
│       from django.contrib import admin
│       from django.urls import path, include
│       
│       urlpatterns = [
│           # Main entry point for all Inference Service API calls.
│           # It delegates all requests starting with '/ms5/api/v1/' to the 'inference_engine' app.
│           path('ms5/api/v1/', include('inference_engine.api_urls')),
│       
│           # Optional: Include the Django admin interface for debugging and management.
│           # This will be accessible at '/ms5/admin/' if you configure an admin user.
│           path('ms5/admin/', admin.site.urls),
│       ]
│       
│       
│       
│   ]
│   views.py
│   [
│       from django.contrib.auth.decorators import login_required
│       from django.http import HttpResponse
│       from django.views.static import serve
│       from django.conf import settings
│       
│       # Protected media view
│       def protected_media_view(request, path):
│           if not request.user.is_authenticated:
│               return HttpResponse('Unauthorized', status=401)
│           # Add additional permission checks here if needed
│           return serve(request, path, document_root=settings.MEDIA_ROOT)
│       
│   ]
│   wsgi.py
│   [
│       """
│       WSGI config for MS5 project.
│       
│       It exposes the WSGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
│       """
│       
│       import os
│       
│       from django.core.wsgi import get_wsgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS5.settings')
│       
│       application = get_wsgi_application()
│       
│   ]
│   db.sqlite3
│   [
│       
│   ]
├───inference_engine
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       
│       # Register your models here.
│       
│   ]
│   api_urls.py
│   [
│       from django.urls import path
│       from .views import InferenceAPIView
│       urlpatterns = [
│           path('nodes/<uuid:node_id>/infer/', InferenceAPIView.as_view(), name='node-infer'),
│       ]
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class InferenceEngineConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'inference_engine'
│       
│   ]
│   custom_auth.py
│   [
│       # MS2/products/custom_auth.py
│       from rest_framework_simplejwt.authentication import JWTAuthentication
│       from rest_framework_simplejwt.models import TokenUser # Import TokenUser
│       from rest_framework_simplejwt.settings import api_settings as simple_jwt_settings
│       from django.utils.translation import gettext_lazy as _
│       from rest_framework_simplejwt.exceptions import InvalidToken
│       
│       class ForceTokenUserJWTAuthentication(JWTAuthentication):
│           def get_user(self, validated_token):
│               """
│               Returns a TokenUser instance based on the validated token.
│               Bypasses any local database User lookup for JWT authentication.
│               """
│               try:
│                   # simple_jwt_settings.USER_ID_CLAIM refers to what you set in settings.py
│                   # e.g., "user_id"
│                   user_id = validated_token[simple_jwt_settings.USER_ID_CLAIM]
│               except KeyError:
│                   raise InvalidToken(_("Token contained no recognizable user identification"))
│       
│               # Correct way to instantiate TokenUser: pass the validated_token
│               # The TokenUser class will internally use USER_ID_CLAIM and USER_ID_FIELD
│               # from your SIMPLE_JWT settings to extract the user ID and set its 'id' or 'pk'.
│               token_user = TokenUser(validated_token)
│       
│               # The TokenUser's 'id' (and 'pk') attribute should now be populated correctly
│               # by its own __init__ method based on the validated_token and your SIMPLE_JWT settings
│               # for USER_ID_CLAIM and USER_ID_FIELD.
│       
│               # Example: If you wanted to verify or access it (not strictly necessary here)
│               # print(f"TokenUser ID: {token_user.id}, TokenUser PK: {token_user.pk}")
│       
│               return token_user
│   ]
│   models.py
│   [
│       from django.db import models
│       
│       # Create your models here.
│       
│   ]
│   serializers.py
│   [
│       # MS5/inference_engine/serializers.py
│       
│       from rest_framework import serializers
│       
│       class InputObjectSerializer(serializers.Serializer):
│           """Defines the structure for a single input item (e.g., a file or image)."""
│           type = serializers.ChoiceField(
│               choices=["file_id", "image_url", "audio_id"],
│               required=True
│           )
│           id = serializers.CharField(required=False)
│           url = serializers.URLField(required=False)
│           # Adding source for better metadata tracking
│           source = serializers.CharField(required=False, default="unknown") 
│       
│           def validate(self, data):
│               """Ensure that the correct reference (id or url) is provided for the type."""
│               input_type = data.get('type')
│               if input_type in ['file_id', 'audio_id'] and not data.get('id'):
│                   raise serializers.ValidationError(f"Input of type '{input_type}' must have an 'id'.")
│               if input_type == 'image_url' and not data.get('url'):
│                   raise serializers.ValidationError("Input of type 'image_url' must have a 'url'.")
│               return data
│       
│       class InferenceRequestSerializer(serializers.Serializer):
│           """
│           The definitive, flexible serializer for all inference requests.
│           It validates the user's immediate intent for a single execution.
│           """
│           prompt = serializers.CharField(required=False, allow_blank=True, allow_null=True)
│           inputs = InputObjectSerializer(many=True, required=False, default=[])
│           
│           resource_overrides = serializers.DictField(required=False, default={})
│           parameter_overrides = serializers.DictField(required=False, default={})
│           output_config = serializers.DictField(required=False, default={})
│           
│           def validate(self, data):
│               """Ensure that at least a prompt or an input is provided."""
│               if not data.get('prompt') and not data.get('inputs'):
│                   raise serializers.ValidationError("An inference request must contain at least a 'prompt' or an 'inputs' array.")
│               return data
│   ]
│   services.py
│   [
│       # MS5/inference_engine/services.py
│       
│       import uuid
│       import json
│       from datetime import datetime
│       import concurrent.futures
│       from rest_framework.exceptions import PermissionDenied, ValidationError
│       import logging
│       
│       from .ticket_manager import generate_ticket
│       from inference_internals.clients import (
│           NodeServiceClient, 
│           ModelServiceClient, 
│           ToolServiceClient,
│           MemoryServiceClient
│       )
│       
│       # Use Django's logging configuration
│       logger = logging.getLogger(__name__)
│       
│       class InferenceOrchestrationService:
│           def __init__(self):
│               self.node_client = NodeServiceClient()
│               self.model_client = ModelServiceClient()
│               self.tool_client = ToolServiceClient()
│               self.memory_client = MemoryServiceClient()
│       
│           def process_inference_request(self, node_id: str, user_id: str, query_data: dict):
│               job_id = str(uuid.uuid4())
│               logger.info(f"--- [JOB {job_id}] ORCHESTRATION STARTED ---")
│               logger.info(f"    Node ID: {node_id} | User ID: {user_id}")
│       
│               # 1. Fetch primary sources of truth
│               logger.info(f"[{job_id}] Step 1/5: Fetching Node details...")
│               node_details = self.node_client.get_node_details(node_id, user_id)
│               
│               node_config = node_details.get("configuration", {})
│               model_id = node_config.get("model_config", {}).get("model_id")
│               if not model_id:
│                   raise ValidationError("Node is not configured with a valid model.")
│               
│               logger.info(f"[{job_id}] Step 1/5: Fetching Model configuration for model_id: {model_id}...")
│               model_details = self.model_client.get_model_configuration(model_id, user_id)
│               
│               # 2. Perform Validation Gauntlet
│               logger.info(f"[{job_id}] Step 2/5: Performing validation gauntlet...")
│               self._validate_request(query_data, node_details, model_details)
│               logger.info(f"[{job_id}] Validation passed.")
│               
│               # 3. Dynamically collect resources
│               logger.info(f"[{job_id}] Step 3/5: Starting parallel resource collection...")
│               collected_resources = self._collect_resources_dynamically(
│                   job_id, user_id, node_config, model_details, query_data
│               )
│               logger.info(f"[{job_id}] Resource collection finished.")
│       
│               # 4. Assemble job payload
│               job_payload = self._assemble_job_payload(
│                   job_id, user_id, node_details, query_data, collected_resources
│               )
│               logger.info(f"[{job_id}] Step 4/5: Job payload assembled.")
│               # logger.debug(json.dumps(job_payload, indent=2)) # Uncomment for deep debugging
│               
│               ws_ticket = generate_ticket(job_id=job_payload["job_id"], user_id=user_id)
│       
│               # 5. Publish the job
│               from messaging.event_publisher import inference_job_publisher
│               inference_job_publisher.publish_job(job_payload)
│               logger.info(f"[{job_id}] Step 5/5: Job published to queue.")
│               logger.info(f"--- [JOB {job_id}] ORCHESTRATION FINISHED ---")
│       
│               return {"job_id": job_payload["job_id"], "status": "Job submitted successfully.", "websocket_ticket": ws_ticket}
│       
│           def _validate_request(self, query_data: dict, node_details: dict, model_details: dict):
│               node_status = node_details.get("status")
│               if node_status in ["inactive", "draft"]:
│                   raise PermissionDenied(f"Node {node_details.get('id')} is in status '{node_status}' and cannot be used for inference.")
│               # ... (rest of validation is correct)
│       
│           def _collect_resources_dynamically(self, job_id: str, user_id: str, node_config: dict, model_details: dict, query_data: dict) -> dict:
│               collected_resources = {"model_config": model_details}
│               overrides = query_data.get("resource_overrides", {})
│       
│               with concurrent.futures.ThreadPoolExecutor() as executor:
│                   future_to_resource = {}
│                   
│                   # Tool collection
│                   if tool_config := node_config.get("tool_config"):
│                       if tool_ids := tool_config.get("tool_ids"):
│                           logger.info(f"[{job_id}] Submitting task: GetToolDefinitions for {len(tool_ids)} tool(s).")
│                           future = executor.submit(self.tool_client.get_tool_definitions, tool_ids, user_id)
│                           future_to_resource[future] = "tools"
│       
│                   # Memory collection
│                   if memory_config := node_config.get("memory_config"):
│                       use_memory = overrides.get("use_memory", memory_config.get("is_enabled", False))
│                       if str(use_memory).lower() == 'true':
│                           bucket_id = memory_config.get("bucket_id")
│                           if not bucket_id:
│                               raise ValidationError("Memory is enabled but no 'bucket_id' is configured.")
│                           logger.info(f"[{job_id}] Submitting task: GetHistory for memory bucket: {bucket_id}")
│                           future = executor.submit(self.memory_client.get_history, bucket_id, user_id)
│                           future_to_resource[future] = "memory_context" # <-- This key is correct
│       
│                   logger.info(f"[{job_id}] Awaiting {len(future_to_resource)} resource task(s)...")
│                   for future in concurrent.futures.as_completed(future_to_resource):
│                       resource_name = future_to_resource[future]
│                       try:
│                           # THE BUG WAS HERE. The gRPC response IS the value.
│                           collected_resources[resource_name] = future.result()
│                           logger.info(f"[{job_id}] --> Successfully collected resource: '{resource_name}'")
│                       except Exception as exc:
│                           logger.error(f"[{job_id}] --> FAILED to collect resource: '{resource_name}'. Reason: {exc}")
│                           raise RuntimeError(f'Resource collection for "{resource_name}" failed') from exc
│               
│               return collected_resources
│       
│           def _assemble_job_payload(self, job_id: str, user_id: str, node_details: dict, query_data: dict, resources: dict) -> dict:
│               node_config = node_details.get("configuration", {})
│               final_resources = {
│                   "model_config": resources.get("model_config"), "tools": resources.get("tools"),
│                   "rag_context": resources.get("rag_context"), "memory_context": resources.get("memory_context"),
│                   "on_the_fly_data": []
│               }
│               return { "job_id": job_id, "user_id": user_id, "timestamp": datetime.utcnow().isoformat(), "query": query_data, "default_parameters": node_config.get("model_config", {}).get("parameters", {}), "resources": final_resources }
│   ]
│   ├───strategies
│   │   __init__.py
│   │   [
│   │       from .base_strategy import BaseCollectionStrategy
│   │       from .llm_strategy import LLMCollectionStrategy
│   │       
│   │       def get_strategy(node_config: dict) -> BaseCollectionStrategy:
│   │           """Factory function to select the appropriate collection strategy."""
│   │           node_type = node_config.get("configuration", {}).get("node_type")
│   │       
│   │           if node_type in ["ai_agent", "llm_chat"]:
│   │               return LLMCollectionStrategy
│   │           # elif node_type == "image_diffusion":
│   │           #     return DiffusionCollectionStrategy
│   │           else:
│   │               raise ValueError(f"No collection strategy found for node type: {node_type}")
│   │           
│   │       
│   │       
│   │   ]
│   │   base_strategy.py
│   │   [
│   │       from abc import ABC, abstractmethod
│   │       
│   │       class BaseCollectionStrategy(ABC):
│   │           """Abstract base class for a resource collection strategy."""
│   │           def __init__(self, user_id: str, node_config: dict):
│   │               self.user_id = user_id
│   │               self.node_config = node_config
│   │       
│   │           @abstractmethod
│   │           def collect_resources(self) -> dict:
│   │               """
│   │               Collects all necessary resources for an inference job.
│   │               This method must be implemented by all concrete strategies.
│   │               Returns a dictionary of the collected resources.
│   │               """
│   │               pass
│   │   ]
│   │   llm_strategy.py
│   │   [
│   │       import concurrent.futures
│   │       from .base_strategy import BaseCollectionStrategy
│   │       # --- FIX 1: Clean and correct imports ---
│   │       from inference_internals.clients import ModelServiceClient, ToolServiceClient
│   │       # (Future: from inference_internals.clients import MemoryServiceClient)
│   │       
│   │       class LLMCollectionStrategy(BaseCollectionStrategy):
│   │           """
│   │           Strategy for collecting resources for an LLM job.
│   │           Gathers model configuration and, if enabled, tool definitions, memory, etc.
│   │           """
│   │           def collect_resources(self) -> dict:
│   │               # --- FIX 2: Use a single dictionary to store all collected resources ---
│   │               collected_resources = {
│   │                   "model_config": {},
│   │                   "tools": [],
│   │                   "memory_context": None,
│   │                   "rag_documents": None,
│   │               }
│   │       
│   │               with concurrent.futures.ThreadPoolExecutor() as executor:
│   │                   future_to_resource = {}
│   │                   
│   │                   # --- Submit Model Config Task (unchanged) ---
│   │                   model_id = self.node_config.get("configuration", {}).get("model_config", {}).get("model_id")
│   │                   if not model_id:
│   │                       raise ValueError("Node configuration is missing a model_id.")
│   │                   
│   │                   model_client = ModelServiceClient()
│   │                   future_model = executor.submit(model_client.get_model_configuration, model_id, self.user_id)
│   │                   future_to_resource[future_model] = "model_config"
│   │       
│   │                   # --- Submit Tool Definitions Task (unchanged logic, just integrated) ---
│   │                   tool_config = self.node_config.get("configuration", {}).get("tool_config", {})
│   │                   if "tool_ids" in tool_config and tool_config["tool_ids"]:
│   │                       tool_client = ToolServiceClient()
│   │                       future_tools = executor.submit(
│   │                           tool_client.get_tool_definitions,
│   │                           tool_config["tool_ids"],
│   │                           self.user_id
│   │                       )
│   │                       future_to_resource[future_tools] = "tools"
│   │       
│   │                   # --- Submit Memory Task (if enabled) ---
│   │                   # ... (your future memory logic would go here)
│   │       
│   │                   # --- Process results as they complete ---
│   │                   for future in concurrent.futures.as_completed(future_to_resource):
│   │                       resource_name = future_to_resource[future]
│   │                       try:
│   │                           result = future.result()
│   │                           # --- FIX 3: Store the result in the correct dictionary key ---
│   │                           collected_resources[resource_name] = result
│   │       
│   │                       except Exception as exc:
│   │                           print(f'Resource collection for "{resource_name}" failed: {exc}')
│   │                           raise # Re-raise the exception to fail the entire request
│   │       
│   │               # --- FIX 4: Return the complete dictionary ---
│   │               return collected_resources
│   │   ]
│   │   vision_strategy.py
│   │   [
│   │       
│   │   ]
│   tests.py
│   [
│       from django.test import TestCase
│       
│       # Create your tests here.
│       
│   ]
│   ticket_manager.py
│   [
│       # MS5/inference_engine/ticket_manager.py
│       import secrets
│       import json
│       from django.conf import settings
│       
│       def generate_ticket(job_id: str, user_id: str) -> str:
│           """
│           Generates a secure, one-time ticket and stores it in Redis with a TTL.
│           """
│           ticket = f"ws_ticket_{secrets.token_urlsafe(32)}"
│           ticket_data = {
│               "job_id": job_id,
│               "user_id": user_id
│           }
│           
│           # Use SETEX to set the key with an expiry of 60 seconds.
│           settings.REDIS_CLIENT.setex(
│               f"ws_ticket:{ticket}",
│               60,
│               json.dumps(ticket_data)
│           )
│           return ticket
│   ]
│   views.py
│   [
│       # MS5/inference_engine/views.py
│       
│       from rest_framework.views import APIView
│       from rest_framework.response import Response
│       from rest_framework import status, permissions
│       from rest_framework.exceptions import PermissionDenied, ValidationError
│       
│       from .serializers import InferenceRequestSerializer
│       from .services import InferenceOrchestrationService
│       
│       class InferenceAPIView(APIView):
│           """
│           The single entry point for initiating an inference job on a configured node.
│           It delegates all complex logic to the InferenceOrchestrationService.
│           
│           Endpoint: POST /ms5/api/v1/nodes/{node_id}/infer/
│           """
│           permission_classes = [permissions.IsAuthenticated]
│           
│           def post(self, request, node_id):
│               """
│               Handles the submission of a new inference job.
│               
│               1. Validates the user's incoming query data.
│               2. Passes the request to the service layer for orchestration.
│               3. Catches any exceptions (e.g., for invalid nodes, permission issues)
│                  and formats them into a user-friendly error response.
│               """
│               # Step 1: Validate the request body (e.g., ensure 'prompt' is present)
│               serializer = InferenceRequestSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│       
│               service = InferenceOrchestrationService()
│               try:
│                   # Step 2: Delegate the core logic to the service layer
│                   result = service.process_inference_request(
│                       node_id=str(node_id),
│                       user_id=str(request.user.id),
│                       query_data=serializer.validated_data
│                   )
│                   
│                   # Step 3: Return a success response indicating the job was submitted
│                   return Response(result, status=status.HTTP_202_ACCEPTED)
│               
│               # Step 4: Handle specific, known errors gracefully
│               except FileNotFoundError as e:
│                   # This is typically raised by a gRPC client if a resource (node, model, tool) is not found.
│                   return Response({"error": str(e)}, status=status.HTTP_404_NOT_FOUND)
│                   
│               except PermissionDenied as e:
│                   # This can be raised by a gRPC client OR by our own service layer
│                   # (e.g., if the node status is 'inactive' or 'draft').
│                   return Response({"error": str(e)}, status=status.HTTP_403_FORBIDDEN)
│                   
│               except ValidationError as e:
│                   # Catches validation errors from other services.
│                   return Response({"error": str(e)}, status=status.HTTP_400_BAD_REQUEST)
│                   
│               except Exception as e:
│                   # Catch-all for any other unexpected server errors
│                   print(f"CRITICAL: Unexpected error in inference orchestration for node {node_id}: {e}")
│                   return Response(
│                       {"error": "An unexpected server error occurred during job orchestration."}, 
│                       status=status.HTTP_500_INTERNAL_SERVER_ERROR
│                   )
│   ]
├───inference_internals
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       
│       # Register your models here.
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class InferenceInternalsConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'inference_internals'
│       
│   ]
│   clients.py
│   [
│       # MS5/inference_internals/clients.py
│       
│       import grpc
│       from django.conf import settings
│       from google.protobuf.json_format import MessageToDict
│       from rest_framework.exceptions import PermissionDenied, NotFound, ValidationError
│       
│       # Import all generated gRPC stubs from the 'generated' sub-package
│       from .generated import node_pb2, node_pb2_grpc
│       from .generated import model_pb2, model_pb2_grpc
│       from .generated import tool_pb2, tool_pb2_grpc
│       from .generated import memory_pb2, memory_pb2_grpc
│       
│       class NodeServiceClient:
│           """A gRPC client for interacting with the Node Service (MS4)."""
│           def get_node_details(self, node_id: str, user_id: str) -> dict:
│               try:
│                   with grpc.insecure_channel(settings.NODE_SERVICE_GRPC_URL) as channel:
│                       stub = node_pb2_grpc.NodeServiceStub(channel)
│                       request = node_pb2.GetNodeDetailsRequest(node_id=node_id, user_id=user_id)
│                       response = stub.GetNodeDetails(request, timeout=10)
│                       
│                       # Use preserving_proto_field_name=True to ensure snake_case keys
│                       return MessageToDict(response, preserving_proto_field_name=True)
│       
│               except grpc.RpcError as e:
│                   if e.code() == grpc.StatusCode.NOT_FOUND:
│                       raise NotFound(f"Node '{node_id}' not found.")
│                   if e.code() == grpc.StatusCode.PERMISSION_DENIED:
│                       raise PermissionDenied(f"Permission denied for node '{node_id}'.")
│                   raise RuntimeError(f"gRPC error from Node Service: {e.details()}")
│               except Exception as e:
│                   raise RuntimeError(f"Unexpected error in NodeServiceClient: {e}")
│       
│       class ModelServiceClient:
│           """A gRPC client for interacting with the Model Service (MS3)."""
│           def get_model_configuration(self, model_id: str, user_id: str) -> dict:
│               try:
│                   with grpc.insecure_channel(settings.MODEL_SERVICE_GRPC_URL) as channel:
│                       stub = model_pb2_grpc.ModelServiceStub(channel)
│                       request = model_pb2.GetModelConfigurationRequest(model_id=model_id, user_id=user_id)
│                       response = stub.GetModelConfiguration(request, timeout=10)
│                       
│                       # Use preserving_proto_field_name=True to ensure snake_case keys
│                       return MessageToDict(response, preserving_proto_field_name=True)
│                       
│               except grpc.RpcError as e:
│                   if e.code() == grpc.StatusCode.NOT_FOUND:
│                       raise NotFound(f"Model '{model_id}' not found.")
│                   if e.code() == grpc.StatusCode.PERMISSION_DENIED:
│                       raise PermissionDenied(f"Permission denied for model '{model_id}'.")
│                   raise RuntimeError(f"gRPC error from Model Service: {e.details()}")
│               except Exception as e:
│                   raise RuntimeError(f"Unexpected error in ModelServiceClient: {e}")
│       
│       class ToolServiceClient:
│           """A gRPC client for interacting with the Tool Service (MS7)."""
│           def get_tool_definitions(self, tool_ids: list[str], user_id: str) -> list[dict]:
│               try:
│                   with grpc.insecure_channel(settings.TOOL_SERVICE_GRPC_URL) as channel:
│                       stub = tool_pb2_grpc.ToolServiceStub(channel)
│                       request = tool_pb2.GetToolDefinitionsRequest(user_id=user_id, tool_ids=tool_ids)
│                       response = stub.GetToolDefinitions(request, timeout=10)
│                       
│                       # Use preserving_proto_field_name=True for each item in the list
│                       return [MessageToDict(d, preserving_proto_field_name=True) for d in response.definitions]
│                       
│               except grpc.RpcError as e:
│                   if e.code() == grpc.StatusCode.NOT_FOUND:
│                       raise NotFound("One or more requested tools were not found.")
│                   raise RuntimeError(f"gRPC error from Tool Service: {e.details()}")
│               except Exception as e:
│                   raise RuntimeError(f"Unexpected error in ToolServiceClient: {e}")
│       
│       class MemoryServiceClient:
│           """A gRPC client for interacting with the Memory Service (MS9)."""
│           def get_history(self, bucket_id: str, user_id: str) -> dict:
│               if not settings.MEMORY_SERVICE_GRPC_URL:
│                   print("WARNING: MEMORY_SERVICE_GRPC_URL not set in .env. Skipping memory call.")
│                   return {} # Return empty dict to avoid crashes if the service isn't configured
│       
│               try:
│                   with grpc.insecure_channel(settings.MEMORY_SERVICE_GRPC_URL) as channel:
│                       stub = memory_pb2_grpc.MemoryServiceStub(channel)
│                       request = memory_pb2.GetHistoryRequest(bucket_id=bucket_id, user_id=user_id)
│                       
│                       print(f"DEBUG [MS5]: Calling GetHistory for bucket: {bucket_id}")
│                       response = stub.GetHistory(request, timeout=10)
│                       
│                       # Use preserving_proto_field_name=True to ensure snake_case keys
│                       return MessageToDict(response, preserving_proto_field_name=True)
│       
│               except grpc.RpcError as e:
│                   if e.code() == grpc.StatusCode.NOT_FOUND:
│                       raise NotFound(f"Memory bucket '{bucket_id}' not found.")
│                   if e.code() == grpc.StatusCode.PERMISSION_DENIED:
│                       raise PermissionDenied(f"Permission denied for memory bucket '{bucket_id}'.")
│                   raise RuntimeError(f"A gRPC error occurred while contacting the Memory Service: {e.details()}")
│               except Exception as e:
│                   raise RuntimeError(f"Unexpected error in MemoryServiceClient: {e}")
│   ]
│   ├───generated
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   memory_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: memory.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'memory.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0cmemory.proto\x12\x06memory\x1a\x1cgoogle/protobuf/struct.proto\"7\n\x11GetHistoryRequest\x12\x11\n\tbucket_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"f\n\x12GetHistoryResponse\x12\x11\n\tbucket_id\x18\x01 \x01(\t\x12\x13\n\x0bmemory_type\x18\x02 \x01(\t\x12(\n\x07history\x18\x03 \x03(\x0b\x32\x17.google.protobuf.Struct2T\n\rMemoryService\x12\x43\n\nGetHistory\x12\x19.memory.GetHistoryRequest\x1a\x1a.memory.GetHistoryResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'memory_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETHISTORYREQUEST']._serialized_start=54
│   │         _globals['_GETHISTORYREQUEST']._serialized_end=109
│   │         _globals['_GETHISTORYRESPONSE']._serialized_start=111
│   │         _globals['_GETHISTORYRESPONSE']._serialized_end=213
│   │         _globals['_MEMORYSERVICE']._serialized_start=215
│   │         _globals['_MEMORYSERVICE']._serialized_end=299
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   memory_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import memory_pb2 as memory__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in memory_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class MemoryServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetHistory = channel.unary_unary(
│   │                       '/memory.MemoryService/GetHistory',
│   │                       request_serializer=memory__pb2.GetHistoryRequest.SerializeToString,
│   │                       response_deserializer=memory__pb2.GetHistoryResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class MemoryServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetHistory(self, request, context):
│   │               """Fetches and processes the history for a given memory bucket.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_MemoryServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetHistory': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetHistory,
│   │                           request_deserializer=memory__pb2.GetHistoryRequest.FromString,
│   │                           response_serializer=memory__pb2.GetHistoryResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'memory.MemoryService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('memory.MemoryService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class MemoryService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetHistory(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/memory.MemoryService/GetHistory',
│   │                   memory__pb2.GetHistoryRequest.SerializeToString,
│   │                   memory__pb2.GetHistoryResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   │   model_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: model.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'model.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0bmodel.proto\x12\x05model\x1a\x1cgoogle/protobuf/struct.proto\"A\n\x1cGetModelConfigurationRequest\x12\x10\n\x08model_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"w\n\x1dGetModelConfigurationResponse\x12\x10\n\x08provider\x18\x01 \x01(\t\x12.\n\rconfiguration\x18\x02 \x01(\x0b\x32\x17.google.protobuf.Struct\x12\x14\n\x0c\x63\x61pabilities\x18\x03 \x03(\t2r\n\x0cModelService\x12\x62\n\x15GetModelConfiguration\x12#.model.GetModelConfigurationRequest\x1a$.model.GetModelConfigurationResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'model_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETMODELCONFIGURATIONREQUEST']._serialized_start=52
│   │         _globals['_GETMODELCONFIGURATIONREQUEST']._serialized_end=117
│   │         _globals['_GETMODELCONFIGURATIONRESPONSE']._serialized_start=119
│   │         _globals['_GETMODELCONFIGURATIONRESPONSE']._serialized_end=238
│   │         _globals['_MODELSERVICE']._serialized_start=240
│   │         _globals['_MODELSERVICE']._serialized_end=354
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   model_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import model_pb2 as model__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in model_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class ModelServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetModelConfiguration = channel.unary_unary(
│   │                       '/model.ModelService/GetModelConfiguration',
│   │                       request_serializer=model__pb2.GetModelConfigurationRequest.SerializeToString,
│   │                       response_deserializer=model__pb2.GetModelConfigurationResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class ModelServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetModelConfiguration(self, request, context):
│   │               """Authorizes and retrieves the full, decrypted configuration for a model.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_ModelServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetModelConfiguration': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetModelConfiguration,
│   │                           request_deserializer=model__pb2.GetModelConfigurationRequest.FromString,
│   │                           response_serializer=model__pb2.GetModelConfigurationResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'model.ModelService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('model.ModelService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class ModelService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetModelConfiguration(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/model.ModelService/GetModelConfiguration',
│   │                   model__pb2.GetModelConfigurationRequest.SerializeToString,
│   │                   model__pb2.GetModelConfigurationResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   │   node_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: node.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'node.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\nnode.proto\x12\x04node\x1a\x1cgoogle/protobuf/struct.proto\"9\n\x15GetNodeDetailsRequest\x12\x0f\n\x07node_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"\x98\x01\n\x16GetNodeDetailsResponse\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nproject_id\x18\x02 \x01(\t\x12\x10\n\x08owner_id\x18\x03 \x01(\t\x12\x0c\n\x04name\x18\x04 \x01(\t\x12.\n\rconfiguration\x18\x05 \x01(\x0b\x32\x17.google.protobuf.Struct\x12\x0e\n\x06status\x18\x06 \x01(\t2Z\n\x0bNodeService\x12K\n\x0eGetNodeDetails\x12\x1b.node.GetNodeDetailsRequest\x1a\x1c.node.GetNodeDetailsResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'node_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETNODEDETAILSREQUEST']._serialized_start=50
│   │         _globals['_GETNODEDETAILSREQUEST']._serialized_end=107
│   │         _globals['_GETNODEDETAILSRESPONSE']._serialized_start=110
│   │         _globals['_GETNODEDETAILSRESPONSE']._serialized_end=262
│   │         _globals['_NODESERVICE']._serialized_start=264
│   │         _globals['_NODESERVICE']._serialized_end=354
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   node_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import node_pb2 as node__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in node_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class NodeServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetNodeDetails = channel.unary_unary(
│   │                       '/node.NodeService/GetNodeDetails',
│   │                       request_serializer=node__pb2.GetNodeDetailsRequest.SerializeToString,
│   │                       response_deserializer=node__pb2.GetNodeDetailsResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class NodeServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetNodeDetails(self, request, context):
│   │               """Authorizes and retrieves the full details of a node for the Inference Service.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_NodeServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetNodeDetails': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetNodeDetails,
│   │                           request_deserializer=node__pb2.GetNodeDetailsRequest.FromString,
│   │                           response_serializer=node__pb2.GetNodeDetailsResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'node.NodeService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('node.NodeService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class NodeService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetNodeDetails(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/node.NodeService/GetNodeDetails',
│   │                   node__pb2.GetNodeDetailsRequest.SerializeToString,
│   │                   node__pb2.GetNodeDetailsResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   │   tool_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: tool.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'tool.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\ntool.proto\x12\x04tool\x1a\x1cgoogle/protobuf/struct.proto\">\n\x19GetToolDefinitionsRequest\x12\x0f\n\x07user_id\x18\x01 \x01(\t\x12\x10\n\x08tool_ids\x18\x02 \x03(\t\"J\n\x1aGetToolDefinitionsResponse\x12,\n\x0b\x64\x65\x66initions\x18\x01 \x03(\x0b\x32\x17.google.protobuf.Struct\"9\n\x14ValidateToolsRequest\x12\x0f\n\x07user_id\x18\x01 \x01(\t\x12\x10\n\x08tool_ids\x18\x02 \x03(\t\"B\n\x15ValidateToolsResponse\x12\x12\n\nauthorized\x18\x01 \x01(\x08\x12\x15\n\rerror_message\x18\x02 \x01(\t\"P\n\x08ToolCall\x12\n\n\x02id\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12*\n\targuments\x18\x03 \x01(\x0b\x32\x17.google.protobuf.Struct\"A\n\x1b\x45xecuteMultipleToolsRequest\x12\"\n\ntool_calls\x18\x01 \x03(\x0b\x32\x0e.tool.ToolCall\"P\n\nToolResult\x12\x14\n\x0ctool_call_id\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x0e\n\x06status\x18\x03 \x01(\t\x12\x0e\n\x06output\x18\x04 \x01(\t\"A\n\x1c\x45xecuteMultipleToolsResponse\x12!\n\x07results\x18\x01 \x03(\x0b\x32\x10.tool.ToolResult2\x8f\x02\n\x0bToolService\x12W\n\x12GetToolDefinitions\x12\x1f.tool.GetToolDefinitionsRequest\x1a .tool.GetToolDefinitionsResponse\x12H\n\rValidateTools\x12\x1a.tool.ValidateToolsRequest\x1a\x1b.tool.ValidateToolsResponse\x12]\n\x14\x45xecuteMultipleTools\x12!.tool.ExecuteMultipleToolsRequest\x1a\".tool.ExecuteMultipleToolsResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tool_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETTOOLDEFINITIONSREQUEST']._serialized_start=50
│   │         _globals['_GETTOOLDEFINITIONSREQUEST']._serialized_end=112
│   │         _globals['_GETTOOLDEFINITIONSRESPONSE']._serialized_start=114
│   │         _globals['_GETTOOLDEFINITIONSRESPONSE']._serialized_end=188
│   │         _globals['_VALIDATETOOLSREQUEST']._serialized_start=190
│   │         _globals['_VALIDATETOOLSREQUEST']._serialized_end=247
│   │         _globals['_VALIDATETOOLSRESPONSE']._serialized_start=249
│   │         _globals['_VALIDATETOOLSRESPONSE']._serialized_end=315
│   │         _globals['_TOOLCALL']._serialized_start=317
│   │         _globals['_TOOLCALL']._serialized_end=397
│   │         _globals['_EXECUTEMULTIPLETOOLSREQUEST']._serialized_start=399
│   │         _globals['_EXECUTEMULTIPLETOOLSREQUEST']._serialized_end=464
│   │         _globals['_TOOLRESULT']._serialized_start=466
│   │         _globals['_TOOLRESULT']._serialized_end=546
│   │         _globals['_EXECUTEMULTIPLETOOLSRESPONSE']._serialized_start=548
│   │         _globals['_EXECUTEMULTIPLETOOLSRESPONSE']._serialized_end=613
│   │         _globals['_TOOLSERVICE']._serialized_start=616
│   │         _globals['_TOOLSERVICE']._serialized_end=887
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   tool_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import tool_pb2 as tool__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in tool_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class ToolServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetToolDefinitions = channel.unary_unary(
│   │                       '/tool.ToolService/GetToolDefinitions',
│   │                       request_serializer=tool__pb2.GetToolDefinitionsRequest.SerializeToString,
│   │                       response_deserializer=tool__pb2.GetToolDefinitionsResponse.FromString,
│   │                       _registered_method=True)
│   │               self.ValidateTools = channel.unary_unary(
│   │                       '/tool.ToolService/ValidateTools',
│   │                       request_serializer=tool__pb2.ValidateToolsRequest.SerializeToString,
│   │                       response_deserializer=tool__pb2.ValidateToolsResponse.FromString,
│   │                       _registered_method=True)
│   │               self.ExecuteMultipleTools = channel.unary_unary(
│   │                       '/tool.ToolService/ExecuteMultipleTools',
│   │                       request_serializer=tool__pb2.ExecuteMultipleToolsRequest.SerializeToString,
│   │                       response_deserializer=tool__pb2.ExecuteMultipleToolsResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class ToolServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetToolDefinitions(self, request, context):
│   │               """For Inference P1: Fetches the full definitions for a list of tools.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │           def ValidateTools(self, request, context):
│   │               """These other RPCs are for different services, but it's good practice
│   │               for the proto to contain the full service definition.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │           def ExecuteMultipleTools(self, request, context):
│   │               """Missing associated documentation comment in .proto file."""
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_ToolServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetToolDefinitions': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetToolDefinitions,
│   │                           request_deserializer=tool__pb2.GetToolDefinitionsRequest.FromString,
│   │                           response_serializer=tool__pb2.GetToolDefinitionsResponse.SerializeToString,
│   │                   ),
│   │                   'ValidateTools': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.ValidateTools,
│   │                           request_deserializer=tool__pb2.ValidateToolsRequest.FromString,
│   │                           response_serializer=tool__pb2.ValidateToolsResponse.SerializeToString,
│   │                   ),
│   │                   'ExecuteMultipleTools': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.ExecuteMultipleTools,
│   │                           request_deserializer=tool__pb2.ExecuteMultipleToolsRequest.FromString,
│   │                           response_serializer=tool__pb2.ExecuteMultipleToolsResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'tool.ToolService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('tool.ToolService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class ToolService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetToolDefinitions(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/tool.ToolService/GetToolDefinitions',
│   │                   tool__pb2.GetToolDefinitionsRequest.SerializeToString,
│   │                   tool__pb2.GetToolDefinitionsResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │           @staticmethod
│   │           def ValidateTools(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/tool.ToolService/ValidateTools',
│   │                   tool__pb2.ValidateToolsRequest.SerializeToString,
│   │                   tool__pb2.ValidateToolsResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │           @staticmethod
│   │           def ExecuteMultipleTools(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/tool.ToolService/ExecuteMultipleTools',
│   │                   tool__pb2.ExecuteMultipleToolsRequest.SerializeToString,
│   │                   tool__pb2.ExecuteMultipleToolsResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   ├───management
│   │   └───commands
│   │       generate_protos.py
│   │       [
│   │           # MS5/inference_internals/management/commands/generate_protos.py (Final Version)
│   │           import os
│   │           import subprocess
│   │           import fileinput
│   │           from django.core.management.base import BaseCommand
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Generates Python gRPC code from .proto files.'
│   │               
│   │               # --- THE FIX IS HERE ---
│   │               # This tells Django that this command does not need to check the entire
│   │               # application state (like models and URLs) before running. It can run in isolation.
│   │               requires_system_checks = []
│   │               # --- END OF FIX ---
│   │           
│   │               def handle(self, *args, **options):
│   │                   proto_path = 'inference_internals/protos'
│   │                   output_path = 'inference_internals/generated'
│   │                   
│   │                   if not os.path.exists(proto_path):
│   │                       self.stderr.write(self.style.ERROR(f"Proto path '{proto_path}' does not exist."))
│   │                       return
│   │                   
│   │                   os.makedirs(output_path, exist_ok=True)
│   │                   open(os.path.join(output_path, '__init__.py'), 'a').close()
│   │           
│   │                   proto_files = [f for f in os.listdir(proto_path) if f.endswith('.proto')]
│   │                   if not proto_files:
│   │                       self.stdout.write(self.style.WARNING('No .proto files found.'))
│   │                       return
│   │                       
│   │                   command = [
│   │                       'python', '-m', 'grpc_tools.protoc',
│   │                       f'--proto_path={proto_path}',
│   │                       f'--python_out={output_path}',
│   │                       f'--grpc_python_out={output_path}',
│   │                   ] + proto_files
│   │           
│   │                   self.stdout.write(f"Running command: {' '.join(command)}")
│   │                   try:
│   │                       subprocess.run(command, check=True, capture_output=True, text=True)
│   │                       self.stdout.write(self.style.SUCCESS('Successfully generated gRPC Python stubs.'))
│   │                       
│   │                       for proto_file in proto_files:
│   │                           base_name = proto_file.replace('.proto', '')
│   │                           grpc_file_path = os.path.join(output_path, f'{base_name}_pb2_grpc.py')
│   │                           
│   │                           self.stdout.write(f"Fixing imports in {grpc_file_path}...")
│   │                           with fileinput.FileInput(grpc_file_path, inplace=True) as file:
│   │                               for line in file:
│   │                                   if line.strip() == f'import {base_name}_pb2 as {base_name}__pb2':
│   │                                       print(f'from . import {base_name}_pb2 as {base_name}__pb2')
│   │                                   else:
│   │                                       print(line, end='')
│   │                           self.stdout.write(self.style.SUCCESS('Imports fixed.'))
│   │           
│   │                   except subprocess.CalledProcessError as e:
│   │                       self.stderr.write(self.style.ERROR('Failed to generate gRPC stubs.'))
│   │                       self.stderr.write(e.stderr)
│   │       ]
│   └───protos
│       memory.proto
│       [
│           // MS5/inference_internals/protos/memory.proto
│           syntax = "proto3";
│           
│           import "google/protobuf/struct.proto";
│           
│           package memory;
│           
│           service MemoryService {
│             // Fetches and processes the history for a given memory bucket.
│             rpc GetHistory(GetHistoryRequest) returns (GetHistoryResponse);
│           }
│           
│           message GetHistoryRequest {
│             string bucket_id = 1;
│             string user_id = 2; // For authorization
│           }
│           
│           message GetHistoryResponse {
│             string bucket_id = 1;
│             string memory_type = 2;
│             repeated google.protobuf.Struct history = 3; // The processed, ready-to-use context
│           }
│       ]
│       model.proto
│       [
│           syntax = "proto3";
│           
│           import "google/protobuf/struct.proto";
│           
│           package model;
│           
│           service ModelService {
│             // Authorizes and retrieves the full, decrypted configuration for a model.
│             rpc GetModelConfiguration(GetModelConfigurationRequest) returns (GetModelConfigurationResponse);
│           }
│           
│           message GetModelConfigurationRequest {
│             string model_id = 1;
│             string user_id = 2; // User ID from the JWT, for authorization.
│           }
│           
│           message GetModelConfigurationResponse {
│             string provider = 1;
│             google.protobuf.Struct configuration = 2; // Full, decrypted config (including API keys).
│             repeated string capabilities = 3; // e.g., ["text", "vision"]
│           }
│       ]
│       node.proto
│       [
│           // MS5/inference_internals/protos/node.proto
│           syntax = "proto3";
│           
│           import "google/protobuf/struct.proto";
│           
│           package node;
│           
│           service NodeService {
│             // Authorizes and retrieves the full details of a node for the Inference Service.
│             rpc GetNodeDetails(GetNodeDetailsRequest) returns (GetNodeDetailsResponse);
│           }
│           
│           message GetNodeDetailsRequest {
│             string node_id = 1;
│             string user_id = 2; // User ID from the JWT, used for authorization.
│           }
│           
│           message GetNodeDetailsResponse {
│             string id = 1;
│             string project_id = 2;
│             string owner_id = 3;
│             string name = 4;
│             google.protobuf.Struct configuration = 5; // The node's JSON configuration.
│             string status = 6; // The current status ('draft', 'active', 'altered', 'inactive')
│           }
│       ]
│       tool.proto
│       [
│           // MS5/inference_internals/protos/tool.proto
│           syntax = "proto3";
│           
│           import "google/protobuf/struct.proto";
│           
│           package tool;
│           
│           service ToolService {
│             // For Inference P1: Fetches the full definitions for a list of tools.
│             rpc GetToolDefinitions(GetToolDefinitionsRequest) returns (GetToolDefinitionsResponse);
│             
│             // These other RPCs are for different services, but it's good practice
│             // for the proto to contain the full service definition.
│             rpc ValidateTools(ValidateToolsRequest) returns (ValidateToolsResponse);
│             rpc ExecuteMultipleTools(ExecuteMultipleToolsRequest) returns (ExecuteMultipleToolsResponse);
│           }
│           
│           // Messages for Definition Fetching (The one MS5 uses)
│           message GetToolDefinitionsRequest {
│             string user_id = 1;
│             repeated string tool_ids = 2;
│           }
│           message GetToolDefinitionsResponse {
│             repeated google.protobuf.Struct definitions = 1;
│           }
│           
│           // Other messages used by other services (for completeness)
│           message ValidateToolsRequest {
│             string user_id = 1;
│             repeated string tool_ids = 2;
│           }
│           message ValidateToolsResponse {
│             bool authorized = 1;
│             string error_message = 2;
│           }
│           message ToolCall {
│               string id = 1;
│               string name = 2;
│               google.protobuf.Struct arguments = 3;
│           }
│           message ExecuteMultipleToolsRequest {
│             repeated ToolCall tool_calls = 1;
│           }
│           message ToolResult {
│               string tool_call_id = 1;
│               string name = 2;
│               string status = 3;
│               string output = 4;
│           }
│           message ExecuteMultipleToolsResponse {
│               repeated ToolResult results = 1;
│           }
│       ]
│   manage.py
│   [
│       #!/usr/bin/env python
│       """Django's command-line utility for administrative tasks."""
│       import os
│       import sys
│       
│       
│       def main():
│           """Run administrative tasks."""
│           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS5.settings')
│           try:
│               from django.core.management import execute_from_command_line
│           except ImportError as exc:
│               raise ImportError(
│                   "Couldn't import Django. Are you sure it's installed and "
│                   "available on your PYTHONPATH environment variable? Did you "
│                   "forget to activate a virtual environment?"
│               ) from exc
│           execute_from_command_line(sys.argv)
│       
│       
│       if __name__ == '__main__':
│           main()
│       
│   ]
├───messaging
│   __init__.py
│   [
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MessagingConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'messaging'
│       
│   ]
│   event_publisher.py
│   [
│       
│       
│       # messaging/event_publisher.py
│       from .rabbitmq_client import rabbitmq_client
│       
│       class InferenceJobPublisher:
│           def publish_job(self, job_payload: dict):
│               rabbitmq_client.publish(
│                   exchange_name='inference_exchange', # Use a dedicated exchange
│                   routing_key='inference.job.start',  # A specific routing key
│                   body=job_payload
│               )
│       
│       inference_job_publisher = InferenceJobPublisher()
│   ]
│   rabbitmq_client.py
│   [
│       # messaging/rabbitmq_client.py (Definitive Resilient Version)
│       
│       import pika
│       import json
│       import threading
│       import time
│       from django.conf import settings
│       
│       class RabbitMQClient:
│           """
│           A robust, thread-safe RabbitMQ client that manages connections on a per-thread
│           basis, uses a fresh channel per operation, and includes an automatic retry
│           mechanism for publishing messages to handle transient network failures.
│           """
│           _thread_local = threading.local()
│       
│           def __init__(self, max_retries=3, retry_delay=2):
│               self.max_retries = max_retries
│               self.retry_delay = retry_delay
│       
│           def _get_connection(self):
│               """
│               Gets or creates a dedicated connection for the current thread.
│               """
│               if not hasattr(self._thread_local, 'connection') or self._thread_local.connection.is_closed:
│                   print(f"Thread {threading.get_ident()}: No active RabbitMQ connection. Creating new one...")
│                   try:
│                       params = pika.URLParameters(settings.RABBITMQ_URL)
│                       self._thread_local.connection = pika.BlockingConnection(params)
│                       print(f"Thread {threading.get_ident()}: Connection successful.")
│                   except pika.exceptions.AMQPConnectionError as e:
│                       print(f"CRITICAL: Thread {threading.get_ident()} failed to connect to RabbitMQ: {e}")
│                       raise
│               return self._thread_local.connection
│       
│           def _invalidate_connection(self):
│               """Forcefully closes and removes the connection for the current thread."""
│               if hasattr(self._thread_local, 'connection') and self._thread_local.connection.is_open:
│                   self._thread_local.connection.close()
│               if hasattr(self._thread_local, 'connection'):
│                   del self._thread_local.connection
│               print(f"Thread {threading.get_ident()}: Invalidated RabbitMQ connection.")
│       
│           def publish(self, exchange_name, routing_key, body):
│               """
│               Publishes a message with a built-in retry mechanism.
│               """
│               attempt = 0
│               while attempt < self.max_retries:
│                   try:
│                       connection = self._get_connection()
│                       with connection.channel() as channel:
│                           channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│                           message_body = json.dumps(body, default=str)
│                           channel.basic_publish(
│                               exchange=exchange_name,
│                               routing_key=routing_key,
│                               body=message_body,
│                               properties=pika.BasicProperties(
│                                   content_type='application/json',
│                                   delivery_mode=pika.DeliveryMode.Persistent,
│                               )
│                           )
│                           print(f" [x] Sent '{routing_key}':'{message_body}' on attempt {attempt + 1}")
│                           return # --- SUCCESS, exit the loop ---
│       
│                   except (pika.exceptions.AMQPError, OSError) as e:
│                       print(f"WARN: Publish attempt {attempt + 1} failed: {e}. Invalidating connection and retrying...")
│                       self._invalidate_connection() # Invalidate the bad connection
│                       attempt += 1
│                       if attempt < self.max_retries:
│                           time.sleep(self.retry_delay) # Wait before next attempt
│                       else:
│                           print(f"CRITICAL: Failed to publish message after {self.max_retries} attempts.")
│                           raise  # Re-raise the final exception
│       
│       # Create a single, globally accessible instance.
│       rabbitmq_client = RabbitMQClient()
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       Django
│       djangorestframework
│       djangorestframework-simplejwt
│       python-dotenv
│       pika
│       
│       PyJWT
│       grpcio
│       grpcio-tools
│       protobuf
│       google-api-python-client
│       dotenv
│       redis
│   ]
