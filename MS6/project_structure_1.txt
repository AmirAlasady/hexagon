│   .env
│   [
│       RABBITMQ_URL=amqp://guest:guest@localhost:5672/
│       TOOL_SERVICE_GRPC_URL=localhost:50057
│       # DATA_SERVICE_URL=http://localhost:50058
│   ]
├───app
│   __init__.py
│   [
│       
│   ]
│   config.py
│   [
│       import os
│       from dotenv import load_dotenv
│       
│       load_dotenv()
│       
│       RABBITMQ_URL = os.getenv("RABBITMQ_URL", "amqp://guest:guest@localhost:5672/")
│       TOOL_SERVICE_GRPC_URL = os.getenv("TOOL_SERVICE_GRPC_URL", "localhost:50057")
│       DATA_SERVICE_URL = os.getenv("DATA_SERVICE_URL")
│   ]
│   ├───execution
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   build_context.py
│   │   [
│   │       from dataclasses import dataclass, field
│   │       from langchain_core.language_models import BaseChatModel
│   │       from langchain_core.memory import BaseMemory
│   │       from langchain_core.tools import BaseTool
│   │       from langchain_core.prompts import ChatPromptTemplate
│   │       from app.execution.job import Job
│   │       
│   │       @dataclass
│   │       class BuildContext:
│   │           """Holds the state of the chain construction process. Each builder populates a field."""
│   │           job: Job
│   │           llm: BaseChatModel = None
│   │           memory: BaseMemory = None
│   │           tools: list[BaseTool] = field(default_factory=list)
│   │           prompt_template: ChatPromptTemplate = None
│   │           on_the_fly_data: list[dict] = field(default_factory=list)
│   │           final_input: dict = field(default_factory=dict)
│   │   ]
│   │   ├───builders
│   │   │   __init__.py
│   │   │   [
│   │   │       
│   │   │   ]
│   │   │   base_builder.py
│   │   │   [
│   │   │       # Abstract base class for all builders
│   │   │       from abc import ABC, abstractmethod
│   │   │       from app.execution.build_context import BuildContext
│   │   │       
│   │   │       class BaseBuilder(ABC):
│   │   │           """Abstract base class for all components in the chain construction pipeline."""
│   │   │           
│   │   │           @abstractmethod
│   │   │           async def build(self, context: BuildContext) -> BuildContext:
│   │   │               """
│   │   │               Processes the input context, adds its component, and returns the modified context.
│   │   │               """
│   │   │               pass
│   │   │   ]
│   │   │   data_builder.py
│   │   │   [
│   │   │       from .base_builder import BaseBuilder
│   │   │       from app.execution.build_context import BuildContext
│   │   │       from app.internals.clients import DataServiceClient
│   │   │       from app.logging_config import logging
│   │   │       import asyncio
│   │   │       
│   │   │       logger = logging.getLogger(__name__)
│   │   │       
│   │   │       class DataBuilder(BaseBuilder):
│   │   │           """Fetches and parses on-the-fly data like user-uploaded files."""
│   │   │           def __init__(self):
│   │   │               self.data_client = DataServiceClient()
│   │   │       
│   │   │           async def build(self, context: BuildContext) -> BuildContext:
│   │   │               if not context.job.inputs:
│   │   │                   return context
│   │   │       
│   │   │               logger.info(f"[{context.job.id}] Fetching content for {len(context.job.inputs)} on-the-fly inputs.")
│   │   │               
│   │   │               fetch_tasks = [
│   │   │                   self.data_client.get_file_content(inp['id'])
│   │   │                   for inp in context.job.inputs if inp.get('type') == 'file_id'
│   │   │               ]
│   │   │               
│   │   │               results = await asyncio.gather(*fetch_tasks)
│   │   │               context.on_the_fly_data = results
│   │   │               
│   │   │               logger.info(f"[{context.job.id}] Successfully fetched {len(results)} on-the-fly data item(s).")
│   │   │               return context
│   │   │   ]
│   │   │   memory_builder.py
│   │   │   [
│   │   │       # Builds the Memory
│   │   │       from .base_builder import BaseBuilder
│   │   │       from app.execution.build_context import BuildContext
│   │   │       from app.logging_config import logger
│   │   │       from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory
│   │   │       from langchain_core.messages import AIMessage, HumanMessage
│   │   │       
│   │   │       class MemoryBuilder(BaseBuilder):
│   │   │           """Instantiates the correct LangChain memory object and loads history."""
│   │   │           async def build(self, context: BuildContext) -> BuildContext:
│   │   │               job = context.job
│   │   │               memory_context = job.memory_context
│   │   │               
│   │   │               if not memory_context or not memory_context.get("bucket_id"):
│   │   │                   return context # No memory configured for this job
│   │   │       
│   │   │               memory_type = job.query.get("resource_overrides", {}).get("memory_type") or memory_context.get("memory_type")
│   │   │               logger.info(f"[{job.id}] Building memory of type: '{memory_type}'.")
│   │   │       
│   │   │               if memory_type == "conversation_buffer_window":
│   │   │                   memory = ConversationBufferWindowMemory(k=10, memory_key="chat_history", return_messages=True)
│   │   │                   # Load history
│   │   │                   for msg in memory_context.get("history", []):
│   │   │                       if msg.get("role") == "user":
│   │   │                           memory.chat_memory.add_message(HumanMessage(content=msg["content"][0]["text"]))
│   │   │                       elif msg.get("role") == "assistant":
│   │   │                           memory.chat_memory.add_message(AIMessage(content=msg["content"][0]["text"]))
│   │   │                   context.memory = memory
│   │   │       
│   │   │               elif memory_type == "summary":
│   │   │                   # In a real system, the summary would be pre-calculated by the memory service
│   │   │                   summary_text = memory_context.get("history", [{}])[0].get("content", "")
│   │   │                   memory = ConversationSummaryMemory(llm=context.llm, memory_key="chat_history", return_messages=True)
│   │   │                   memory.buffer = summary_text
│   │   │                   context.memory = memory
│   │   │               
│   │   │               else:
│   │   │                   logger.warning(f"[{job.id}] Unsupported memory type '{memory_type}'. Skipping.")
│   │   │               
│   │   │               return context
│   │   │   ]
│   │   │   model_builder.py
│   │   │   [
│   │   │       # Builds the LLM
│   │   │       from .base_builder import BaseBuilder
│   │   │       from app.execution.build_context import BuildContext
│   │   │       from app.logging_config import logger
│   │   │       from langchain_openai import ChatOpenAI
│   │   │       from langchain_anthropic import ChatAnthropic
│   │   │       from langchain_google_genai import ChatGoogleGenerativeAI
│   │   │       from langchain_community.chat_models import ChatOllama
│   │   │       
│   │   │       class ModelBuilder(BaseBuilder):
│   │   │           """Instantiates the correct LangChain chat model based on the job's provider."""
│   │   │           async def build(self, context: BuildContext) -> BuildContext:
│   │   │               job = context.job
│   │   │               provider = job.model_config.get("provider")
│   │   │               credentials = job.model_config.get("configuration", {})
│   │   │               final_params = {**job.default_params, **job.param_overrides}
│   │   │               
│   │   │               logger.info(f"[{job.id}] Building LLM for provider: '{provider}'.")
│   │   │       
│   │   │               if provider == "openai":
│   │   │                   context.llm = ChatOpenAI(
│   │   │                       api_key=credentials.get("api_key"),
│   │   │                       model=final_params.pop("model_name", "gpt-4o"),
│   │   │                       streaming=True, **final_params
│   │   │                   )
│   │   │               elif provider == "anthropic":
│   │   │                   context.llm = ChatAnthropic(
│   │   │                       api_key=credentials.get("anthropic_api_key"),
│   │   │                       model=final_params.pop("model_name", "claude-3-opus-20240229"),
│   │   │                       **final_params
│   │   │                   )
│   │   │               elif provider == "google":
│   │   │                   context.llm = ChatGoogleGenerativeAI(
│   │   │                       google_api_key=credentials.get("google_api_key"),
│   │   │                       model=final_params.pop("model_name", "gemini-pro"),
│   │   │                       **final_params
│   │   │                   )
│   │   │               elif provider == "ollama":
│   │   │                   context.llm = ChatOllama(
│   │   │                       base_url=credentials.get("base_url"),
│   │   │                       model=final_params.pop("model_name", "llama3"),
│   │   │                       **final_params
│   │   │                   )
│   │   │               else:
│   │   │                   raise ValueError(f"Unsupported LLM provider: {provider}")
│   │   │               
│   │   │               logger.info(f"[{job.id}] LLM built successfully.")
│   │   │               return context
│   │   │   ]
│   │   │   prompt_builder.py
│   │   │   [
│   │   │       # Builds the Prompt Template
│   │   │       from .base_builder import BaseBuilder
│   │   │       from app.execution.build_context import BuildContext
│   │   │       from app.logging_config import logger
│   │   │       from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
│   │   │       
│   │   │       class PromptBuilder(BaseBuilder):
│   │   │           """Assembles the final prompt template and input variables."""
│   │   │           async def build(self, context: BuildContext) -> BuildContext:
│   │   │               job = context.job
│   │   │               logger.info(f"[{job.id}] Assembling final prompt.")
│   │   │               
│   │   │               # Build context string from RAG and on-the-fly data
│   │   │               context_str = ""
│   │   │               if job.rag_docs:
│   │   │                   context_str += "--- Context from Knowledge Base ---\n"
│   │   │                   for doc in job.rag_docs:
│   │   │                       context_str += f"Content: {doc.get('content')}\n\n"
│   │   │               
│   │   │               if context.on_the_fly_data:
│   │   │                   context_str += "--- Context from Provided Files ---\n"
│   │   │                   for data in context.on_the_fly_data:
│   │   │                       context_str += f"Content: {data.get('content')}\n\n"
│   │   │       
│   │   │               if context_str:
│   │   │                   final_prompt_text = f"{context_str}Based on the context above, please respond to the following:\n\n{job.prompt_text}"
│   │   │               else:
│   │   │                   final_prompt_text = job.prompt_text
│   │   │                   
│   │   │               context.final_prompt_input = {"input": final_prompt_text}
│   │   │       
│   │   │               # Create the prompt template
│   │   │               messages = [("system", "You are a helpful and intelligent AI assistant.")]
│   │   │               if context.memory:
│   │   │                   messages.append(MessagesPlaceholder(variable_name="chat_history"))
│   │   │               
│   │   │               messages.append(("user", "{input}"))
│   │   │               
│   │   │               if context.tools:
│   │   │                   messages.append(MessagesPlaceholder(variable_name="agent_scratchpad"))
│   │   │       
│   │   │               context.prompt_template = ChatPromptTemplate.from_messages(messages)
│   │   │               logger.info(f"[{job.id}] Prompt assembly complete.")
│   │   │               return context
│   │   │   ]
│   │   │   tool_builder.py
│   │   │   [
│   │   │       # This is the file you requested to be commented out but structured.
│   │   │       # We will create a full, working version that uses the gRPC client.
│   │   │       from .base_builder import BaseBuilder
│   │   │       from app.execution.build_context import BuildContext
│   │   │       from app.logging_config import logger
│   │   │       from app.internals.clients import ToolServiceClient
│   │   │       from langchain_core.tools import tool as langchain_tool
│   │   │       from pydantic import BaseModel, Field
│   │   │       
│   │   │       class ToolBuilder(BaseBuilder):
│   │   │           """Creates LangChain-compatible tool objects from definitions."""
│   │   │           def __init__(self):
│   │   │               self.tool_service_client = ToolServiceClient()
│   │   │       
│   │   │           async def build(self, context: BuildContext) -> BuildContext:
│   │   │               if not context.job.tool_definitions:
│   │   │                   logger.info(f"[{context.job.id}] No tools defined. Skipping tool builder.")
│   │   │                   return context
│   │   │               
│   │   │               logger.info(f"[{context.job.id}] Building {len(context.job.tool_definitions)} tools.")
│   │   │               
│   │   │               for definition in context.job.tool_definitions:
│   │   │                   tool_name = definition["name"]
│   │   │                   
│   │   │                   fields = {
│   │   │                       param_name: (str, Field(..., description=schema.get("description")))
│   │   │                       for param_name, schema in definition.get("parameters", {}).get("properties", {}).items()
│   │   │                   }
│   │   │                   
│   │   │                   DynamicArgsSchema = type(f"{tool_name}ArgsSchema", (BaseModel,), fields)
│   │   │       
│   │   │                   # This closure captures the tool_name and the client instance
│   │   │                   async def _execute_tool(**kwargs):
│   │   │                       tool_call = [{"id": "call_1", "name": tool_name, "args": kwargs}]
│   │   │                       logger.info(f"[{context.job.id}] Agent requested to execute tool '{tool_name}' with args: {kwargs}")
│   │   │                       results = await self.tool_service_client.execute_tools(tool_call)
│   │   │                       output = results[0]["output"] if results else f"Error: No result from tool '{tool_name}'."
│   │   │                       logger.info(f"[{context.job.id}] Tool '{tool_name}' returned: {output[:100]}...")
│   │   │                       return output
│   │   │                   
│   │   │                   dynamic_tool = langchain_tool(
│   │   │                       name=tool_name,
│   │   │                       description=definition["description"],
│   │   │                       args_schema=DynamicArgsSchema,
│   │   │                       coroutine=_execute_tool
│   │   │                   )
│   │   │                   context.tools.append(dynamic_tool)
│   │   │       
│   │   │               logger.info(f"[{context.job.id}] Tools built successfully.")
│   │   │               return context
│   │   │   ]
│   │   executor.py
│   │   [
│   │       from langchain.agents import AgentExecutor, create_tool_calling_agent
│   │       from langchain_core.runnables.history import RunnableWithMessageHistory
│   │       from app.logging_config import logger
│   │       from app.execution.job import Job
│   │       from app.execution.build_context import BuildContext
│   │       from app.execution.pipeline import ChainConstructionPipeline
│   │       
│   │       # This is a simple in-memory store. In production, use Redis.
│   │       chat_history_store = {}
│   │       
│   │       class Executor:
│   │           """
│   │           Takes a fully built context and executes the final LangChain runnable.
│   │           """
│   │           async def run(self, context: BuildContext):
│   │               job = context.job
│   │               logger.info(f"[{job.id}] Starting final chain execution.")
│   │       
│   │               # If tools are present, create an agent. Otherwise, create a simple chain.
│   │               if context.tools:
│   │                   agent = create_tool_calling_agent(context.llm, context.tools, context.prompt_template)
│   │                   runnable = AgentExecutor(agent=agent, tools=context.tools, verbose=True)
│   │               else:
│   │                   runnable = context.prompt_template | context.llm
│   │       
│   │               # Add memory to the runnable if it was built
│   │               if context.memory:
│   │                   runnable_with_memory = RunnableWithMessageHistory(
│   │                       runnable,
│   │                       lambda session_id: chat_history_store.setdefault(
│   │                           session_id, context.memory
│   │                       ),
│   │                       input_messages_key="input",
│   │                       history_messages_key="chat_history",
│   │                   )
│   │                   # Invoke with session_id for memory
│   │                   return await runnable_with_memory.ainvoke(
│   │                       context.final_prompt_input,
│   │                       config={"configurable": {"session_id": job.memory_context.get("bucket_id")}}
│   │                   )
│   │               else:
│   │                   # Invoke without memory
│   │                   return await runnable.ainvoke(context.final_prompt_input)
│   │   ]
│   │   job.py
│   │   [
│   │       import uuid
│   │       
│   │       class Job:
│   │           """A data class providing a clean, validated interface to the raw job payload."""
│   │           def __init__(self, payload: dict):
│   │               if not isinstance(payload, dict):
│   │                   raise TypeError("Job payload must be a dictionary.")
│   │               
│   │               self.id = payload.get("job_id", str(uuid.uuid4()))
│   │               self.user_id = payload.get("user_id")
│   │       
│   │               self.query = payload.get("query", {})
│   │               self.prompt_text = self.query.get("prompt", "")
│   │               self.inputs = self.query.get("inputs", [])
│   │               
│   │               self.default_params = payload.get("default_parameters", {})
│   │               self.param_overrides = self.query.get("parameter_overrides", {})
│   │               
│   │               self.output_config = self.query.get("output_config", {})
│   │               self.is_streaming = self.output_config.get("mode") == "streaming"
│   │       
│   │               self.resources = payload.get("resources", {})
│   │               self.model_config = self.resources.get("model_config", {})
│   │               self.tool_definitions = self.resources.get("tools") # Can be None
│   │               self.rag_docs = self.resources.get("rag_context", {}).get("documents", [])
│   │               self.memory_context = self.resources.get("memory_context", {})
│   │           
│   │           @property
│   │           def feedback_ids(self):
│   │               return {
│   │                   "memory_bucket_id": self.memory_context.get("bucket_id"),
│   │                   "rag_collection_id": self.resources.get("rag_context", {}).get("collection_id")
│   │               }
│   │   ]
│   │   pipeline.py
│   │   [
│   │       from app.execution.build_context import BuildContext
│   │       from app.execution.builders.data_builder import DataBuilder
│   │       from app.execution.builders.model_builder import ModelBuilder
│   │       from app.execution.builders.memory_builder import MemoryBuilder
│   │       from app.execution.builders.tool_builder import ToolBuilder
│   │       from app.execution.builders.prompt_builder import PromptBuilder
│   │       
│   │       class ChainConstructionPipeline:
│   │           """Orchestrates the step-by-step construction of a runnable LangChain chain."""
│   │           def __init__(self, context: BuildContext):
│   │               self.context = context
│   │               # The order of builders is critical
│   │               self.pipeline = [
│   │                   DataBuilder(),
│   │                   ModelBuilder(),
│   │                   MemoryBuilder(),
│   │                   ToolBuilder(), # Now included
│   │                   PromptBuilder(),
│   │               ]
│   │       
│   │           async def run(self) -> BuildContext:
│   │               for builder in self.pipeline:
│   │                   self.context = await builder.build(self.context)
│   │               return self.context
│   │   ]
│   ├───internals
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   clients.py
│   │   [
│   │       # gRPC and HTTP clients
│   │       import grpc
│   │       import httpx
│   │       import asyncio
│   │       import json
│   │       from app import config
│   │       from app.internals.generated import tool_pb2, tool_pb2_grpc
│   │       from google.protobuf.json_format import MessageToDict
│   │       from google.protobuf.struct_pb2 import Struct
│   │       from app.logging_config import logging
│   │       
│   │       logger = logging.getLogger(__name__)
│   │       
│   │       class ToolServiceClient:
│   │           """A client for interacting with the gRPC Tool Service."""
│   │           
│   │           async def execute_tools(self, tool_calls: list[dict]) -> list[dict]:
│   │               """
│   │               Executes one or more tools in parallel by calling the Tool Service.
│   │               """
│   │               try:
│   │                   async with grpc.aio.insecure_channel(config.TOOL_SERVICE_GRPC_URL) as channel:
│   │                       stub = tool_pb2_grpc.ToolServiceStub(channel)
│   │                       
│   │                       proto_tool_calls = []
│   │                       for call in tool_calls:
│   │                           arguments = Struct()
│   │                           # LangChain can sometimes pass stringified JSON, so we handle both dicts and strings.
│   │                           args_data = call.get("args", {})
│   │                           if isinstance(args_data, str):
│   │                               try:
│   │                                   args_data = json.loads(args_data)
│   │                               except json.JSONDecodeError:
│   │                                   logger.warning(f"Could not decode string arguments for tool {call.get('name')}: {args_data}")
│   │                                   args_data = {}
│   │                           
│   │                           if isinstance(args_data, dict):
│   │                               arguments.update(args_data)
│   │       
│   │                           proto_tool_calls.append(tool_pb2.ToolCall(
│   │                               id=call.get("id"),
│   │                               name=call.get("name"),
│   │                               arguments=arguments
│   │                           ))
│   │       
│   │                       request = tool_pb2.ExecuteMultipleToolsRequest(tool_calls=proto_tool_calls)
│   │                       response = await stub.ExecuteMultipleTools(request, timeout=30.0)
│   │                       
│   │                       return [
│   │                           {
│   │                               "tool_call_id": res.tool_call_id,
│   │                               "name": res.name,
│   │                               "status": res.status,
│   │                               "output": res.output
│   │                           }
│   │                           for res in response.results
│   │                       ]
│   │               except grpc.aio.AioRpcError as e:
│   │                   logger.error(f"gRPC error executing tools: {e.details()}")
│   │                   # Return an error structure that the agent can understand
│   │                   return [
│   │                       {
│   │                           "tool_call_id": call.get("id"),
│   │                           "name": call.get("name"),
│   │                           "status": "error",
│   │                           "output": f"Error calling tool service: {e.details()}"
│   │                       } for call in tool_calls
│   │                   ]
│   │       
│   │       class DataServiceClient:
│   │           """
│   │           A client for fetching the content of on-the-fly files from a data service.
│   │           This is a placeholder and should be adapted to your actual Data Service (MS-Data/RAG).
│   │           """
│   │           
│   │           async def get_file_content(self, file_id: str) -> dict:
│   │               """
│   │               Fetches and parses a file's content.
│   │               """
│   │               logger.info(f"Fetching content for file_id: {file_id}")
│   │               if not config.DATA_SERVICE_URL:
│   │                   logger.warning("DATA_SERVICE_URL not set. Returning mock data.")
│   │                   await asyncio.sleep(0.1) # Simulate network call
│   │                   return {
│   │                       "source_id": file_id,
│   │                       "type": "text_content",
│   │                       "content": f"This is the mock parsed text content of file {file_id}."
│   │                   }
│   │                   
│   │               try:
│   │                   async with httpx.AsyncClient(timeout=30.0) as client:
│   │                       # This assumes your Data service has an internal endpoint like this
│   │                       response = await client.get(f"{config.DATA_SERVICE_URL}/internal/v1/files/{file_id}/content")
│   │                       response.raise_for_status()
│   │                       return response.json()
│   │               except httpx.RequestError as e:
│   │                   logger.error(f"Could not connect to Data Service to fetch file {file_id}: {e}")
│   │                   return {"source_id": file_id, "type": "error", "content": "Could not connect to Data Service."}
│   │               except httpx.HTTPStatusError as e:
│   │                   logger.error(f"Data Service returned an error for file {file_id}: {e.response.status_code} {e.response.text}")
│   │                   return {"source_id": file_id, "type": "error", "content": f"Error from Data Service: {e.response.status_code}"}
│   │   ]
│   │   ├───generated
│   │   │   __init__.py
│   │   │   [
│   │   │       
│   │   │   ]
│   │   └───protos
│   │       tool.proto
│   │       [
│   │           # (For future tool builder)
│   │           
│   │       ]
│   logging_config.py
│   [
│       import logging
│       import sys
│       
│       def setup_logging():
│           logging.basicConfig(
│               level=logging.INFO,
│               format="%(asctime)s - [%(levelname)s] - %(name)s - %(message)s",
│               stream=sys.stdout,
│           )
│           logging.getLogger("aio_pika").setLevel(logging.WARNING)
│   ]
│   └───messaging
│       __init__.py
│       [
│           
│       ]
│       publisher.py
│       [
│           # MS6/app/messaging/publisher.py
│           
│           from app.logging_config import logger
│           from .rabbitmq_client import rabbitmq_client # <-- Imports the client from the other file
│           
│           class ResultPublisher:
│               """
│               Handles publishing all outgoing messages from the executor by using the
│               thread-safe RabbitMQ client.
│               """
│               def _publish(self, exchange_name, routing_key, body):
│                   try:
│                       rabbitmq_client.publish(exchange_name, routing_key, body)
│                   except Exception as e:
│                       logger.error(f"FINAL ATTEMPT FAILED to publish to exchange '{exchange_name}': {e}", exc_info=True)
│           
│               def publish_stream_chunk(self, job_id, chunk_content):
│                   self._publish(
│                       "results_exchange", 
│                       f"inference.result.streaming.{job_id}",
│                       {"job_id": job_id, "type": "chunk", "content": chunk_content}
│                   )
│               
│               def publish_final_result(self, job_id, result_content):
│                   self._publish(
│                       "results_exchange", 
│                       "inference.result.final", 
│                       {"job_id": job_id, "status": "success", "content": result_content}
│                   )
│           
│               def publish_error_result(self, job_id, error_message):
│                   self._publish(
│                       "results_exchange", 
│                       "inference.result.error", 
│                       {"job_id": job_id, "status": "error", "error": error_message}
│                   )
│           
│               def publish_memory_update(self, job, final_result: str):
│                   memory_ids = job.feedback_ids
│                   bucket_id = memory_ids.get("memory_bucket_id")
│                   
│                   if not bucket_id:
│                       return
│                       
│                   user_message = {"role": "user", "content": [{"type": "text", "text": job.prompt_text}]}
│                   for inp in job.inputs:
│                       if inp.get('type') == 'file_id':
│                           user_message['content'].append({"type": "file_ref", "file_id": inp.get('id')})
│                       elif inp.get('type') == 'image_url':
│                           user_message['content'].append({"type": "image_ref", "url": inp.get('url')})
│           
│                   assistant_message = {
│                       "role": "assistant",
│                       "content": [{"type": "text", "text": final_result}]
│                   }
│                   
│                   update_payload = {
│                       "memory_bucket_id": bucket_id,
│                       "messages_to_add": [user_message, assistant_message]
│                   }
│                   self._publish("memory_exchange", "memory.context.update", update_payload)
│       ]
│       rabbitmq_client.py
│       [
│           # MS6/app/messaging/rabbitmq_client.py
│           
│           import pika
│           import json
│           import threading
│           import time
│           from app import config
│           from app.logging_config import logger
│           
│           class ThreadSafeRabbitMQClient:
│               """
│               A robust, thread-safe RabbitMQ client that manages connections on a per-thread
│               basis. This is essential because the executor will process each job in a new thread,
│               and each thread needs its own connection to RabbitMQ to publish results.
│               """
│               _thread_local = threading.local()
│           
│               def __init__(self, max_retries=3, retry_delay=2):
│                   self.max_retries = max_retries
│                   self.retry_delay = retry_delay
│           
│               def _get_connection(self):
│                   """Gets or creates a dedicated connection for the current thread."""
│                   if not hasattr(self._thread_local, 'connection') or self._thread_local.connection.is_closed:
│                       logger.info(f"Thread {threading.get_ident()}: No active publisher connection. Creating new one...")
│                       params = pika.URLParameters(config.RABBITMQ_URL)
│                       self._thread_local.connection = pika.BlockingConnection(params)
│                       logger.info(f"Thread {threading.get_ident()}: Publisher connection successful.")
│                   return self._thread_local.connection
│           
│               def _invalidate_connection(self):
│                   """Forcefully closes the connection for the current thread."""
│                   if hasattr(self._thread_local, 'connection') and self._thread_local.connection.is_open:
│                       self._thread_local.connection.close()
│                   if hasattr(self._thread_local, 'connection'):
│                       del self._thread_local.connection
│                   logger.warning(f"Thread {threading.get_ident()}: Invalidated publisher connection.")
│           
│               def publish(self, exchange_name, routing_key, body):
│                   """Publishes a message with a built-in retry mechanism."""
│                   attempt = 0
│                   while attempt < self.max_retries:
│                       try:
│                           connection = self._get_connection()
│                           with connection.channel() as channel:
│                               channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│                               message_body = json.dumps(body, default=str).encode('utf-8')
│                               channel.basic_publish(
│                                   exchange=exchange_name,
│                                   routing_key=routing_key,
│                                   body=message_body,
│                                   properties=pika.BasicProperties(
│                                       content_type='application/json',
│                                       delivery_mode=pika.DeliveryMode.PERSISTENT,
│                                   )
│                               )
│                               logger.info(f" [x] Sent to '{routing_key}': '{message_body.decode()[:150]}...' on attempt {attempt + 1}")
│                               return
│                       except (pika.exceptions.AMQPError, OSError) as e:
│                           logger.warning(f"Publish attempt {attempt + 1} failed: {e}. Invalidating connection and retrying...")
│                           self._invalidate_connection()
│                           attempt += 1
│                           if attempt < self.max_retries:
│                               time.sleep(self.retry_delay)
│                           else:
│                               logger.critical(f"Failed to publish message to '{routing_key}' after {self.max_retries} attempts.")
│                               raise
│           
│           # Create a single, globally accessible instance of the client.
│           rabbitmq_client = ThreadSafeRabbitMQClient()
│       ]
│       worker.py
│       [
│           import pika
│           import json
│           import time
│           import threading
│           from app import config
│           from app.logging_config import logger
│           from app.execution.job import Job
│           from app.execution.pipeline import ChainConstructionPipeline
│           from app.execution.executor import Executor
│           from app.messaging.publisher import ResultPublisher
│           from app.execution.build_context import BuildContext
│           
│           class RabbitMQWorker:
│               """
│               Manages a robust, blocking connection to RabbitMQ to consume inference jobs.
│               This runs in its own thread.
│               """
│               def __init__(self):
│                   self.connection = None
│                   self.channel = None
│                   self.result_publisher = None
│           
│               def _connect(self):
│                   """Establishes a connection to RabbitMQ."""
│                   logger.info("Attempting to connect to RabbitMQ...")
│                   params = pika.URLParameters(config.RABBITMQ_URL)
│                   self.connection = pika.BlockingConnection(params)
│                   self.channel = self.connection.channel()
│                   
│                   # Initialize the thread-safe publisher for sending results
│                   self.result_publisher = ResultPublisher()
│           
│                   exchange_name = 'inference_exchange'
│                   self.channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│                   
│                   queue_name = 'inference_jobs_queue'
│                   self.channel.queue_declare(queue=queue_name, durable=True)
│                   self.channel.queue_bind(exchange=exchange_name, queue=queue_name, routing_key='inference.job.start')
│                   
│                   self.channel.basic_qos(prefetch_count=1)
│                   self.channel.basic_consume(queue=queue_name, on_message_callback=self._on_message)
│                   
│                   logger.info("Successfully connected to RabbitMQ and set up consumer.")
│           
│               def _on_message(self, channel, method_frame, header_properties, body):
│                   """Callback executed for each message received."""
│                   job_id = "unknown"
│                   try:
│                       payload = json.loads(body.decode())
│                       job = Job(payload)
│                       job_id = job.id
│                       logger.info(f"[{job_id}] Received new job. Starting processing in a new thread.")
│                       
│                       # --- EXECUTE JOB IN A SEPARATE THREAD ---
│                       # This prevents a long-running AI job from blocking the RabbitMQ connection heartbeat.
│                       processing_thread = threading.Thread(
│                           target=self.process_job,
│                           args=(job,)
│                       )
│                       processing_thread.start()
│                       
│                   except json.JSONDecodeError:
│                       logger.error(f"Message body is not valid JSON: {body.decode()[:200]}...")
│                       self.result_publisher.publish_error_result(job_id, "Invalid job format received.")
│                   except Exception:
│                       logger.error(f"[{job_id}] A critical error occurred before job processing could start.", exc_info=True)
│                       self.result_publisher.publish_error_result(job_id, "An unexpected internal worker error occurred.")
│                   
│                   # Acknowledge the message now that the job has been handed off
│                   channel.basic_ack(delivery_tag=method_frame.delivery_tag)
│           
│               def process_job(self, job: Job):
│                   """
│                   The target function for the processing thread. This is where the real work happens.
│                   """
│                   try:
│                       # The Executor now needs to be run with asyncio
│                       import asyncio
│                       asyncio.run(self._run_async_executor(job))
│                   except Exception:
│                       logger.error(f"[{job.id}] Unhandled exception in async job processor.", exc_info=True)
│                       self.result_publisher.publish_error_result(job.id, "A fatal error occurred during async execution.")
│               
│               async def _run_async_executor(self, job: Job):
│                   """Helper to run the async executor logic from a sync thread."""
│                   logger.info(f"[{job.id}] Async execution started.")
│                   build_context = BuildContext(job)
│                   pipeline = ChainConstructionPipeline(build_context)
│                   final_context = await pipeline.run()
│                   executor = Executor(final_context, self.result_publisher)
│                   await executor.run()
│                   logger.info(f"[{job.id}] Async execution finished.")
│           
│           
│               def run(self):
│                   """Starts the worker and enters a reconnect loop."""
│                   while True:
│                       try:
│                           self._connect()
│                           logger.info(" [*] Inference Executor Worker is waiting for jobs. To exit press CTRL+C")
│                           self.channel.start_consuming()
│                       except KeyboardInterrupt:
│                           logger.info("Worker shutting down gracefully.")
│                           if self.connection and self.connection.is_open:
│                               self.connection.close()
│                           break
│                       except pika.exceptions.AMQPConnectionError as e:
│                           logger.error(f"RabbitMQ connection lost: {e}. Reconnecting in 5 seconds...")
│                           time.sleep(5)
│       ]
│   generate_protos.py
│   [
│       import os
│       import subprocess
│       import fileinput
│       import sys
│       from pathlib import Path
│       
│       def main():
│           """Generates and fixes gRPC stubs for the executor."""
│           root_dir = Path(__file__).parent
│           proto_path = root_dir / 'app' / 'internals' / 'protos'
│           output_path = root_dir / 'app' / 'internals' / 'generated'
│           
│           print(f"Project root directory: {root_dir}")
│           print(f"Proto source directory: {proto_path}")
│           print(f"Generated code output directory: {output_path}")
│       
│           if not proto_path.is_dir():
│               print(f"ERROR: Proto path '{proto_path}' does not exist.", file=sys.stderr)
│               sys.exit(1)
│       
│           output_path.mkdir(parents=True, exist_ok=True)
│           (output_path / '__init__.py').touch()
│       
│           proto_files = [f for f in proto_path.iterdir() if f.suffix == '.proto']
│           if not proto_files:
│               print('No .proto files found. Exiting.')
│               return
│               
│           command = [
│               sys.executable,  # Use the same python interpreter running the script
│               '-m',
│               'grpc_tools.protoc',
│               f'--proto_path={proto_path}',
│               f'--python_out={output_path}',
│               f'--grpc_python_out={output_path}',
│           ] + [str(pf) for pf in proto_files]
│       
│           print(f"Running command: {' '.join(command)}")
│           try:
│               subprocess.run(command, check=True, capture_output=True, text=True)
│           except subprocess.CalledProcessError as e:
│               print("ERROR: Failed to generate gRPC stubs.", file=sys.stderr)
│               print(e.stderr, file=sys.stderr)
│               sys.exit(1)
│       
│           print('Successfully generated gRPC stubs. Now fixing imports...')
│           for proto_file in proto_files:
│               base_name = proto_file.stem
│               grpc_file_path = output_path / f'{base_name}_pb2_grpc.py'
│               
│               with fileinput.FileInput(str(grpc_file_path), inplace=True) as file:
│                   for line in file:
│                       if line.strip() == f'import {base_name}_pb2 as {base_name}__pb2':
│                           print(f'from . import {base_name}_pb2 as {base_name}__pb2', end='\n')
│                       else:
│                           print(line, end='')
│       
│           print('Imports fixed successfully.')
│       
│       if __name__ == '__main__':
│           main()
│   ]
│   main.py
│   [
│       import asyncio
│       from app.logging_config import setup_logging, logger
│       from app.messaging.worker import RabbitMQWorker
│       
│       def main():
│           """
│           The main entry point for the Inference Executor (MS6) application.
│           """
│           setup_logging()
│           worker = RabbitMQWorker()
│           
│           try:
│               logger.info("Starting Inference Executor Worker...")
│               asyncio.run(worker.run())
│           except KeyboardInterrupt:
│               logger.info("Executor shutting down gracefully due to user request (CTRL+C).")
│           except Exception as e:
│               # This catches errors during initial connection
│               logger.critical(f"FATAL: Worker crashed during startup: {e}", exc_info=True)
│       
│       if __name__ == "__main__":
│           main()
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       aio-pika
│       grpcio
│       grpcio-tools
│       protobuf
│       google-api-python-client
│       python-dotenv
│       httpx
│       pydantic
│       langchain
│       langchain-core
│       langchain-openai
│       langchain-anthropic
│       langchain-google-genai
│       langchain-community
│   ]
