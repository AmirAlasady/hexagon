│   .env
│   [
│       # MS8/.env
│       
│       # The address of the Inference Service (MS5) for validating websocket tickets
│       INFERENCE_SERVICE_URL=http://localhost:8004
│       
│       # RabbitMQ connection URL
│       RABBITMQ_URL=amqp://guest:guest@localhost:5672/
│       
│       # A shared secret between MS8 and MS5 for securing the internal ticket validation call
│       INTERNAL_API_SECRET_KEY="a_very_strong_and_long_secret_for_service_to_service_auth"
│       REDIS_URL="redis://localhost:6379/0"
│       
│   ]
├───app
│   __init__.py
│   [
│       
│   ]
│   config.py
│   [
│       import os
│       import redis
│       from dotenv import load_dotenv
│       
│       load_dotenv()
│       
│       RABBITMQ_URL = os.getenv("RABBITMQ_URL", "amqp://guest:guest@localhost:5672/")
│       REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
│       
│       # Create a global Redis client instance
│       redis_client = redis.from_url(REDIS_URL, decode_responses=True)
│   ]
│   logging_config.py
│   [
│       # MS8/app/logging_config.py
│       
│       import logging
│       import sys
│       
│       # --- THE DEFINITIVE FIX ---
│       # 1. Define the logger at the top level of the module so it can be imported.
│       logger = logging.getLogger("MS8-ResultsService")
│       # --- END OF FIX ---
│       
│       def setup_logging():
│           """
│           Configures the root logger for the application.
│           This function should be called ONLY ONCE at startup in main.py.
│           """
│           logging.basicConfig(
│               level=logging.INFO,
│               format="%(asctime)s - %(name)s - [%(levelname)s] - %(message)s",
│               stream=sys.stdout,
│           )
│           # Silence noisy libraries
│           logging.getLogger("aio_pika").setLevel(logging.WARNING)
│           logging.getLogger("aiormq").setLevel(logging.WARNING)
│           logging.getLogger("websockets").setLevel(logging.INFO)
│   ]
│   ├───messaging
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   rabbitmq_consumer.py
│   │   [
│   │       import asyncio
│   │       import json
│   │       import aio_pika
│   │       from app import config
│   │       from app.logging_config import logger
│   │       from app.server.connection_manager import manager
│   │       
│   │       class RabbitMQConsumer:
│   │           """
│   │           Consumes messages from the results exchange and routes them
│   │           to the correct WebSocket via the ConnectionManager.
│   │           """
│   │           async def run(self):
│   │               while True:
│   │                   try:
│   │                       connection = await aio_pika.connect_robust(config.RABBITMQ_URL)
│   │                       async with connection:
│   │                           channel = await connection.channel()
│   │                           
│   │                           exchange_name = 'results_exchange'
│   │                           await channel.declare_exchange(exchange_name, aio_pika.ExchangeType.TOPIC, durable=True)
│   │                           
│   │                           # Declare an exclusive queue. If this worker dies, the queue is deleted.
│   │                           # When it restarts, it gets a fresh one. Good for load balancing.
│   │                           queue = await channel.declare_queue(exclusive=True)
│   │                           
│   │                           # Listen for all result messages
│   │                           await queue.bind(exchange_name, 'inference.result.#')
│   │                           
│   │                           logger.info(" [*] RabbitMQ consumer is waiting for result messages.")
│   │                           await queue.consume(self.on_message)
│   │                           
│   │                           await asyncio.Event().wait()
│   │                   except aio_pika.exceptions.AMQPConnectionError as e:
│   │                       logger.error(f"RabbitMQ connection lost: {e}. Retrying in 5 seconds...")
│   │                       await asyncio.sleep(5)
│   │       
│   │           async def on_message(self, message: aio_pika.IncomingMessage):
│   │               """Callback for processing a message from the results queue."""
│   │               async with message.process():
│   │                   try:
│   │                       body = json.loads(message.body.decode())
│   │                       job_id = body.get("job_id")
│   │                       
│   │                       if not job_id:
│   │                           logger.warning(f"Received message without job_id: {body}")
│   │                           return
│   │       
│   │                       await manager.send_message(job_id, body)
│   │       
│   │                       # If the job is finished, close the connection
│   │                       if body.get("status") in ["success", "error"]:
│   │                           await manager.close_connection(job_id)
│   │       
│   │                   except json.JSONDecodeError:
│   │                       logger.error(f"Could not decode result message body: {message.body.decode()[:200]}")
│   │                   except Exception as e:
│   │                       logger.error("Error processing result message", exc_info=True)
│   │   ]
│   └───server
│       __init__.py
│       [
│           
│       ]
│       connection_manager.py
│       [
│           from fastapi import WebSocket
│           from typing import Dict
│           from app.logging_config import logging
│           
│           logger = logging.getLogger(__name__)
│           
│           class ConnectionManager:
│               """
│               Manages active WebSocket connections, mapping job_ids to WebSocket objects.
│               This class is a singleton, ensuring a single state across the application.
│               """
│               def __init__(self):
│                   self.active_connections: Dict[str, WebSocket] = {}
│           
│               async def connect(self, websocket: WebSocket, job_id: str):
│                   """Accepts a new connection and maps it to a job_id."""
│                   await websocket.accept()
│                   self.active_connections[job_id] = websocket
│                   logger.info(f"WebSocket connected for job_id: {job_id}. Total connections: {len(self.active_connections)}")
│           
│               def disconnect(self, job_id: str):
│                   """Removes a connection from the manager."""
│                   if job_id in self.active_connections:
│                       del self.active_connections[job_id]
│                       logger.info(f"WebSocket disconnected for job_id: {job_id}. Total connections: {len(self.active_connections)}")
│           
│               async def send_message(self, job_id: str, message: dict):
│                   """Sends a JSON message to a specific client by job_id."""
│                   websocket = self.active_connections.get(job_id)
│                   if websocket:
│                       try:
│                           await websocket.send_json(message)
│                           logger.debug(f"Sent message to job_id {job_id}: {str(message)[:100]}...")
│                       except Exception as e:
│                           logger.warning(f"Could not send message to job_id {job_id} (client may have disconnected): {e}")
│                           self.disconnect(job_id)
│               
│               async def close_connection(self, job_id: str, reason: str = "Job finished"):
│                   """Closes a specific connection from the server side."""
│                   websocket = self.active_connections.get(job_id)
│                   if websocket:
│                       await websocket.close(code=1000, reason=reason)
│                       self.disconnect(job_id)
│           
│           # Create a single global instance of the manager
│           manager = ConnectionManager()
│       ]
│       routes.py
│       [
│           # MS8/app/server/routes.py
│           from fastapi import APIRouter, WebSocket, WebSocketDisconnect, Query
│           from typing import Optional
│           import json
│           from app.server.connection_manager import manager
│           from app.config import redis_client
│           from app.logging_config import logger
│           
│           router = APIRouter()
│           
│           def validate_and_consume_ticket(ticket: str) -> Optional[dict]:
│               """
│               Checks Redis for a ticket. If found, it returns the data and
│               immediately deletes the ticket to ensure it's single-use.
│               """
│               redis_key = f"ws_ticket:{ticket}"
│               try:
│                   # Use a pipeline for an atomic GET and DEL operation
│                   pipe = redis_client.pipeline()
│                   pipe.get(redis_key)
│                   pipe.delete(redis_key)
│                   results = pipe.execute()
│                   
│                   ticket_data_str = results[0]
│           
│                   if ticket_data_str:
│                       ticket_data = json.loads(ticket_data_str)
│                       logger.info(f"Ticket validation successful for job_id: {ticket_data.get('job_id')}")
│                       return ticket_data
│                   else:
│                       logger.warning(f"Ticket '{ticket}' not found in Redis.")
│                       return None
│               except Exception as e:
│                   logger.error(f"Redis error during ticket validation: {e}", exc_info=True)
│                   return None
│           
│           @router.websocket("/ws/results/")
│           async def websocket_endpoint(websocket: WebSocket, ticket: Optional[str] = Query(None)):
│               if not ticket:
│                   await websocket.close(code=4001, reason="Ticket query parameter is required.")
│                   return
│           
│               ticket_data = validate_and_consume_ticket(ticket)
│               if not ticket_data:
│                   await websocket.close(code=4003, reason="Invalid, expired, or already used ticket.")
│                   return
│               
│               job_id = ticket_data.get("job_id")
│               # user_id = ticket_data.get("user_id") # You can use this for logging or further auth
│               
│               await manager.connect(websocket, job_id)
│               
│               try:
│                   # Keep the connection alive by listening for messages from the client.
│                   # This loop will break if the client disconnects.
│                   while True:
│                       await websocket.receive_text()
│               except WebSocketDisconnect:
│                   manager.disconnect(job_id)
│       ]
│   main.py
│   [
│       # MS8/main.py
│       
│       from fastapi import FastAPI
│       import asyncio
│       import uvicorn
│       from app.logging_config import setup_logging, logger # <-- This import now works
│       from app.server.routes import router as websocket_router
│       from app.messaging.rabbitmq_consumer import RabbitMQConsumer
│       
│       # Create the FastAPI app instance BEFORE the startup event
│       app = FastAPI(title="Real-time Results Service")
│       
│       @app.on_event("startup")
│       async def startup_event():
│           """On startup, configure logging and create the background consumer task."""
│           setup_logging() # Configure the logger
│           logger.info("Application startup...")
│           consumer = RabbitMQConsumer()
│           # Create a background task that will run the consumer loop indefinitely
│           asyncio.create_task(consumer.run())
│           logger.info("RabbitMQ consumer background task created.")
│       
│       # Include the WebSocket router
│       app.include_router(websocket_router)
│       
│       @app.get("/health", tags=["System"])
│       def health_check():
│           """A simple health check endpoint."""
│           return {"status": "ok"}
│       
│       # This block allows running the server directly with `python main.py`
│       if __name__ == "__main__":
│           uvicorn.run("main:app", host="0.0.0.0", port=8008, reload=True)
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       fastapi
│       uvicorn[standard]  # High-performance ASGI server
│       websockets
│       aio-pika           # Async RabbitMQ library
│       python-dotenv
│       httpx              # For calling the validation endpoint on MS5
│       redis
│   ]
