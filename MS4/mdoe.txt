│   .env
│   [
│       # Django's main secret key
│       DJANGO_SECRET_KEY='django-inergegbnm546334rg56564ldpcuck6bytc4h1*8v!=8(_wau6g8or'
│       JWT_SECRET_KEY ='jwt-secure-m3x$DFGRTJRTYNEHRETNEFDDHD43.m<?><DFGRTJYRJGc4h1*8v!=8(_wau6g8or'
│       # You can also add other environment-specific settings here
│       DJANGO_DEBUG='True'
│       DATABASE_URL='sqlite:///./db.sqlite3' # Example for database config
│       JWT_ISSUER="https://ms1.auth-service.com"
│       
│       
│       
│       
│       # custom attributes for Node Service
│       NODE_SERVICE_VALIDATION_ENABLED=True
│       
│       # in .env file for the Node Service
│       # The base URL for the entire Project Service application.
│       PROJECT_SERVICE_URL=http://localhost:8001
│       
│       # The base URL for the entire AIModel Service application.
│       MODEL_SERVICE_URL=http://localhost:8002
│       
│       # The base URL for the entire Memory Service application.
│       MEMORY_SERVICE_URL=non
│       
│       # The base URL for the entire Tool Service application.
│       TOOL_SERVICE_URL=http://localhost:8007
│       
│       
│       # RabbitMQ connection string for the Node Service
│       RABBITMQ_URL='amqp://guest:guest@localhost:5672/'
│   ]
├───MS4
│   __init__.py
│   [
│       
│   ]
│   asgi.py
│   [
│       """
│       ASGI config for MS4 project.
│       
│       It exposes the ASGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
│       """
│       
│       import os
│       
│       from django.core.asgi import get_asgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│       
│       application = get_asgi_application()
│       
│   ]
│   settings.py
│   [
│       
│       from datetime import timedelta
│       from pathlib import Path
│       import os
│       
│       
│       #======================================================
│       NODE_SERVICE_VALIDATION_ENABLED = os.getenv('NODE_SERVICE_VALIDATION_ENABLED', 'True').lower() == 'true'
│       #======================================================
│       
│       
│       # Build paths inside the project like this: BASE_DIR / 'subdir'.
│       BASE_DIR = Path(__file__).resolve().parent.parent
│       
│       from dotenv import load_dotenv
│       load_dotenv(BASE_DIR / '.env')
│       # Quick-start development settings - unsuitable for production
│       # See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/
│       
│       SECRET_KEY = os.getenv('DJANGO_SECRET_KEY')
│       if not SECRET_KEY:
│           # This fallback should ideally not be hit if .env is loaded correctly
│           # or if the environment variable is set directly in the deployment environment.
│           SECRET_KEY = 'django-insecure-fallback-dev-key-!!change-me!!'
│           print("WARNING: DJANGO_SECRET_KEY not found in environment or .env. Using fallback. THIS IS INSECURE FOR PRODUCTION.")
│       
│       DEBUG = os.getenv('DJANGO_DEBUG', 'True').lower() in ('true', '1', 't')
│       
│       ALLOWED_HOSTS = ['*']
│       
│       
│       # Application definition
│       
│       INSTALLED_APPS = [
│           'django.contrib.admin',
│           'django.contrib.auth',
│           'django.contrib.contenttypes',
│           'django.contrib.sessions',
│           'django.contrib.messages',
│           'django.contrib.staticfiles',
│           'rest_framework',
│           'rest_framework_simplejwt',
│           'nodes',  # Your custom app
│           'messaging',
│       ]
│       
│       MIDDLEWARE = [
│           'django.middleware.security.SecurityMiddleware',
│           'django.contrib.sessions.middleware.SessionMiddleware',
│           'django.middleware.common.CommonMiddleware',
│           'django.middleware.csrf.CsrfViewMiddleware',
│           'django.contrib.auth.middleware.AuthenticationMiddleware',
│           'django.contrib.messages.middleware.MessageMiddleware',
│           'django.middleware.clickjacking.XFrameOptionsMiddleware',
│       ]
│       
│       ROOT_URLCONF = 'MS4.urls'
│       
│       TEMPLATES = [
│           {
│               'BACKEND': 'django.template.backends.django.DjangoTemplates',
│               'DIRS': [],
│               'APP_DIRS': True,
│               'OPTIONS': {
│                   'context_processors': [
│                       'django.template.context_processors.request',
│                       'django.contrib.auth.context_processors.auth',
│                       'django.contrib.messages.context_processors.messages',
│                   ],
│               },
│           },
│       ]
│       
│       WSGI_APPLICATION = 'MS4.wsgi.application'
│       
│       
│       # Database
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#databases
│       
│       DATABASES = {
│           'default': {
│               'ENGINE': 'django.db.backends.sqlite3',
│               'NAME': BASE_DIR / 'db.sqlite3',
│           }
│       }
│       
│       
│       # Password validation
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators
│       
│       AUTH_PASSWORD_VALIDATORS = [
│           {
│               'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
│           },
│       ]
│       
│       
│       # Internationalization
│       # https://docs.djangoproject.com/en/5.2/topics/i18n/
│       
│       LANGUAGE_CODE = 'en-us'
│       
│       TIME_ZONE = 'UTC'
│       
│       USE_I18N = True
│       
│       USE_TZ = True
│       
│       
│       # Static files (CSS, JavaScript, Images)
│       # https://docs.djangoproject.com/en/5.2/howto/static-files/
│       
│       STATIC_URL = '/static/'
│       STATIC_ROOT = BASE_DIR / 'staticfiles'
│       
│       # Default primary key field type
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field
│       
│       DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
│       
│       
│       
│       JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY')
│       
│       # REST Framework
│       REST_FRAMEWORK = {
│           "DEFAULT_PERMISSION_CLASSES": ["rest_framework.permissions.IsAuthenticated"],
│           "DEFAULT_AUTHENTICATION_CLASSES": (
│       
│          
│               "nodes.custom_auth.ForceTokenUserJWTAuthentication", # <<< YOUR CUSTOM AUTH CLASS
│           ),
│           
│           'DEFAULT_THROTTLE_CLASSES': (
│               'rest_framework.throttling.AnonRateThrottle',
│               'rest_framework.throttling.UserRateThrottle'
│           ),
│           'DEFAULT_THROTTLE_RATES': {
│               'anon': '100/day',  # Adjust as needed for unauthenticated requests
│               'user': '20000/day' # Adjust as needed for authenticated requests
│           }
│       }
│       
│       
│       SIMPLE_JWT = {
│       
│           "SIGNING_KEY": JWT_SECRET_KEY,  # <<< USE DJANGO'S SECRET_KEY LOADED FROM ENV
│           "VERIFYING_KEY": JWT_SECRET_KEY,
│           "ISSUER": os.getenv('JWT_ISSUER', "https://ms1.auth-service.com"), # MUST match MS1's issuer
│           "AUTH_HEADER_TYPES": ("Bearer",),
│           "ACCESS_TOKEN_LIFETIME": timedelta(minutes=60), # e.g., 1 hour
│           "REFRESH_TOKEN_LIFETIME": timedelta(days=1),    # e.g., 1 day
│           "LEEWAY": timedelta(seconds=10),
│           "ALGORITHM": "HS256",
│           
│           # --- Settings related to interpreting the token payload ---
│           """
│       "USER_ID_CLAIM": "user_id": (Your Specific Question)
│        This is a critical instruction. It tells simple-jwt:
│          "When you parse the token's payload (the data inside),
│            the claim that contains the user's primary identifier is named 'user_id'."
│              Your MS1's CustomTokenObtainPairSerializer probably adds a claim with this name.
│           """
│       
│           "USER_ID_CLAIM": "user_id",
│           "USER_ID_FIELD": "id",
│           "TOKEN_USER_CLASS": "rest_framework_simplejwt.models.TokenUser", # Explicitly use TokenUse
│       
│           # --- Settings for features MS2 likely DOES NOT use ---
│           "UPDATE_LAST_LOGIN": False,
│           "ROTATE_REFRESH_TOKENS": False,
│           "BLACKLIST_AFTER_ROTATION": False, 
│       
│       }
│       
│       RABBITMQ_URL = os.getenv('RABBITMQ_URL', 'amqp://guest:guest@localhost:5672/')
│   ]
│   urls.py
│   [
│       
│       from django.contrib import admin
│       from django.urls import path, include
│       
│       urlpatterns = [
│           # Wrap all paths under the 'ms4/' prefix
│           path('ms4/', include([
│               path('admin/', admin.site.urls),
│               path('api/v1/', include('nodes.urls')),
│           ]))
│       ]
│   ]
│   wsgi.py
│   [
│       """
│       WSGI config for MS4 project.
│       
│       It exposes the WSGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
│       """
│       
│       import os
│       
│       from django.core.wsgi import get_wsgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│       
│       application = get_wsgi_application()
│       
│   ]
│   db.sqlite3
│   [
│       [Binary file - content not shown]
│   ]
│   manage.py
│   [
│       #!/usr/bin/env python
│       """Django's command-line utility for administrative tasks."""
│       import os
│       import sys
│       
│       
│       def main():
│           """Run administrative tasks."""
│           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│           try:
│               from django.core.management import execute_from_command_line
│           except ImportError as exc:
│               raise ImportError(
│                   "Couldn't import Django. Are you sure it's installed and "
│                   "available on your PYTHONPATH environment variable? Did you "
│                   "forget to activate a virtual environment?"
│               ) from exc
│           execute_from_command_line(sys.argv)
│       
│       
│       if __name__ == '__main__':
│           main()
│       
│   ]
├───messaging
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       
│       # Register your models here.
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MessagingConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'messaging'
│       
│   ]
│   event_publisher.py
│   [
│       # This file will be very similar to the one in Project Service
│       # but it publishes a different event.
│       
│       import json
│       import pika
│       from django.conf import settings
│       
│       # A simplified, direct publisher for this worker's specific need
│       def publish_event(exchange_name, routing_key, body):
│           # In a real app, you'd use a shared client, but this is simple and clear.
│           params = pika.URLParameters(settings.RABBITMQ_URL)
│           connection = pika.BlockingConnection(params)
│           channel = connection.channel()
│       
│           # Ensure exchange exists
│           channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│           
│           channel.basic_publish(
│               exchange=exchange_name,
│               routing_key=routing_key,
│               body=json.dumps(body),
│               properties=pika.BasicProperties(content_type='application/json', delivery_mode=2)
│           )
│           print(f" [x] Client Sent '{routing_key}':'{json.dumps(body)}'")
│           connection.close()
│       
│       class NodeEventPublisher:
│           def publish_nodes_for_project_deleted(self, project_id: str):
│               event_name = "resource.for_project.deleted.NodeService" # Very specific routing key
│               payload = {
│                   "project_id": str(project_id),
│                   "service_name": "NodeService" # Self-identifies who is confirming
│               }
│               publish_event(
│                   exchange_name='project_events',
│                   routing_key=event_name,
│                   body=payload
│               )
│       
│       # Create an instance for our worker to use
│       node_event_publisher = NodeEventPublisher()
│   ]
│   ├───management
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   └───commands
│   │       __init__.py
│   │       [
│   │           
│   │       ]
│   │       generate_protos.py
│   │       [
│   │           import os
│   │           import subprocess
│   │           import fileinput
│   │           from django.core.management.base import BaseCommand
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Generates Python gRPC code from .proto files for the Node Service.'
│   │           
│   │               def handle(self, *args, **options):
│   │                   proto_path = 'nodes/protos'
│   │                   output_path = 'nodes'
│   │                   
│   │                   # ... (Identical logic to the MS3 version) ...
│   │                   proto_files = [f for f in os.listdir(proto_path) if f.endswith('.proto')]
│   │                   command = [
│   │                       'python', '-m', 'grpc_tools.protoc',
│   │                       f'--proto_path={proto_path}',
│   │                       f'--python_out={output_path}',
│   │                       f'--grpc_python_out={output_path}',
│   │                   ] + proto_files
│   │           
│   │                   subprocess.run(command, check=True)
│   │                   self.stdout.write(self.style.SUCCESS('Successfully generated gRPC Python stubs for Node Service.'))
│   │                   
│   │                   # --- THE FIX IS HERE ---
│   │                   for proto_file in proto_files:
│   │                       base_name = proto_file.replace('.proto', '')
│   │                       grpc_file_path = os.path.join(output_path, f'{base_name}_pb2_grpc.py')
│   │                       
│   │                       self.stdout.write(f"Fixing imports in {grpc_file_path}...")
│   │                       with fileinput.FileInput(grpc_file_path, inplace=True) as file:
│   │                           for line in file:
│   │                               if line.strip() == f'import {base_name}_pb2 as {base_name}__pb2':
│   │                                   print(f'from . import {base_name}_pb2 as {base_name}__pb2')
│   │                               else:
│   │                                   print(line, end='')
│   │                       self.stdout.write(self.style.SUCCESS('Imports fixed.'))
│   │                   # --- END OF FIX ---
│   │       ]
│   │       run_grpc_server.py
│   │       [
│   │           import grpc
│   │           from concurrent import futures
│   │           import time
│   │           from django.core.management.base import BaseCommand
│   │           import django
│   │           import os
│   │           
│   │           # Setup Django environment
│   │           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│   │           django.setup()
│   │           
│   │           from nodes import node_pb2_grpc
│   │           from nodes.servicer import NodeServicer
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Starts the gRPC server for the Node Service'
│   │           
│   │               def handle(self, *args, **options):
│   │                   self.stdout.write("Starting Node Service gRPC server on port 50051...")
│   │                   
│   │                   server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
│   │                   
│   │                   # Attach your servicer to the server
│   │                   node_pb2_grpc.add_NodeServiceServicer_to_server(NodeServicer(), server)
│   │                   
│   │                   # Start listening
│   │                   server.add_insecure_port('[::]:50051')
│   │                   server.start()
│   │                   self.stdout.write(self.style.SUCCESS('Node Service gRPC server started successfully.'))
│   │                   
│   │                   try:
│   │                       while True:
│   │                           time.sleep(86400) # One day
│   │                   except KeyboardInterrupt:
│   │                       self.stdout.write(self.style.WARNING('Stopping gRPC server...'))
│   │                       server.stop(0)
│   │       ]
│   │       run_project_cleanup_worker.py
│   │       [
│   │           import pika
│   │           import json
│   │           import time
│   │           from django.core.management.base import BaseCommand
│   │           from nodes.models import Node # Import your Node model
│   │           from messaging.event_publisher import node_event_publisher
│   │           from django.conf import settings
│   │           
│   │           def handle_project_deletion(project_id: str):
│   │               """
│   │               The core business logic for cleaning up nodes.
│   │               This is idempotent.
│   │               """
│   │               print(f" [!] Received request to delete nodes for project: {project_id}")
│   │               
│   │               # Use the Django ORM to delete all nodes belonging to this project
│   │               nodes_deleted, _ = Node.objects.filter(project_id=project_id).delete()
│   │               
│   │               print(f" [✓] Deleted {nodes_deleted} nodes for project {project_id}.")
│   │               
│   │               # After successful deletion, publish the confirmation event.
│   │               node_event_publisher.publish_nodes_for_project_deleted(project_id)
│   │           
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Runs a RabbitMQ worker to listen for project deletion events.'
│   │           
│   │               def handle(self, *args, **options):
│   │                   connection = pika.BlockingConnection(pika.URLParameters(settings.RABBITMQ_URL))
│   │                   channel = connection.channel()
│   │           
│   │                   channel.exchange_declare(exchange='project_events', exchange_type='topic', durable=True)
│   │                   
│   │                   # Create an exclusive queue for this worker. When the worker disconnects, the queue is deleted.
│   │                   # Or, create a named, durable queue if you want messages to persist while the worker is down.
│   │                   result = channel.queue_declare(queue='', exclusive=True)
│   │                   queue_name = result.method.queue
│   │           
│   │                   # Listen for the specific event
│   │                   routing_key = 'project.deletion.initiated'
│   │                   channel.queue_bind(exchange='project_events', queue=queue_name, routing_key=routing_key)
│   │           
│   │                   self.stdout.write(' [*] NodeService cleanup worker waiting for messages. To exit press CTRL+C')
│   │           
│   │                   def callback(ch, method, properties, body):
│   │                       data = json.loads(body)
│   │                       project_id = data.get('project_id')
│   │           
│   │                       if project_id:
│   │                           try:
│   │                               handle_project_deletion(project_id)
│   │                           except Exception as e:
│   │                               self.stderr.write(f" [!] Error handling project deletion for {project_id}: {e}")
│   │                               # In production, you would add nack logic and a Dead Letter Queue here.
│   │                       
│   │                       ch.basic_ack(delivery_tag=method.delivery_tag)
│   │           
│   │                   channel.basic_consume(queue=queue_name, on_message_callback=callback)
│   │                   channel.start_consuming()
│   │       ]
│   models.py
│   [
│       from django.db import models
│       
│       # Create your models here.
│       
│   ]
│   tests.py
│   [
│       from django.test import TestCase
│       
│       # Create your tests here.
│       
│   ]
│   views.py
│   [
│       from django.shortcuts import render
│       
│       # Create your views here.
│       
│   ]
├───nodes
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       from .models import Node
│       # Register your models here.
│       admin.site.register(Node)
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class NodesConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'nodes'
│       
│   ]
│   clients.py
│   [
│       # in nodes/clients.py
│       
│       import httpx
│       import os
│       from django.core.exceptions import ImproperlyConfigured
│       from rest_framework.exceptions import PermissionDenied, ValidationError, NotFound
│       
│       class BaseServiceClient:
│           def __init__(self, service_name: str, env_var_name: str):
│               self.service_name = service_name
│               base_url = os.getenv(env_var_name)
│               if not base_url:
│                   raise ImproperlyConfigured(f"{env_var_name} is not set in the environment.")
│               self.client = httpx.Client(base_url=base_url, timeout=10.0)
│       
│           def _handle_response(self, response: httpx.Response):
│               """
│               A centralized function to interpret HTTP responses from other services
│               and raise appropriate DRF exceptions. This version is robust and can
│               handle both dictionary and list-based error responses.
│               """
│               if 200 <= response.status_code < 300:
│                   return response.json() if response.content else None
│               
│               # --- THE FIX IS HERE ---
│               try:
│                   error_data = response.json()
│               except Exception:
│                   # If the response isn't valid JSON, use the reason phrase.
│                   error_data = response.reason_phrase
│       
│               error_message = f"Error from {self.service_name}"
│               if isinstance(error_data, dict):
│                   # Handle standard DRF error format: {"detail": "..."} or {"error": "..."}
│                   error_message = error_data.get("detail", error_data.get("error", str(error_data)))
│               elif isinstance(error_data, list):
│                   # Handle DRF validation error format: ["Error message."]
│                   error_message = ". ".join(str(item) for item in error_data)
│               elif isinstance(error_data, str):
│                   error_message = error_data
│                   
│               # Raise the appropriate exception with the formatted message.
│               if response.status_code == 403:
│                   raise PermissionDenied(error_message)
│               elif response.status_code == 404:
│                   raise NotFound(error_message)
│               elif response.status_code == 400:
│                   raise ValidationError(error_message)
│               else:
│                   # For 5xx errors or other unexpected codes.
│                   response.raise_for_status()
│       
│       
│       # --- The rest of the client classes (ProjectServiceClient, ModelServiceClient) ---
│       # --- remain exactly the same. No changes are needed there. ---
│       
│       class ProjectServiceClient(BaseServiceClient):
│           def __init__(self):
│               super().__init__("Project Service", "PROJECT_SERVICE_URL")
│       
│           def authorize_user(self, jwt_token: str, project_id: str):
│               headers = {"Authorization": f"Bearer {jwt_token}",
│                          "Host": "localhost"  }
│               internal_path = f"/ms2/internal/v1/projects/{project_id}/authorize"
│               response = self.client.get(internal_path, headers=headers)
│               self._handle_response(response)
│       
│       
│       class ModelServiceClient(BaseServiceClient):
│           def __init__(self):
│               super().__init__("Model Service", "MODEL_SERVICE_URL")
│       
│           def validate_model(self, jwt_token: str, model_id: str):
│               headers = {"Authorization": f"Bearer {jwt_token}",
│                          "Host": "localhost"  }
│               internal_path = f"/ms3/internal/v1/models/{model_id}/validate"
│               response = self.client.get(internal_path, headers=headers)
│               self._handle_response(response)
│       
│       
│       
│       
│       class ToolServiceClient(BaseServiceClient):
│           def __init__(self):
│               super().__init__("Tool Service", "TOOL_SERVICE_URL")
│       
│           def validate_tools(self, jwt_token: str, tool_ids: list[str]):
│               """
│               Calls the Tool Service's internal validation endpoint to check
│               if the user has permission to use the given tool IDs.
│               """
│               headers = {"Authorization": f"Bearer {jwt_token}"}
│               payload = {"tool_ids": tool_ids}
│               # The endpoint path must match the one in MS7's internal_urls.py
│               internal_path = "/ms7/internal/v1/tools/validate/"
│               
│               response = self.client.post(internal_path, headers=headers, json=payload)
│               
│               # The _handle_response method will raise PermissionDenied on 403, etc.
│               self._handle_response(response)
│   ]
│   custom_auth.py
│   [
│       from rest_framework_simplejwt.authentication import JWTAuthentication
│       from rest_framework_simplejwt.models import TokenUser # Import TokenUser
│       from rest_framework_simplejwt.settings import api_settings as simple_jwt_settings
│       from django.utils.translation import gettext_lazy as _
│       from rest_framework_simplejwt.exceptions import InvalidToken
│       
│       class ForceTokenUserJWTAuthentication(JWTAuthentication):
│           def get_user(self, validated_token):
│               """
│               Returns a TokenUser instance based on the validated token.
│               Bypasses any local database User lookup for JWT authentication.
│               """
│               try:
│                   # simple_jwt_settings.USER_ID_CLAIM refers to what you set in settings.py
│                   # e.g., "user_id"
│                   user_id = validated_token[simple_jwt_settings.USER_ID_CLAIM]
│               except KeyError:
│                   raise InvalidToken(_("Token contained no recognizable user identification"))
│       
│               # Correct way to instantiate TokenUser: pass the validated_token
│               # The TokenUser class will internally use USER_ID_CLAIM and USER_ID_FIELD
│               # from your SIMPLE_JWT settings to extract the user ID and set its 'id' or 'pk'.
│               token_user = TokenUser(validated_token)
│       
│               # The TokenUser's 'id' (and 'pk') attribute should now be populated correctly
│               # by its own __init__ method based on the validated_token and your SIMPLE_JWT settings
│               # for USER_ID_CLAIM and USER_ID_FIELD.
│       
│               # Example: If you wanted to verify or access it (not strictly necessary here)
│               # print(f"TokenUser ID: {token_user.id}, TokenUser PK: {token_user.pk}")
│       
│               return token_user
│   ]
│   models.py
│   [
│       # in nodes/models.py
│       
│       import uuid
│       from django.db import models
│       
│       
│       class NodeStatus(models.TextChoices):
│           ACTIVE = 'active', 'Active'             # Fully functional
│           ALTERED = 'altered', 'Altered'           # A non-critical dependency was removed (e.g., a tool)
│           INACTIVE = 'inactive', 'Inactive'
│       
│       class Node(models.Model):
│           id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
│           
│       
│           status = models.CharField(
│               max_length=20,
│               choices=NodeStatus.choices,
│               default=NodeStatus.ACTIVE,
│               db_index=True,
│               help_text="The current status of the node (e.g., active, altered, inactive)."
│           )
│       
│       
│           project_id = models.UUIDField(
│               db_index=True,
│               help_text="The Project this Node belongs to."
│           )
│           
│           # ------------------ THE ONE AND ONLY FIX IS HERE ------------------
│           owner_id = models.UUIDField(
│               db_index=True,
│               help_text="The UUID of the user who owns this Node. Corresponds to the User UUID in the Auth service JWT."
│           )
│           # --------------------------------------------------------------------
│           
│           name = models.CharField(
│               max_length=255,
│               help_text="The user-defined name for this Node (e.g., 'My Research Agent')."
│           )
│       
│           configuration = models.JSONField(
│               default=dict,
│               help_text="The complete configuration blueprint for this node's behavior."
│           )
│       
│           created_at = models.DateTimeField(auto_now_add=True, editable=False)
│           updated_at = models.DateTimeField(auto_now=True)
│       
│           class Meta:
│               ordering = ['-created_at']
│               verbose_name = "Node"
│               verbose_name_plural = "Nodes"
│       
│           def __str__(self):
│               return f"{self.name} (Project: {self.project_id})"
│   ]
│   node_pb2.py
│   [
│       # -*- coding: utf-8 -*-
│       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│       # NO CHECKED-IN PROTOBUF GENCODE
│       # source: node.proto
│       # Protobuf Python Version: 6.31.1
│       """Generated protocol buffer code."""
│       from google.protobuf import descriptor as _descriptor
│       from google.protobuf import descriptor_pool as _descriptor_pool
│       from google.protobuf import runtime_version as _runtime_version
│       from google.protobuf import symbol_database as _symbol_database
│       from google.protobuf.internal import builder as _builder
│       _runtime_version.ValidateProtobufRuntimeVersion(
│           _runtime_version.Domain.PUBLIC,
│           6,
│           31,
│           1,
│           '',
│           'node.proto'
│       )
│       # @@protoc_insertion_point(imports)
│       
│       _sym_db = _symbol_database.Default()
│       
│       
│       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│       
│       
│       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\nnode.proto\x12\x04node\x1a\x1cgoogle/protobuf/struct.proto\"9\n\x15GetNodeDetailsRequest\x12\x0f\n\x07node_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"\x88\x01\n\x16GetNodeDetailsResponse\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nproject_id\x18\x02 \x01(\t\x12\x10\n\x08owner_id\x18\x03 \x01(\t\x12\x0c\n\x04name\x18\x04 \x01(\t\x12.\n\rconfiguration\x18\x05 \x01(\x0b\x32\x17.google.protobuf.Struct2Z\n\x0bNodeService\x12K\n\x0eGetNodeDetails\x12\x1b.node.GetNodeDetailsRequest\x1a\x1c.node.GetNodeDetailsResponseb\x06proto3')
│       
│       _globals = globals()
│       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'node_pb2', _globals)
│       if not _descriptor._USE_C_DESCRIPTORS:
│         DESCRIPTOR._loaded_options = None
│         _globals['_GETNODEDETAILSREQUEST']._serialized_start=50
│         _globals['_GETNODEDETAILSREQUEST']._serialized_end=107
│         _globals['_GETNODEDETAILSRESPONSE']._serialized_start=110
│         _globals['_GETNODEDETAILSRESPONSE']._serialized_end=246
│         _globals['_NODESERVICE']._serialized_start=248
│         _globals['_NODESERVICE']._serialized_end=338
│       # @@protoc_insertion_point(module_scope)
│       
│   ]
│   node_pb2_grpc.py
│   [
│       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│       """Client and server classes corresponding to protobuf-defined services."""
│       import grpc
│       import warnings
│       
│       from . import node_pb2 as node__pb2
│       
│       GRPC_GENERATED_VERSION = '1.74.0'
│       GRPC_VERSION = grpc.__version__
│       _version_not_supported = False
│       
│       try:
│           from grpc._utilities import first_version_is_lower
│           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│       except ImportError:
│           _version_not_supported = True
│       
│       if _version_not_supported:
│           raise RuntimeError(
│               f'The grpc package installed is at version {GRPC_VERSION},'
│               + f' but the generated code in node_pb2_grpc.py depends on'
│               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│           )
│       
│       
│       class NodeServiceStub(object):
│           """Missing associated documentation comment in .proto file."""
│       
│           def __init__(self, channel):
│               """Constructor.
│       
│               Args:
│                   channel: A grpc.Channel.
│               """
│               self.GetNodeDetails = channel.unary_unary(
│                       '/node.NodeService/GetNodeDetails',
│                       request_serializer=node__pb2.GetNodeDetailsRequest.SerializeToString,
│                       response_deserializer=node__pb2.GetNodeDetailsResponse.FromString,
│                       _registered_method=True)
│       
│       
│       class NodeServiceServicer(object):
│           """Missing associated documentation comment in .proto file."""
│       
│           def GetNodeDetails(self, request, context):
│               """Authorizes and retrieves the full details of a node.
│               """
│               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│               context.set_details('Method not implemented!')
│               raise NotImplementedError('Method not implemented!')
│       
│       
│       def add_NodeServiceServicer_to_server(servicer, server):
│           rpc_method_handlers = {
│                   'GetNodeDetails': grpc.unary_unary_rpc_method_handler(
│                           servicer.GetNodeDetails,
│                           request_deserializer=node__pb2.GetNodeDetailsRequest.FromString,
│                           response_serializer=node__pb2.GetNodeDetailsResponse.SerializeToString,
│                   ),
│           }
│           generic_handler = grpc.method_handlers_generic_handler(
│                   'node.NodeService', rpc_method_handlers)
│           server.add_generic_rpc_handlers((generic_handler,))
│           server.add_registered_method_handlers('node.NodeService', rpc_method_handlers)
│       
│       
│        # This class is part of an EXPERIMENTAL API.
│       class NodeService(object):
│           """Missing associated documentation comment in .proto file."""
│       
│           @staticmethod
│           def GetNodeDetails(request,
│                   target,
│                   options=(),
│                   channel_credentials=None,
│                   call_credentials=None,
│                   insecure=False,
│                   compression=None,
│                   wait_for_ready=None,
│                   timeout=None,
│                   metadata=None):
│               return grpc.experimental.unary_unary(
│                   request,
│                   target,
│                   '/node.NodeService/GetNodeDetails',
│                   node__pb2.GetNodeDetailsRequest.SerializeToString,
│                   node__pb2.GetNodeDetailsResponse.FromString,
│                   options,
│                   channel_credentials,
│                   insecure,
│                   call_credentials,
│                   compression,
│                   wait_for_ready,
│                   timeout,
│                   metadata,
│                   _registered_method=True)
│       
│   ]
│   permissions.py
│   [
│       from rest_framework import permissions
│       import uuid # Import the uuid module
│       
│       class IsOwner(permissions.BasePermission):
│           """
│           Custom permission to only allow owners of an object to edit or view it.
│           This version is robust against type mismatches (str vs. UUID).
│           """
│           def has_object_permission(self, request, view, obj):
│               """
│               Return `True` if permission is granted, `False` otherwise.
│               """
│               # obj is the Node instance from the database. obj.owner_id is a UUID object.
│               # request.user is the TokenUser from the JWT. request.user.id can be str or UUID.
│               
│               # --- THE FIX IS HERE ---
│               # We convert both the object's owner_id and the user's id to strings
│               # before comparing them. This ensures a reliable, type-safe comparison.
│               try:
│                   # Ensure the object's owner_id can be represented as a string
│                   obj_owner_id_str = str(obj.owner_id)
│                   
│                   # Ensure the request user's ID can be represented as a string
│                   request_user_id_str = str(request.user.id)
│                   
│                   return obj_owner_id_str == request_user_id_str
│                   
│               except (TypeError, AttributeError):
│                   # If for some reason either field is missing or invalid, deny permission.
│                   return False
│   ]
│   ├───protos
│   │   node.proto
│   │   [
│   │       syntax = "proto3";
│   │       
│   │       import "google/protobuf/struct.proto";
│   │       
│   │       package node;
│   │       
│   │       service NodeService {
│   │         // Authorizes and retrieves the full details of a node.
│   │         rpc GetNodeDetails(GetNodeDetailsRequest) returns (GetNodeDetailsResponse);
│   │       }
│   │       
│   │       message GetNodeDetailsRequest {
│   │         string node_id = 1;
│   │         string user_id = 2; // User ID from the JWT, used for authorization.
│   │       }
│   │       
│   │       message GetNodeDetailsResponse {
│   │         string id = 1;
│   │         string project_id = 2;
│   │         string owner_id = 3;
│   │         string name = 4;
│   │         google.protobuf.Struct configuration = 5; // The node's JSON configuration.
│   │       }
│   │   ]
│   repository.py
│   [
│       # MS4/nodes/repository.py
│       
│       from typing import List, Optional
│       import uuid
│       from .models import Node, NodeStatus
│       
│       class NodeRepository:
│           """
│           Acts as a data access layer for the Node model.
│           All direct database interactions for Nodes should be in this class.
│           """
│       
│           def find_by_id(self, node_id: uuid.UUID) -> Optional[Node]:
│               """
│               Finds a single Node instance by its primary key.
│       
│               Returns:
│                   The Node instance or None if not found.
│               """
│               try:
│                   return Node.objects.get(id=node_id)
│               except Node.DoesNotExist:
│                   return None
│       
│           def find_by_project(self, project_id: uuid.UUID) -> List[Node]:
│               """
│               Finds all Node instances belonging to a specific project.
│       
│               Returns:
│                   A list of Node instances.
│               """
│               return list(Node.objects.filter(project_id=project_id))
│       
│           def create(self, *, project_id: uuid.UUID, owner_id: uuid.UUID, name: str, configuration: dict) -> Node:
│               """
│               Creates and saves a new Node instance in the database.
│               The default status will be 'active' as defined in the model.
│               """
│               return Node.objects.create(
│                   project_id=project_id,
│                   owner_id=owner_id,
│                   name=name,
│                   configuration=configuration
│               )
│       
│           def update(self, node: Node, name: str, configuration: dict, status: str = None) -> Node:
│               """
│               Updates an existing Node instance.
│               Optionally allows the status to be updated.
│               """
│               node.name = name
│               node.configuration = configuration
│               
│               # Prepare a list of fields to be updated to make the save() call more efficient.
│               update_fields = ['name', 'configuration', 'updated_at']
│       
│               # If a new status is provided by the service layer (e.g., 'active' after a fix),
│               # add it to the instance and the list of fields to update.
│               if status and status in NodeStatus.values:
│                   node.status = status
│                   update_fields.append('status')
│       
│               node.save(update_fields=update_fields)
│               return node
│       
│           def delete(self, node: Node) -> None:
│               """
│               Deletes a Node instance from the database.
│               """
│               node.delete()
│   ]
│   serializers.py
│   [
│       # in nodes/serializers.py
│       
│       from rest_framework import serializers
│       from .models import Node
│       
│       class NodeSerializer(serializers.ModelSerializer):
│           class Meta:
│               model = Node
│               fields = ['id', 'project_id', 'owner_id', 'name', 'status', 'configuration', 'created_at', 'updated_at']
│               # --- AND ENSURE 'status' IS IN read_only_fields ---
│               read_only_fields = ['id', 'project_id', 'owner_id', 'status', 'created_at', 'updated_at']
│       
│       class NodeCreateSerializer(serializers.Serializer):
│           name = serializers.CharField(max_length=255, required=True)
│           configuration = serializers.JSONField(required=True)
│       
│           def validate_configuration(self, value):
│               if not isinstance(value, dict):
│                   raise serializers.ValidationError("Configuration must be a valid JSON object.")
│               if "node_type" not in value:
│                   raise serializers.ValidationError("Configuration must include a 'node_type'.")
│               if "model_config" not in value or "model_id" not in value["model_config"]:
│                   raise serializers.ValidationError("Configuration must include 'model_config' with a 'model_id'.")
│               return value
│       
│       class NodeUpdateSerializer(NodeCreateSerializer):
│           # Inherits all fields and validation from the create serializer.
│           pass
│   ]
│   servicer.py
│   [
│       import grpc
│       from google.protobuf.struct_pb2 import Struct
│       
│       # Import generated classes and your existing repository/permissions
│       from . import node_pb2, node_pb2_grpc
│       from .repository import NodeRepository
│       from .models import Node
│       
│       class NodeServicer(node_pb2_grpc.NodeServiceServicer):
│           """Implements the gRPC service for Nodes."""
│       
│           def GetNodeDetails(self, request, context):
│               print(f"gRPC [NodeService]: Received GetNodeDetails request for node_id={request.node_id}")
│               repo = NodeRepository()
│               
│               try:
│                   node_instance = repo.find_by_id(request.node_id)
│                   
│                   if not node_instance:
│                       context.set_code(grpc.StatusCode.NOT_FOUND)
│                       context.set_details(f"Node with ID {request.node_id} not found.")
│                       return node_pb2.GetNodeDetailsResponse()
│       
│                   # --- AUTHORIZATION CHECK ---
│                   if str(node_instance.owner_id) != request.user_id:
│                       context.set_code(grpc.StatusCode.PERMISSION_DENIED)
│                       context.set_details("User does not have permission to access this node.")
│                       return node_pb2.GetNodeDetailsResponse()
│       
│                   # Convert Python dict to protobuf Struct
│                   proto_config = Struct()
│                   proto_config.update(node_instance.configuration)
│       
│                   return node_pb2.GetNodeDetailsResponse(
│                       id=str(node_instance.id),
│                       project_id=str(node_instance.project_id),
│                       owner_id=str(node_instance.owner_id),
│                       name=node_instance.name,
│                       configuration=proto_config
│                   )
│       
│               except Exception as e:
│                   print(f"gRPC [NodeService]: Internal error - {e}")
│                   context.set_code(grpc.StatusCode.INTERNAL)
│                   context.set_details('An internal error occurred.')
│                   return node_pb2.GetNodeDetailsResponse()
│   ]
│   services.py
│   [
│       # in nodes/services.py
│       
│       from django.conf import settings
│       from rest_framework.exceptions import PermissionDenied, ValidationError, NotFound
│       import concurrent.futures
│       import uuid
│       
│       # These imports must correctly point to your client, repository, and model files.
│       from .clients import ProjectServiceClient, ModelServiceClient, ToolServiceClient #, MemoryServiceClient, KnowledgeServiceClient
│       from .repository import NodeRepository
│       from .models import Node, NodeStatus
│       
│       class NodeService:
│           """
│           The service layer for handling all business logic related to Nodes.
│           This is the definitive, synchronous, multi-threaded version.
│           """
│           def __init__(self):
│               # Instantiate all dependencies. In a larger app, this would use dependency injection.
│               self.node_repo = NodeRepository()
│               self.project_client = ProjectServiceClient()
│               self.model_client = ModelServiceClient()
│               self.tool_client = ToolServiceClient()
│               # self.memory_client = MemoryServiceClient()
│               # self.knowledge_client = KnowledgeServiceClient()
│       
│           def _validate_resources(self, jwt_token: str, project_id: str, configuration: dict):
│               """
│               Runs all validation checks against other microservices in parallel using a thread pool.
│               """
│               # If validation is disabled in settings, skip this entire process.
│               if not settings.NODE_SERVICE_VALIDATION_ENABLED:
│                   return
│                   
│               with concurrent.futures.ThreadPoolExecutor() as executor:
│                   futures = []
│                   
│                   # --- Submit All Validation Tasks Concurrently ---
│       
│                   # Task 1: Check project ownership.
│                   futures.append(executor.submit(self.project_client.authorize_user, jwt_token, project_id))
│                   
│                   # Task 2: Validate the AI Model.
│                   model_id = configuration.get("model_config", {}).get("model_id")
│                   if not model_id:
│                       raise ValidationError("Configuration must include 'model_config' with a 'model_id'.")
│                   futures.append(executor.submit(self.model_client.validate_model, jwt_token, model_id))
│       
│       
│                   # Task 3: Validate the Tools.
│                   tool_config = configuration.get("tool_config", {})
│                   if "tool_ids" in tool_config and tool_config["tool_ids"]:
│                       futures.append(
│                           executor.submit(
│                               self.tool_client.validate_tools,
│                               jwt_token,
│                               tool_config["tool_ids"]
│                           )
│                       )
│       
│                   # Task 3 & 4 (Future): Validate Memory and Knowledge resources.
│                   # These are commented out but show how to extend the pattern.
│                   # memory_config = configuration.get("memory_config", {})
│                   # if memory_config.get("is_enabled", False):
│                   #     bucket_id = memory_config.get("bucket_id")
│                   #     if not bucket_id:
│                   #         raise ValidationError("Memory is enabled, but no 'bucket_id' was provided.")
│                   #     futures.append(executor.submit(self.memory_client.validate_bucket, jwt_token, project_id, bucket_id))
│                   
│                   # knowledge_config = configuration.get("knowledge_config", {})
│                   # if knowledge_config.get("is_enabled", False):
│                   #     collection_id = knowledge_config.get("collection_id")
│                   #     if not collection_id:
│                   #         raise ValidationError("Knowledge is enabled, but no 'collection_id' was provided.")
│                   #     futures.append(executor.submit(self.knowledge_client.validate_collection, jwt_token, project_id, collection_id))
│       
│                   # --- Wait for all tasks to complete and check for any failures ---
│                   for future in concurrent.futures.as_completed(futures):
│                       # future.result() will do nothing if the task succeeded, but will
│                       # re-raise any exception (like PermissionDenied, NotFound) that
│                       # occurred in the thread, causing this entire method to fail.
│                       future.result()
│       
│           def create_node(self, *, jwt_token: str, user_id: int, project_id: uuid.UUID, name: str, configuration: dict) -> Node:
│               """
│               The use case for creating a new node. It handles validation and persistence.
│               """
│               # The service layer is responsible for calling the validation logic.
│               self._validate_resources(jwt_token, str(project_id), configuration)
│               
│               # If validation succeeds, delegate creation to the repository.
│               return self.node_repo.create(
│                   project_id=project_id,
│                   owner_id=user_id,
│                   name=name,
│                   configuration=configuration
│               )
│       
│           def update_node(self, *, jwt_token: str, user_id: int, node: Node, name: str, configuration: dict) -> Node:
│               """
│               The use case for updating an existing node. It re-validates the new
│               configuration and resets the node's status to ACTIVE upon success.
│               """
│               # 1. Re-validate all dependencies in the new configuration.
│               # If any of the new IDs are invalid, this will raise an exception and the update will fail.
│               self._validate_resources(jwt_token, str(node.project_id), configuration)
│                   
│               # 2. If validation succeeds, update the node and reset its status.
│               # This user action implies they have "fixed" any previous invalid/altered state.
│               return self.node_repo.update(
│                   node=node,
│                   name=name,
│                   configuration=configuration,
│                   status=NodeStatus.ACTIVE # <-- THE KEY CHANGE IS HERE
│               )
│       
│           def delete_node(self, node: Node):
│               self.node_repo.delete(node)
│       
│           def get_nodes_for_project(self, project_id: uuid.UUID, user_id: int, jwt_token: str) -> list[Node]:
│               """
│               The use case for listing all nodes in a project.
│               It first authorizes project-level access, then fetches the data.
│               """
│               # Step 1: Authorize that the user can even view this project's contents.
│               self.project_client.authorize_user(jwt_token, str(project_id))
│               
│               # Step 2: If authorized, retrieve the nodes from the local database.
│               return self.node_repo.find_by_project(project_id)
│               
│           def delete_node(self, node: Node):
│               """
│               The use case for deleting a node. The view handles ownership check.
│               This service delegates the deletion to the repository.
│               """
│               self.node_repo.delete(node)
│   ]
│   tests.py
│   [
│       from django.test import TestCase
│       
│       # Create your tests here.
│       
│   ]
│   urls.py
│   [
│       # in nodes/urls.py
│       
│       from django.urls import path
│       from .views import NodeListCreateAPIView, NodeDetailAPIView, ResourceDeletionHookAPIView
│       
│       app_name = 'nodes'
│       
│       urlpatterns = [
│       
│           path(
│               'projects/<uuid:project_id>/nodes/', 
│               NodeListCreateAPIView.as_view(), 
│               name='node-list-create'
│           ),
│       
│           path(
│               'nodes/<uuid:pk>/', 
│               NodeDetailAPIView.as_view(), 
│               name='node-detail'
│           ),
│       
│           # Hook for resource deletion enternal API
│           path('hooks/resource-deleted/', ResourceDeletionHookAPIView.as_view(), name='hook-resource-deleted'),
│       ]
│   ]
│   views.py
│   [
│       # in nodes/views.py
│       
│       from rest_framework.views import APIView
│       from rest_framework.response import Response
│       from rest_framework import status, permissions
│       from rest_framework.exceptions import NotFound
│       
│       # Import everything needed from the project
│       from .services import NodeService
│       from .serializers import NodeSerializer, NodeCreateSerializer, NodeUpdateSerializer
│       from .permissions import IsOwner
│       from .repository import NodeRepository # Needed for get_object
│       import uuid
│       from django.db import transaction # <-- Import transaction
│       
│       class NodeListCreateAPIView(APIView):
│           """
│           Handles listing available nodes within a project and creating a new node.
│           - GET /ms4/api/v1/projects/{project_id}/nodes/
│           - POST /ms4/api/v1/projects/{project_id}/nodes/
│           """
│           permission_classes = [permissions.IsAuthenticated]
│       
│           def get(self, request, project_id):
│               """
│               Handles GET requests to list nodes for a specific project.
│               """
│               service = NodeService()
│               # The project_id from the URL is already a UUID object thanks to the URL converter.
│               jwt_token = str(request.auth)
│               
│               # The service layer handles both authorization and data fetching.
│               nodes = service.get_nodes_for_project(
│                   project_id=project_id, 
│                   user_id=request.user.id, 
│                   jwt_token=jwt_token
│               )
│               
│               serializer = NodeSerializer(nodes, many=True)
│               return Response(serializer.data, status=status.HTTP_200_OK)
│       
│           def post(self, request, project_id):
│               """
│               Handles POST requests to create a new node.
│               """
│               service = NodeService()
│               serializer = NodeCreateSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│               
│               jwt_token = str(request.auth)
│               validated_data = serializer.validated_data
│               
│               # Call the service to perform validation and creation.
│               new_node = service.create_node(
│                   jwt_token=jwt_token,
│                   user_id=request.user.id,
│                   project_id=project_id,
│                   name=validated_data['name'],
│                   configuration=validated_data['configuration']
│               )
│               
│               response_serializer = NodeSerializer(new_node)
│               return Response(response_serializer.data, status=status.HTTP_201_CREATED)
│       
│       
│       class NodeDetailAPIView(APIView):
│           """
│           Handles retrieving, updating, and deleting a specific model instance.
│           - GET /ms4/api/v1/nodes/{pk}/
│           - PUT /ms4/api/v1/nodes/{pk}/
│           - DELETE /ms4/api/v1/nodes/{pk}/
│           """
│           permission_classes = [permissions.IsAuthenticated, IsOwner]
│       
│           def get_object(self, pk):
│               """
│               Helper method to fetch a node object by its primary key (pk).
│               It also runs DRF's object-level permission checks.
│               """
│               repo = NodeRepository()
│               node = repo.find_by_id(pk)
│               if not node:
│                   raise NotFound("Node not found.")
│               
│               # This line is crucial for the IsOwner permission to work.
│               self.check_object_permissions(self.request, node)
│               return node
│               
│           def get(self, request, pk):
│               """
│               Handles GET requests to retrieve a single node.
│               """
│               node = self.get_object(pk)
│               serializer = NodeSerializer(node)
│               return Response(serializer.data)
│       
│           def put(self, request, pk):
│               """
│               Handles PUT requests to update a node.
│               """
│               service = NodeService()
│               node_to_update = self.get_object(pk)
│               
│               serializer = NodeUpdateSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│               
│               validated_data = serializer.validated_data
│               jwt_token = str(request.auth)
│               
│               # Call the service to perform validation and update.
│               updated_node = service.update_node(
│                   jwt_token=jwt_token,
│                   user_id=request.user.id,
│                   node=node_to_update,
│                   name=validated_data['name'],
│                   configuration=validated_data['configuration']
│               )
│               
│               response_serializer = NodeSerializer(updated_node)
│               return Response(response_serializer.data)
│       
│           def delete(self, request, pk):
│               """
│               Handles DELETE requests to remove a node.
│               """
│               service = NodeService()
│               node_to_delete = self.get_object(pk)
│               
│               # The service layer handles the deletion logic.
│               service.delete_node(node_to_delete)
│               
│               return Response(status=status.HTTP_204_NO_CONTENT)
│           
│       from rest_framework.views import APIView
│       from rest_framework.response import Response
│       from rest_framework import status, permissions
│       from .models import Node, NodeStatus
│       
│       class IsInternalServicePermission(permissions.BasePermission):
│           """
│           A simple permission to ensure requests come from a trusted source.
│           In a real system, this would be more robust (e.g., checking for a
│           shared secret header, IP allowlisting, or mTLS).
│           """
│           def has_permission(self, request, view):
│               # For now, we trust any authenticated request to this internal endpoint.
│               # This assumes your gateway doesn't expose this endpoint publicly.
│               return request.user and request.user.is_authenticated
│       
│       # resource_deletion_hook.py
│       class ResourceDeletionHookAPIView(APIView):
│           """
│           An internal webhook endpoint for other services to call before they
│           delete a resource, allowing this service to clean up dependencies.
│           """
│           permission_classes = [IsInternalServicePermission]
│       
│       class ResourceDeletionHookAPIView(APIView):
│           permission_classes = [IsInternalServicePermission]
│       
│           @transaction.atomic
│           def post(self, request):
│               resource_type = request.data.get('resource_type')
│               resource_id = request.data.get('resource_id')
│       
│               if not resource_type or not resource_id:
│                   return Response({"error": "Missing 'resource_type' or 'resource_id'."}, status=status.HTTP_400_BAD_REQUEST)
│       
│               updated_count = 0
│               message = ""
│       
│               if resource_type == 'model':
│                   # This query is simple and works fine on SQLite.
│                   nodes_to_update = Node.objects.select_for_update().filter(
│                       configuration__model_config__model_id=resource_id
│                   ).exclude(status=NodeStatus.INACTIVE)
│                   
│                   updated_count = nodes_to_update.update(status=NodeStatus.INACTIVE)
│                   message = f"Inactivated {updated_count} nodes."
│       
│               elif resource_type == 'tool':
│                   # --- SQLITE-COMPATIBLE SOLUTION FOR 'contains' LOOKUP ---
│                   #
│                   # 1. Broadly select all potentially relevant nodes.
│                   #    We fetch any node that is not inactive and *might* have tools.
│                   candidate_nodes = Node.objects.select_for_update().filter(
│                       status__in=[NodeStatus.ACTIVE, NodeStatus.ALTERED],
│                       configuration__has_key='tool_config',
│                       configuration__tool_config__has_key='tool_ids'
│                   )
│       
│                   nodes_to_process = []
│                   # 2. Filter the candidates in Python memory.
│                   for node in candidate_nodes:
│                       # Safely access the list of tool IDs
│                       tool_ids = node.configuration.get('tool_config', {}).get('tool_ids', [])
│                       if isinstance(tool_ids, list) and resource_id in tool_ids:
│                           nodes_to_process.append(node)
│       
│                   # 3. If we found matching nodes, loop through them to update and save.
│                   #    We must do this one-by-one because we are modifying the JSON field.
│                   if nodes_to_process:
│                       for node in nodes_to_process:
│                           # HEAL: Remove the deleted tool_id
│                           node.configuration['tool_config']['tool_ids'].remove(resource_id)
│                           
│                           # NOTIFY: Set the status to ALTERED
│                           node.status = NodeStatus.ALTERED
│                           
│                           # SAVE: Persist the changes for this specific node
│                           node.save()
│                       
│                       updated_count = len(nodes_to_process)
│       
│                   message = f"Removed tool and altered {updated_count} nodes."
│                   # --- END OF SQLITE-COMPATIBLE SOLUTION ---
│               
│               else:
│                    return Response({"error": f"Unknown resource_type: {resource_type}"}, status=status.HTTP_400_BAD_REQUEST)
│       
│               return Response(
│                   {"message": f"Processed deletion for {resource_type}:{resource_id}. {message}"},
│                   status=status.HTTP_200_OK
│               )
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       asgiref==3.9.1
│       certifi==2025.7.14
│       cffi==1.17.1
│       charset-normalizer==3.4.2
│       cryptography==45.0.5
│       defusedxml==0.7.1
│       Django==5.2.4
│       djangorestframework==3.16.0
│       djangorestframework_simplejwt==5.5.1
│       djoser==2.3.3
│       idna==3.10
│       oauthlib==3.3.1
│       pillow==11.3.0
│       pycparser==2.22
│       PyJWT==2.10.1
│       python3-openid==3.2.0
│       requests==2.32.4
│       requests-oauthlib==2.0.0
│       social-auth-app-django==5.5.1
│       social-auth-core==4.7.0
│       sqlparse==0.5.3
│       tzdata==2025.2
│       urllib3==2.5.0
│       dotenv
│       httpx
│       pika
│       grpcio
│       grpcio-tools
│       protobuf
│       google-api-python-client
│   ]
