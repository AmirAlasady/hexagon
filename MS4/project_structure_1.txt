│   .env
│   [
│       # Django's main secret key
│       DJANGO_SECRET_KEY='django-inergegbnm546334rg56564ldpcuck6bytc4h1*8v!=8(_wau6g8or'
│       JWT_SECRET_KEY ='jwt-secure-m3x$DFGRTJRTYNEHRETNEFDDHD43.m<?><DFGRTJYRJGc4h1*8v!=8(_wau6g8or'
│       # You can also add other environment-specific settings here
│       DJANGO_DEBUG='True'
│       DATABASE_URL='sqlite:///./db.sqlite3' # Example for database config
│       JWT_ISSUER="https://ms1.auth-service.com"
│       
│       
│       
│       
│       # custom attributes for Node Service
│       NODE_SERVICE_VALIDATION_ENABLED=True
│       
│       # in .env file for the Node Service
│       # The base URL for the entire Project Service application.
│       PROJECT_SERVICE_URL=http://localhost:8001
│       
│       # The base URL for the entire AIModel Service application.
│       MODEL_SERVICE_URL=http://localhost:8002
│       
│       # The base URL for the entire Memory Service application.
│       MEMORY_SERVICE_URL=non
│       
│       # The base URL for the entire Tool Service application.
│       TOOL_SERVICE_URL=http://localhost:8007
│       
│       
│       # RabbitMQ connection string for the Node Service
│       RABBITMQ_URL='amqp://guest:guest@localhost:5672/'
│   ]
├───MS4
│   __init__.py
│   [
│       
│   ]
│   asgi.py
│   [
│       """
│       ASGI config for MS4 project.
│       
│       It exposes the ASGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
│       """
│       
│       import os
│       
│       from django.core.asgi import get_asgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│       
│       application = get_asgi_application()
│       
│   ]
│   settings.py
│   [
│       
│       from datetime import timedelta
│       from pathlib import Path
│       import os
│       
│       
│       #======================================================
│       NODE_SERVICE_VALIDATION_ENABLED = os.getenv('NODE_SERVICE_VALIDATION_ENABLED', 'True').lower() == 'true'
│       #======================================================
│       
│       
│       # Build paths inside the project like this: BASE_DIR / 'subdir'.
│       BASE_DIR = Path(__file__).resolve().parent.parent
│       
│       from dotenv import load_dotenv
│       load_dotenv(BASE_DIR / '.env')
│       # Quick-start development settings - unsuitable for production
│       # See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/
│       
│       SECRET_KEY = os.getenv('DJANGO_SECRET_KEY')
│       if not SECRET_KEY:
│           # This fallback should ideally not be hit if .env is loaded correctly
│           # or if the environment variable is set directly in the deployment environment.
│           SECRET_KEY = 'django-insecure-fallback-dev-key-!!change-me!!'
│           print("WARNING: DJANGO_SECRET_KEY not found in environment or .env. Using fallback. THIS IS INSECURE FOR PRODUCTION.")
│       
│       DEBUG = os.getenv('DJANGO_DEBUG', 'True').lower() in ('true', '1', 't')
│       
│       ALLOWED_HOSTS = ['*']
│       
│       
│       # Application definition
│       
│       INSTALLED_APPS = [
│           'django.contrib.admin',
│           'django.contrib.auth',
│           'django.contrib.contenttypes',
│           'django.contrib.sessions',
│           'django.contrib.messages',
│           'django.contrib.staticfiles',
│           'rest_framework',
│           'rest_framework_simplejwt',
│           'nodes',  # Your custom app
│           'messaging',
│           'nodes_internals'
│       ]
│       
│       MIDDLEWARE = [
│           'django.middleware.security.SecurityMiddleware',
│           'django.contrib.sessions.middleware.SessionMiddleware',
│           'django.middleware.common.CommonMiddleware',
│           'django.middleware.csrf.CsrfViewMiddleware',
│           'django.contrib.auth.middleware.AuthenticationMiddleware',
│           'django.contrib.messages.middleware.MessageMiddleware',
│           'django.middleware.clickjacking.XFrameOptionsMiddleware',
│       ]
│       
│       ROOT_URLCONF = 'MS4.urls'
│       
│       TEMPLATES = [
│           {
│               'BACKEND': 'django.template.backends.django.DjangoTemplates',
│               'DIRS': [],
│               'APP_DIRS': True,
│               'OPTIONS': {
│                   'context_processors': [
│                       'django.template.context_processors.request',
│                       'django.contrib.auth.context_processors.auth',
│                       'django.contrib.messages.context_processors.messages',
│                   ],
│               },
│           },
│       ]
│       
│       WSGI_APPLICATION = 'MS4.wsgi.application'
│       
│       
│       # Database
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#databases
│       
│       DATABASES = {
│           'default': {
│               'ENGINE': 'django.db.backends.sqlite3',
│               'NAME': BASE_DIR / 'db.sqlite3',
│           }
│       }
│       
│       
│       # Password validation
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators
│       
│       AUTH_PASSWORD_VALIDATORS = [
│           {
│               'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
│           },
│       ]
│       
│       
│       # Internationalization
│       # https://docs.djangoproject.com/en/5.2/topics/i18n/
│       
│       LANGUAGE_CODE = 'en-us'
│       
│       TIME_ZONE = 'UTC'
│       
│       USE_I18N = True
│       
│       USE_TZ = True
│       
│       
│       # Static files (CSS, JavaScript, Images)
│       # https://docs.djangoproject.com/en/5.2/howto/static-files/
│       
│       STATIC_URL = '/static/'
│       STATIC_ROOT = BASE_DIR / 'staticfiles'
│       
│       # Default primary key field type
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field
│       
│       DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
│       
│       
│       
│       JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY')
│       
│       # REST Framework
│       REST_FRAMEWORK = {
│           "DEFAULT_PERMISSION_CLASSES": ["rest_framework.permissions.IsAuthenticated"],
│           "DEFAULT_AUTHENTICATION_CLASSES": (
│       
│          
│               "nodes.custom_auth.ForceTokenUserJWTAuthentication", # <<< YOUR CUSTOM AUTH CLASS
│           ),
│           
│           'DEFAULT_THROTTLE_CLASSES': (
│               'rest_framework.throttling.AnonRateThrottle',
│               'rest_framework.throttling.UserRateThrottle'
│           ),
│           'DEFAULT_THROTTLE_RATES': {
│               'anon': '100/day',  # Adjust as needed for unauthenticated requests
│               'user': '20000/day' # Adjust as needed for authenticated requests
│           }
│       }
│       
│       
│       SIMPLE_JWT = {
│       
│           "SIGNING_KEY": JWT_SECRET_KEY,  # <<< USE DJANGO'S SECRET_KEY LOADED FROM ENV
│           "VERIFYING_KEY": JWT_SECRET_KEY,
│           "ISSUER": os.getenv('JWT_ISSUER', "https://ms1.auth-service.com"), # MUST match MS1's issuer
│           "AUTH_HEADER_TYPES": ("Bearer",),
│           "ACCESS_TOKEN_LIFETIME": timedelta(minutes=60), # e.g., 1 hour
│           "REFRESH_TOKEN_LIFETIME": timedelta(days=1),    # e.g., 1 day
│           "LEEWAY": timedelta(seconds=10),
│           "ALGORITHM": "HS256",
│           
│           # --- Settings related to interpreting the token payload ---
│           """
│       "USER_ID_CLAIM": "user_id": (Your Specific Question)
│        This is a critical instruction. It tells simple-jwt:
│          "When you parse the token's payload (the data inside),
│            the claim that contains the user's primary identifier is named 'user_id'."
│              Your MS1's CustomTokenObtainPairSerializer probably adds a claim with this name.
│           """
│       
│           "USER_ID_CLAIM": "user_id",
│           "USER_ID_FIELD": "id",
│           "TOKEN_USER_CLASS": "rest_framework_simplejwt.models.TokenUser", # Explicitly use TokenUse
│       
│           # --- Settings for features MS2 likely DOES NOT use ---
│           "UPDATE_LAST_LOGIN": False,
│           "ROTATE_REFRESH_TOKENS": False,
│           "BLACKLIST_AFTER_ROTATION": False, 
│       
│       }
│       
│       RABBITMQ_URL = os.getenv('RABBITMQ_URL', 'amqp://guest:guest@localhost:5672/')
│   ]
│   urls.py
│   [
│       
│       from django.contrib import admin
│       from django.urls import path, include
│       
│       urlpatterns = [
│           # Wrap all paths under the 'ms4/' prefix
│           path('ms4/', include([
│               path('admin/', admin.site.urls),
│               path('api/v1/', include('nodes.urls')),
│               path('internal/v1/', include('nodes_internals.internal_urls')), # <-- ADD INTERNAL URLS
│           ]))
│       ]
│   ]
│   wsgi.py
│   [
│       """
│       WSGI config for MS4 project.
│       
│       It exposes the WSGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
│       """
│       
│       import os
│       
│       from django.core.wsgi import get_wsgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│       
│       application = get_wsgi_application()
│       
│   ]
│   db.sqlite3
│   [
│       [Binary file - content not shown]
│   ]
│   manage.py
│   [
│       #!/usr/bin/env python
│       """Django's command-line utility for administrative tasks."""
│       import os
│       import sys
│       
│       
│       def main():
│           """Run administrative tasks."""
│           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│           try:
│               from django.core.management import execute_from_command_line
│           except ImportError as exc:
│               raise ImportError(
│                   "Couldn't import Django. Are you sure it's installed and "
│                   "available on your PYTHONPATH environment variable? Did you "
│                   "forget to activate a virtual environment?"
│               ) from exc
│           execute_from_command_line(sys.argv)
│       
│       
│       if __name__ == '__main__':
│           main()
│       
│   ]
├───messaging
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       
│       # Register your models here.
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MessagingConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'messaging'
│       
│   ]
│   event_publisher.py
│   [
│       # MS4/messaging/event_publisher.py
│       
│       from .rabbitmq_client import rabbitmq_client
│       
│       class NodeEventPublisher:
│           def publish_nodes_for_project_deleted(self, project_id: str):
│               """
│               Publishes a confirmation that all Nodes for a given Project
│               have been successfully deleted, fulfilling its part of the saga.
│               """
│               event_name = "resource.for_project.deleted.NodeService"
│               payload = {
│                   "project_id": str(project_id),
│                   "service_name": "NodeService"
│               }
│               rabbitmq_client.publish(
│                   exchange_name='project_events',
│                   routing_key=event_name,
│                   body=payload
│               )
│       
│       # Create a single instance for the application to use
│       node_event_publisher = NodeEventPublisher()
│   ]
│   ├───management
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   └───commands
│   │       __init__.py
│   │       [
│   │           
│   │       ]
│   │       run_dependency_update_worker.py
│   │       [
│   │           import pika
│   │           import json
│   │           import time
│   │           from django.core.management.base import BaseCommand
│   │           from django.conf import settings
│   │           from django.db import transaction
│   │           
│   │           from nodes.models import Node, NodeStatus
│   │           from nodes.services import NodeService
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Listens for resource events to proactively update and validate nodes.'
│   │           
│   │               def handle(self, *args, **options):
│   │                   rabbitmq_url = settings.RABBITMQ_URL
│   │                   while True:
│   │                       try:
│   │                           connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
│   │                           channel = connection.channel()
│   │           
│   │                           exchange_name = 'resource_events'
│   │                           channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│   │                           
│   │                           queue_name = 'node_dependency_update_queue'
│   │                           channel.queue_declare(queue=queue_name, durable=True)
│   │                           
│   │                           bindings = [
│   │                               'model.deleted',
│   │                               'tool.deleted',
│   │                               'model.capabilities.updated'
│   │                           ]
│   │                           
│   │                           for binding_key in bindings:
│   │                               self.stdout.write(f"Binding queue '{queue_name}' to exchange '{exchange_name}' with key '{binding_key}'...")
│   │                               channel.queue_bind(
│   │                                   exchange=exchange_name,
│   │                                   queue=queue_name,
│   │                                   routing_key=binding_key
│   │                               )
│   │                           
│   │                           self.stdout.write(self.style.SUCCESS(' [*] Node dependency worker is waiting for messages.'))
│   │                           channel.basic_consume(queue=queue_name, on_message_callback=self.callback)
│   │                           channel.start_consuming()
│   │           
│   │                       except pika.exceptions.AMQPConnectionError:
│   │                           self.stderr.write(self.style.ERROR('Connection to RabbitMQ failed. Retrying in 5 seconds...'))
│   │                           time.sleep(5)
│   │                       except KeyboardInterrupt:
│   │                           self.stdout.write(self.style.WARNING('Worker stopped.'))
│   │                           break
│   │           
│   │               def callback(self, ch, method, properties, body):
│   │                   try:
│   │                       data = json.loads(body)
│   │                       routing_key = method.routing_key
│   │                       
│   │                       with transaction.atomic():
│   │                           if routing_key == 'model.deleted':
│   │                               self.handle_model_deletion(data.get('model_id'))
│   │                           elif routing_key == 'tool.deleted':
│   │                               self.handle_tool_deletion(data.get('tool_id'))
│   │                           elif routing_key == 'model.capabilities.updated':
│   │                               self.handle_capabilities_update(data.get('model_id'), data.get('new_capabilities'))
│   │                   
│   │                   except json.JSONDecodeError:
│   │                       self.stderr.write(self.style.ERROR(f"Could not decode message body: {body}"))
│   │                   except Exception as e:
│   │                       self.stderr.write(self.style.ERROR(f"An unexpected error occurred in callback: {e}"))
│   │                   
│   │                   ch.basic_ack(delivery_tag=method.delivery_tag)
│   │           
│   │               def handle_model_deletion(self, model_id):
│   │                   if not model_id: return
│   │                   nodes = Node.objects.select_for_update().filter(configuration__model_config__model_id=model_id)
│   │                   count = nodes.update(status=NodeStatus.INACTIVE)
│   │                   self.stdout.write(f"Inactivated {count} nodes due to deletion of model {model_id}")
│   │           
│   │               def handle_tool_deletion(self, tool_id):
│   │                   if not tool_id: return
│   │                   candidate_nodes = Node.objects.select_for_update().filter(
│   │                       status__in=[NodeStatus.ACTIVE, NodeStatus.ALTERED],
│   │                       configuration__has_key='tool_config',
│   │                       configuration__tool_config__has_key='tool_ids'
│   │                   )
│   │                   nodes_to_process = []
│   │                   for node in candidate_nodes:
│   │                       tool_ids = node.configuration.get('tool_config', {}).get('tool_ids', [])
│   │                       if isinstance(tool_ids, list) and tool_id in tool_ids:
│   │                           nodes_to_process.append(node)
│   │                   
│   │                   if nodes_to_process:
│   │                       for node in nodes_to_process:
│   │                           node.configuration['tool_config']['tool_ids'].remove(tool_id)
│   │                           node.status = NodeStatus.ALTERED
│   │                           node.save()
│   │                       self.stdout.write(f"Altered and healed {len(nodes_to_process)} nodes for deleted tool {tool_id}")
│   │           
│   │               def handle_capabilities_update(self, model_id, new_capabilities):
│   │                   if not model_id or new_capabilities is None:
│   │                       return
│   │                   
│   │                   service = NodeService()
│   │                   nodes_to_update = Node.objects.select_for_update().filter(configuration__model_config__model_id=model_id)
│   │                   
│   │                   if not nodes_to_update.exists():
│   │                       self.stdout.write(f"Received capability update for model {model_id}, but no nodes are using it.")
│   │                       return
│   │           
│   │                   new_template = service._generate_config_template_from_capabilities(model_id, new_capabilities)
│   │           
│   │                   for node in nodes_to_update:
│   │                       final_config = new_template.copy()
│   │                       old_config = node.configuration
│   │                       
│   │                       if old_config.get("model_config", {}).get("parameters"):
│   │                           final_config["model_config"]["parameters"] = old_config["model_config"]["parameters"]
│   │                       if "memory_config" in final_config and "memory_config" in old_config:
│   │                           final_config["memory_config"] = old_config["memory_config"]
│   │                       if "rag_config" in final_config and "rag_config" in old_config:
│   │                           final_config["rag_config"] = old_config["rag_config"]
│   │                       if "tool_config" in final_config and "tool_config" in old_config:
│   │                           final_config["tool_config"] = old_config["tool_config"]
│   │                       
│   │                       node.configuration = final_config
│   │                       node.status = NodeStatus.ACTIVE
│   │                       node.save()
│   │                       
│   │                   self.stdout.write(f"Proactively updated {nodes_to_update.count()} nodes for model {model_id} capability change.")
│   │       ]
│   │       run_project_cleanup_worker.py
│   │       [
│   │           # MS4/messaging/management/commands/run_project_cleanup_worker.py
│   │           
│   │           import pika
│   │           import json
│   │           import time
│   │           from django.core.management.base import BaseCommand
│   │           from django.conf import settings
│   │           from nodes.models import Node
│   │           from messaging.event_publisher import node_event_publisher # <-- USE THE STANDARD PUBLISHER
│   │           
│   │           def handle_project_deletion(project_id: str):
│   │               """
│   │               The core business logic for cleaning up nodes. This is idempotent.
│   │               """
│   │               print(f" [!] Received request to delete nodes for project: {project_id}")
│   │               
│   │               nodes_deleted, _ = Node.objects.filter(project_id=project_id).delete()
│   │               
│   │               print(f" [✓] Deleted {nodes_deleted} nodes for project {project_id}.")
│   │               
│   │               # After successful deletion, publish the confirmation event using the standard client.
│   │               node_event_publisher.publish_nodes_for_project_deleted(project_id)
│   │           
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Runs a RabbitMQ worker to listen for project deletion events.'
│   │           
│   │               def handle(self, *args, **options):
│   │                   rabbitmq_url = settings.RABBITMQ_URL
│   │                   while True:
│   │                       try:
│   │                           connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
│   │                           channel = connection.channel()
│   │               
│   │                           channel.exchange_declare(exchange='project_events', exchange_type='topic', durable=True)
│   │                           
│   │                           queue_name = 'node_project_cleanup_queue'
│   │                           channel.queue_declare(queue=queue_name, durable=True)
│   │                           
│   │                           routing_key = 'project.deletion.initiated'
│   │                           channel.queue_bind(exchange='project_events', queue=queue_name, routing_key=routing_key)
│   │               
│   │                           self.stdout.write(self.style.SUCCESS(' [*] NodeService project cleanup worker waiting for messages.'))
│   │               
│   │                           def callback(ch, method, properties, body):
│   │                               try:
│   │                                   data = json.loads(body)
│   │                                   project_id = data.get('project_id')
│   │                   
│   │                                   if project_id:
│   │                                       handle_project_deletion(project_id)
│   │                               except Exception as e:
│   │                                   self.stderr.write(self.style.ERROR(f" [!] Error handling project deletion for {project_id}: {e}"))
│   │                                   # Nack logic would go here in production
│   │                               
│   │                               ch.basic_ack(delivery_tag=method.delivery_tag)
│   │               
│   │                           channel.basic_consume(queue=queue_name, on_message_callback=callback)
│   │                           channel.start_consuming()
│   │                       except pika.exceptions.AMQPConnectionError:
│   │                           self.stderr.write(self.style.ERROR('Connection to RabbitMQ failed. Retrying in 5 seconds...'))
│   │                           time.sleep(5)
│   │                       except KeyboardInterrupt:
│   │                           self.stdout.write(self.style.WARNING('Worker stopped.'))
│   │                           break
│   │       ]
│   models.py
│   [
│       from django.db import models
│       
│       # Create your models here.
│       
│   ]
│   rabbitmq_client.py
│   [
│       # MS4/messaging/rabbitmq_client.py
│       
│       import pika
│       import json
│       import threading
│       from django.conf import settings
│       
│       class RabbitMQClient:
│           """
│           A robust, thread-safe RabbitMQ client that manages connections on a per-thread
│           basis and uses a fresh channel for each publishing operation. This is the
│           recommended pattern for use in multi-threaded applications like Django.
│           """
│           _thread_local = threading.local()
│       
│           def _get_connection(self):
│               """
│               Gets or creates a dedicated connection for the current thread.
│               This method is the core of the thread-safety mechanism.
│               """
│               if not hasattr(self._thread_local, 'connection') or self._thread_local.connection.is_closed:
│                   print(f"Thread {threading.get_ident()}: (MS4) No active RabbitMQ connection. Creating new one...")
│                   try:
│                       params = pika.URLParameters(settings.RABBITMQ_URL)
│                       self._thread_local.connection = pika.BlockingConnection(params)
│                       print(f"Thread {threading.get_ident()}: (MS4) Connection successful.")
│                   except pika.exceptions.AMQPConnectionError as e:
│                       print(f"CRITICAL: (MS4) Thread {threading.get_ident()} failed to connect to RabbitMQ: {e}")
│                       raise
│               return self._thread_local.connection
│       
│           def publish(self, exchange_name, routing_key, body):
│               """
│               Publishes a message using a short-lived, dedicated channel.
│               This is the safest way to publish from multiple threads.
│               """
│               try:
│                   connection = self._get_connection()
│                   with connection.channel() as channel:
│                       channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│                       message_body = json.dumps(body, default=str)
│                       channel.basic_publish(
│                           exchange=exchange_name,
│                           routing_key=routing_key,
│                           body=message_body,
│                           properties=pika.BasicProperties(
│                               content_type='application/json',
│                               delivery_mode=pika.DeliveryMode.Persistent,
│                           )
│                       )
│                       print(f" [x] (MS4) Sent '{routing_key}':'{message_body}'")
│               except (pika.exceptions.AMQPError, OSError) as e:
│                   print(f"Error publishing message from MS4: {e}.")
│                   if hasattr(self._thread_local, 'connection'):
│                       self._thread_local.connection.close()
│                   raise
│       
│       rabbitmq_client = RabbitMQClient()
│   ]
│   tests.py
│   [
│       from django.test import TestCase
│       
│       # Create your tests here.
│       
│   ]
│   views.py
│   [
│       from django.shortcuts import render
│       
│       # Create your views here.
│       
│   ]
├───nodes
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       from .models import Node
│       # Register your models here.
│       admin.site.register(Node)
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class NodesConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'nodes'
│       
│   ]
│   custom_auth.py
│   [
│       from rest_framework_simplejwt.authentication import JWTAuthentication
│       from rest_framework_simplejwt.models import TokenUser # Import TokenUser
│       from rest_framework_simplejwt.settings import api_settings as simple_jwt_settings
│       from django.utils.translation import gettext_lazy as _
│       from rest_framework_simplejwt.exceptions import InvalidToken
│       
│       class ForceTokenUserJWTAuthentication(JWTAuthentication):
│           def get_user(self, validated_token):
│               """
│               Returns a TokenUser instance based on the validated token.
│               Bypasses any local database User lookup for JWT authentication.
│               """
│               try:
│                   # simple_jwt_settings.USER_ID_CLAIM refers to what you set in settings.py
│                   # e.g., "user_id"
│                   user_id = validated_token[simple_jwt_settings.USER_ID_CLAIM]
│               except KeyError:
│                   raise InvalidToken(_("Token contained no recognizable user identification"))
│       
│               # Correct way to instantiate TokenUser: pass the validated_token
│               # The TokenUser class will internally use USER_ID_CLAIM and USER_ID_FIELD
│               # from your SIMPLE_JWT settings to extract the user ID and set its 'id' or 'pk'.
│               token_user = TokenUser(validated_token)
│       
│               # The TokenUser's 'id' (and 'pk') attribute should now be populated correctly
│               # by its own __init__ method based on the validated_token and your SIMPLE_JWT settings
│               # for USER_ID_CLAIM and USER_ID_FIELD.
│       
│               # Example: If you wanted to verify or access it (not strictly necessary here)
│               # print(f"TokenUser ID: {token_user.id}, TokenUser PK: {token_user.pk}")
│       
│               return token_user
│   ]
│   models.py
│   [
│       # in nodes/models.py
│       
│       import uuid
│       from django.db import models
│       
│       
│       class NodeStatus(models.TextChoices):
│           ACTIVE = 'active', 'Active'             # Fully functional
│           ALTERED = 'altered', 'Altered'           # A non-critical dependency was removed (e.g., a tool)
│           INACTIVE = 'inactive', 'Inactive'
│           DRAFT = 'draft', 'Draft'                 # Initial state, not yet configured
│       
│       
│       class Node(models.Model):
│           id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
│           
│       
│           status = models.CharField(
│               max_length=20,
│               choices=NodeStatus.choices,
│               default=NodeStatus.DRAFT, # <-- Set default to DRAFT
│               db_index=True,
│               help_text="The current status of the node (e.g., active, altered, inactive)."
│           )
│       
│       
│           project_id = models.UUIDField(
│               db_index=True,
│               help_text="The Project this Node belongs to."
│           )
│           
│           # ------------------ THE ONE AND ONLY FIX IS HERE ------------------
│           owner_id = models.UUIDField(
│               db_index=True,
│               help_text="The UUID of the user who owns this Node. Corresponds to the User UUID in the Auth service JWT."
│           )
│           # --------------------------------------------------------------------
│           
│           name = models.CharField(
│               max_length=255,
│               help_text="The user-defined name for this Node (e.g., 'My Research Agent')."
│           )
│       
│           configuration = models.JSONField(
│               default=dict,
│               help_text="The complete configuration blueprint for this node's behavior."
│           )
│       
│           created_at = models.DateTimeField(auto_now_add=True, editable=False)
│           updated_at = models.DateTimeField(auto_now=True)
│       
│           class Meta:
│               ordering = ['-created_at']
│               verbose_name = "Node"
│               verbose_name_plural = "Nodes"
│       
│           def __str__(self):
│               return f"{self.name} (Project: {self.project_id})"
│   ]
│   permissions.py
│   [
│       from rest_framework import permissions
│       import uuid # Import the uuid module
│       
│       class IsOwner(permissions.BasePermission):
│           """
│           Custom permission to only allow owners of an object to edit or view it.
│           This version is robust against type mismatches (str vs. UUID).
│           """
│           def has_object_permission(self, request, view, obj):
│               """
│               Return `True` if permission is granted, `False` otherwise.
│               """
│               # obj is the Node instance from the database. obj.owner_id is a UUID object.
│               # request.user is the TokenUser from the JWT. request.user.id can be str or UUID.
│               
│               # --- THE FIX IS HERE ---
│               # We convert both the object's owner_id and the user's id to strings
│               # before comparing them. This ensures a reliable, type-safe comparison.
│               try:
│                   # Ensure the object's owner_id can be represented as a string
│                   obj_owner_id_str = str(obj.owner_id)
│                   
│                   # Ensure the request user's ID can be represented as a string
│                   request_user_id_str = str(request.user.id)
│                   
│                   return obj_owner_id_str == request_user_id_str
│                   
│               except (TypeError, AttributeError):
│                   # If for some reason either field is missing or invalid, deny permission.
│                   return False
│   ]
│   repository.py
│   [
│       # MS4/nodes/repository.py
│       
│       from typing import List, Optional
│       import uuid
│       from .models import Node, NodeStatus
│       
│       class NodeRepository:
│           """
│           Acts as a data access layer for the Node model.
│           All direct database interactions for Nodes should be in this class.
│           """
│       
│           def find_by_id(self, node_id: uuid.UUID) -> Optional[Node]:
│               """
│               Finds a single Node instance by its primary key.
│       
│               Returns:
│                   The Node instance or None if not found.
│               """
│               try:
│                   return Node.objects.get(id=node_id)
│               except Node.DoesNotExist:
│                   return None
│       
│           def find_by_project(self, project_id: uuid.UUID) -> List[Node]:
│               """
│               Finds all Node instances belonging to a specific project.
│       
│               Returns:
│                   A list of Node instances.
│               """
│               return list(Node.objects.filter(project_id=project_id))
│       
│           def create(self, *, project_id: uuid.UUID, owner_id: uuid.UUID, name: str) -> Node:
│               """
│               Creates and saves a new DRAFT Node instance in the database.
│               Configuration is intentionally left empty.
│               """
│               return Node.objects.create(
│                   project_id=project_id,
│                   owner_id=owner_id,
│                   name=name
│                   # Status defaults to 'draft' from the model definition
│               )
│       
│           def update(self, node: Node, name: str, configuration: dict, status: str = None) -> Node:
│               # ... (This method is already correct from our previous step)
│               node.name = name
│               node.configuration = configuration
│               update_fields = ['name', 'configuration', 'updated_at']
│               if status and status in NodeStatus.values:
│                   node.status = status
│                   update_fields.append('status')
│               node.save(update_fields=update_fields)
│               return node
│       
│           def delete(self, node: Node) -> None:
│               """
│               Deletes a Node instance from the database.
│               """
│               node.delete()
│   ]
│   serializers.py
│   [
│       from rest_framework import serializers
│       from .models import Node
│       
│       class NodeSerializer(serializers.ModelSerializer):
│           """
│           The primary, read-only serializer for displaying Node objects.
│           Used for all GET requests (list and detail).
│           """
│           class Meta:
│               model = Node
│               fields = [
│                   'id', 'project_id', 'owner_id', 'name', 'status', 
│                   'configuration', 'created_at', 'updated_at'
│               ]
│               read_only_fields = fields # Make all fields read-only by default
│       
│       class NodeDraftCreateSerializer(serializers.Serializer):
│           """
│           Used ONLY for the STAGE 1 `POST /nodes/draft/` endpoint.
│           Strictly validates only the name and project_id.
│           """
│           name = serializers.CharField(max_length=255, required=True)
│           project_id = serializers.UUIDField(required=True)
│       
│           def to_internal_value(self, data):
│               # This ensures that ONLY 'name' and 'project_id' are processed.
│               # Any other keys in the user's request body are completely discarded.
│               validated_data = {
│                   'name': data.get('name'),
│                   'project_id': data.get('project_id'),
│               }
│               return super().to_internal_value(validated_data)
│       
│       class NodeConfigureModelSerializer(serializers.Serializer):
│           """
│           Used ONLY for the STAGE 2 `POST /nodes/{pk}/configure-model/` endpoint.
│           Validates only the model_id.
│           """
│           model_id = serializers.UUIDField(required=True)
│       
│       class NodeUpdateSerializer(serializers.Serializer):
│           """
│           Used for the `PUT /nodes/{pk}/` endpoint.
│           Allows updates to the name and the configuration values.
│       
│           The primary security function of this serializer is to explicitly
│           REJECT any request that attempts to change the 'model_id' within the configuration.
│           All other structural and logical validation is handled by the service layer.
│           """
│           name = serializers.CharField(max_length=255, required=True)
│           configuration = serializers.JSONField(required=True) # Using JSONField for maximum flexibility
│       
│           def validate_configuration(self, value):
│               """
│               Validates the incoming configuration data.
│               """
│               # Ensure the incoming data is a dictionary.
│               if not isinstance(value, dict):
│                   raise serializers.ValidationError("Configuration must be a dictionary.")
│                   
│               # THE ONE CRITICAL CHECK: Prohibit changing the model_id.
│               model_config = value.get("model_config")
│               if model_config and isinstance(model_config, dict) and "model_id" in model_config:
│                   raise serializers.ValidationError({
│                       "model_config.model_id": "Changing the model is not permitted on this endpoint. Please use the '/configure-model' endpoint to reconfigure the node with a new model."
│                   })
│               
│               # No other structural validation is needed here. The service layer's
│               # template merging and `_validate_resources` call will handle ensuring
│               # the user only provides valid keys and resource IDs.
│       
│               return value
│   ]
│   services.py
│   [
│       # in nodes/services.py
│       
│       from django.conf import settings
│       from rest_framework.exceptions import PermissionDenied, ValidationError, NotFound
│       import concurrent.futures
│       import uuid
│       
│       # These imports must correctly point to your client, repository, and model files.
│       from nodes_internals.clients import ProjectServiceClient, ModelServiceClient, ToolServiceClient #, MemoryServiceClient, KnowledgeServiceClient
│       from .repository import NodeRepository
│       from .models import Node, NodeStatus
│       
│       class NodeService:
│           """
│           The service layer for handling all business logic related to Nodes.
│           This is the definitive, synchronous, multi-threaded version.
│           """
│           def __init__(self):
│               # Instantiate all dependencies. In a larger app, this would use dependency injection.
│               self.node_repo = NodeRepository()
│       
│               # Instantiate all microservice clients.
│               self.project_client = ProjectServiceClient()
│               self.model_client = ModelServiceClient()
│               self.tool_client = ToolServiceClient()
│               # self.memory_client = MemoryServiceClient()
│               # self.knowledge_client = KnowledgeServiceClient()
│       
│       
│           def _validate_resources(self, jwt_token: str, project_id: str, configuration: dict):
│               """
│               Runs all validation checks against other microservices in parallel using a thread pool.
│               """
│               # If validation is disabled in settings, skip this entire process.
│               if not settings.NODE_SERVICE_VALIDATION_ENABLED:
│                   return
│                   
│               with concurrent.futures.ThreadPoolExecutor() as executor:
│                   futures = []
│                   
│                   # --- Submit All Validation Tasks Concurrently ---
│       
│                   # Task 1: Check project ownership.
│           #-->>>>>>>>>>>>>>>>>>     
│                   futures.append(executor.submit(self.project_client.authorize_user, jwt_token, project_id))
│           #-->>>>>>>>>>>>>>>>>>        
│       
│       
│                   # Task 2: Validate the AI Model.
│                   model_id = configuration.get("model_config", {}).get("model_id")
│                   if not model_id:
│                       raise ValidationError("Configuration must include 'model_config' with a 'model_id'.")
│                   futures.append(executor.submit(self.model_client.validate_model, jwt_token, model_id))
│       
│       
│                   # Task 3: Validate the Tools.
│                   tool_config = configuration.get("tool_config", {})
│                   if "tool_ids" in tool_config and tool_config["tool_ids"]:
│                       futures.append(
│                           executor.submit(
│                               self.tool_client.validate_tools,
│                               jwt_token,
│                               tool_config["tool_ids"]
│                           )
│                       )
│       
│                   # Task 3 & 4 (Future): Validate Memory and Knowledge resources.
│                   # These are commented out but show how to extend the pattern.
│                   # memory_config = configuration.get("memory_config", {})
│                   # if memory_config.get("is_enabled", False):
│                   #     bucket_id = memory_config.get("bucket_id")
│                   #     if not bucket_id:
│                   #         raise ValidationError("Memory is enabled, but no 'bucket_id' was provided.")
│                   #     futures.append(executor.submit(self.memory_client.validate_bucket, jwt_token, project_id, bucket_id))
│                   
│                   # knowledge_config = configuration.get("knowledge_config", {})
│                   # if knowledge_config.get("is_enabled", False):
│                   #     collection_id = knowledge_config.get("collection_id")
│                   #     if not collection_id:
│                   #         raise ValidationError("Knowledge is enabled, but no 'collection_id' was provided.")
│                   #     futures.append(executor.submit(self.knowledge_client.validate_collection, jwt_token, project_id, collection_id))
│       
│                   # --- Wait for all tasks to complete and check for any failures ---
│                   for future in concurrent.futures.as_completed(futures):
│                       # future.result() will do nothing if the task succeeded, but will
│                       # re-raise any exception (like PermissionDenied, NotFound) that
│                       # occurred in the thread, causing this entire method to fail.
│                       future.result()
│       
│       
│           def get_nodes_for_project(self, *, project_id: uuid.UUID, user_id: uuid.UUID, jwt_token: str) -> list[Node]:
│               """
│               The use case for listing all nodes in a project.
│               It first authorizes project-level access, then fetches the data.
│               """
│               # Step 1: Authorize that the user can even view this project's contents.
│               self.project_client.authorize_user(jwt_token, str(project_id))
│               
│               # Step 2: If authorized, retrieve the nodes from the local database.
│               return self.node_repo.find_by_project(project_id)
│       
│       
│       
│           def _generate_config_template_from_capabilities(self, model_id: str, capabilities: list) -> dict:
│               """
│               Generates a valid, empty configuration template based on a model's capabilities.
│               """
│               master_config = {"model_config": {"model_id": str(model_id)}}
│               if "text" in capabilities:
│                   master_config["memory_config"] = {"is_enabled": False, "bucket_id": None}
│                   master_config["rag_config"] = {"is_enabled": False, "collection_id": None}
│               if "tool_use" in capabilities:
│                   master_config["tool_config"] = {"tool_ids": []}
│               # Add more capability-based configurations here...
│               return master_config
│       
│           def create_draft_node(self, *, jwt_token: str, user_id: uuid.UUID, project_id: uuid.UUID, name: str) -> Node:
│               # 1. Authorize project ownership BEFORE creating anything.
│               self.project_client.authorize_user(jwt_token, str(project_id))
│       
│               # 2. If authorized, create the draft node.
│               # The repository will save it with status='draft' and empty config by default.
│               return self.node_repo.create(
│                   project_id=project_id,
│                   owner_id=user_id,
│                   name=name
│               )
│       
│           # --- NEW METHOD: STAGE 2 ---
│           def configure_node_model(self, *, jwt_token: str, node: Node, model_id: uuid.UUID) -> Node:
│               """
│               Configures or reconfigures a node with a new model.
│               This process is "forward-looking" and resilient to the old model being deleted.
│               """
│               # 1. Fetch NEW model's capabilities. This is the only external call needed.
│               #    It also validates user access to the new model.
│               new_capabilities = self.model_client.get_model_capabilities(jwt_token, str(model_id))
│               
│               # 2. Generate the ideal, valid template for the NEW model.
│               new_template = self._generate_config_template_from_capabilities(model_id, new_capabilities)
│               
│               # 3. Get the OLD configuration from the node's own storage.
│               #    We trust this as the user's last known good configuration.
│               old_config = node.configuration
│               
│               # 4. Perform best-effort migration of user settings onto the new template.
│               final_config = new_template.copy()
│               
│               # Migrate parameters (always safe to carry over)
│               if old_config.get("model_config", {}).get("parameters"):
│                   final_config["model_config"]["parameters"] = old_config["model_config"]["parameters"]
│       
│               # Migrate memory config IF the new template supports it
│               if "memory_config" in final_config and "memory_config" in old_config:
│                   final_config["memory_config"] = old_config["memory_config"]
│       
│               # Migrate RAG config IF the new template supports it
│               if "rag_config" in final_config and "rag_config" in old_config:
│                   final_config["rag_config"] = old_config["rag_config"]
│               
│               # Migrate tools IF the new template supports them
│               if "tool_config" in final_config and "tool_config" in old_config:
│                   final_config["tool_config"] = old_config["tool_config"]
│               
│               # 5. Save the result. This action always "heals" the node to an ACTIVE state.
│               return self.node_repo.update(
│                   node=node,
│                   name=node.name,
│                   configuration=final_config,
│                   status=NodeStatus.ACTIVE
│               )
│       
│           # --- UPDATED METHOD: FINAL UPDATE ---
│           def update_node(self, *, jwt_token: str, node: Node, name: str, configuration: dict) -> Node:
│               # --- VALIDATION GAUNTLET ---
│       
│               # 1. Fetch the node's TRUSTED configuration template from the DB.
│               trusted_config = node.configuration
│               trusted_model_id = trusted_config.get("model_config", {}).get("model_id")
│       
│               # 2. Prevent Model Change (Problem 2)
│               submitted_model_id = configuration.get("model_config", {}).get("model_id")
│               if submitted_model_id and submitted_model_id != trusted_model_id:
│                   raise ValidationError("Changing the model is not allowed through this endpoint. Please use the '/configure-model' endpoint instead.")
│       
│               # 3. Validate Submitted Structure (Problem 1 - Arbitrary Injection)
│               #    Ensure the user is not submitting keys that are not in the trusted template.
│               for key in configuration:
│                   if key not in trusted_config:
│                       raise ValidationError(f"Configuration key '{key}' is not supported by the current model.")
│       
│               # 4. Deep Merge and Final Validation
│               #    Start with a copy of the trusted config and update it with user values.
│               final_config = trusted_config.copy()
│               
│               # Update parameters
│               if "parameters" in configuration.get("model_config", {}):
│                   final_config["model_config"]["parameters"] = configuration["model_config"]["parameters"]
│               
│               # Update memory, RAG, tools, etc.
│               if "memory_config" in final_config and "memory_config" in configuration:
│                   final_config["memory_config"] = configuration["memory_config"]
│               if "rag_config" in final_config and "rag_config" in configuration:
│                   final_config["rag_config"] = configuration["rag_config"]
│               if "tool_config" in final_config and "tool_config" in configuration:
│                   final_config["tool_config"] = configuration["tool_config"]
│               
│               # 5. Perform final cross-service validation on the merged data
│               #    This checks if the provided tool_ids, bucket_ids, etc. are valid.
│               self._validate_resources(jwt_token, str(node.project_id), final_config)
│                   
│               # 6. If all validations pass, save the changes.
│               return self.node_repo.update(
│                   node=node,
│                   name=name,
│                   configuration=final_config,
│                   status=NodeStatus.ACTIVE
│               )
│               
│           def delete_node(self, node: Node):
│               """
│               The use case for deleting a node. The view handles ownership check.
│               This service delegates the deletion to the repository.
│               """
│               self.node_repo.delete(node)
│   ]
│   tests.py
│   [
│       from django.test import TestCase
│       
│       # Create your tests here.
│       
│   ]
│   urls.py
│   [
│       # MS4/nodes/urls.py
│       from django.urls import path
│       from .views import (
│           NodeListAPIView,
│           NodeDetailAPIView,
│           NodeDraftCreateAPIView,
│           NodeConfigureModelAPIView,
│           #ResourceDeletionHookAPIView
│       )
│       
│       app_name = 'nodes'
│       
│       urlpatterns = [
│           # --- WORKFLOW STEP 1: Create a draft node placeholder ---
│           path('nodes/draft/', NodeDraftCreateAPIView.as_view(), name='node-draft-create'),
│           
│           # --- WORKFLOW STEP 2: Configure the draft node with a model ---
│           path('nodes/<uuid:pk>/configure-model/', NodeConfigureModelAPIView.as_view(), name='node-configure-model'),
│       
│           # --- STANDARD OPERATIONS ---
│           # List nodes within a specific project
│           path('projects/<uuid:project_id>/nodes/', NodeListAPIView.as_view(), name='node-list'),
│       
│           # Manage a single, fully configured node (GET details, PUT updates, DELETE)
│           path('nodes/<uuid:pk>/', NodeDetailAPIView.as_view(), name='node-detail'),
│       
│       
│       ]
│   ]
│   views.py
│   [
│       # MS4/nodes/views.py
│       
│       from rest_framework.views import APIView
│       from rest_framework.response import Response
│       from rest_framework import status, permissions
│       from rest_framework.exceptions import NotFound, ValidationError, PermissionDenied
│       from django.db import transaction
│       import uuid
│       import json
│       
│       from .models import Node, NodeStatus
│       from .repository import NodeRepository
│       from .services import NodeService
│       from .serializers import (
│           NodeSerializer, 
│           NodeUpdateSerializer, 
│           NodeDraftCreateSerializer, 
│           NodeConfigureModelSerializer
│       )
│       from .permissions import IsOwner
│       
│       # --- STAGE 1 & 2: NODE CREATION WORKFLOW VIEWS ---
│       
│       class NodeDraftCreateAPIView(APIView):
│           """
│           Handles STAGE 1 of node creation: creating a placeholder 'draft' node.
│           Endpoint: POST /ms4/api/v1/nodes/draft/
│           """
│           permission_classes = [permissions.IsAuthenticated]
│           
│           def post(self, request):
│               serializer = NodeDraftCreateSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│               
│               service = NodeService()
│               try:
│                   draft_node = service.create_draft_node(
│                       jwt_token=str(request.auth),
│                       user_id=request.user.id,
│                       project_id=serializer.validated_data['project_id'],
│                       name=serializer.validated_data['name']
│                   )
│                   # Use the general purpose serializer for the response
│                   response_serializer = NodeSerializer(draft_node)
│                   return Response(response_serializer.data, status=status.HTTP_201_CREATED)
│               except (PermissionDenied, NotFound) as e:
│                   return Response({"error": str(e)}, status=e.status_code)
│               except Exception as e:
│                   # Catch any other unexpected errors from the service layer
│                   return Response({"error": f"An unexpected server error occurred: {e}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
│       
│       class NodeConfigureModelAPIView(APIView):
│           """
│           Handles STAGE 2 of node creation: linking a model and generating the config template.
│           Endpoint: POST /ms4/api/v1/nodes/{pk}/configure-model/
│           """
│           permission_classes = [permissions.IsAuthenticated, IsOwner] # IsOwner handles ownership check
│           
│           def get_object(self, pk):
│               repo = NodeRepository()
│               node = repo.find_by_id(pk)
│               if not node:
│                   raise NotFound("Node not found.")
│               
│               # This will run the IsOwner permission check on the fetched object
│               self.check_object_permissions(self.request, node)
│       
│             
│               return node
│       
│           def post(self, request, pk):
│               print(f"Configuring model for node with ID: {pk}")
│               node = self.get_object(pk)
│               print(f"Node found: {node.id}")
│               serializer = NodeConfigureModelSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│       
│               service = NodeService()
│               try:
│                   print(f"Configuring node {node.id} with model ID: {serializer.validated_data['model_id']}")
│                   configured_node = service.configure_node_model(
│                       jwt_token=str(request.auth),
│                       node=node,
│                       model_id=serializer.validated_data['model_id']
│                   )
│                   print(f"Node {node.id} configured successfully with model ID: {serializer.validated_data['model_id']}")
│                   response_serializer = NodeSerializer(configured_node)
│                   return Response(response_serializer.data, status=status.HTTP_200_OK)
│               except (PermissionDenied, NotFound, ValidationError) as e:
│                   return Response({"error": str(e)}, status=e.status_code)
│               except Exception as e:
│                   return Response({"error": f"An unexpected server error occurred: {e}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
│       
│       # --- STANDARD CRUD VIEWS FOR CONFIGURED NODES ---
│       
│       class NodeListAPIView(APIView):
│           """
│           Handles LISTING nodes within a project. The POST/Create logic has been moved.
│           Endpoint: GET /ms4/api/v1/projects/{project_id}/nodes/
│           """
│           permission_classes = [permissions.IsAuthenticated]
│       
│           def get(self, request, project_id):
│               service = NodeService()
│               jwt_token = str(request.auth)
│               
│               try:
│                   nodes = service.get_nodes_for_project(
│                       project_id=project_id, 
│                       user_id=request.user.id, 
│                       jwt_token=jwt_token
│                   )
│                   serializer = NodeSerializer(nodes, many=True)
│                   return Response(serializer.data, status=status.HTTP_200_OK)
│               except (PermissionDenied, NotFound) as e:
│                   return Response({"error": str(e)}, status=e.status_code)
│       
│       class NodeDetailAPIView(APIView):
│           """
│           Handles GET, PUT, DELETE for a single, fully configured node.
│           Endpoints:
│           - GET /ms4/api/v1/nodes/{pk}/
│           - PUT /ms4/api/v1/nodes/{pk}/
│           - DELETE /ms4/api/v1/nodes/{pk}/
│           """
│           permission_classes = [permissions.IsAuthenticated, IsOwner]
│       
│           def get_object(self, pk):
│               repo = NodeRepository()
│               node = repo.find_by_id(pk)
│               if not node:
│                   raise NotFound("Node not found.")
│               self.check_object_permissions(self.request, node)
│               return node
│               
│           def get(self, request, pk):
│               node = self.get_object(pk)
│               serializer = NodeSerializer(node)
│               return Response(serializer.data)
│       
│           def put(self, request, pk):
│               service = NodeService()
│               node_to_update = self.get_object(pk)
│               
│               serializer = NodeUpdateSerializer(data=request.data)
│               serializer.is_valid(raise_exception=True)
│               
│               jwt_token = str(request.auth)
│               validated_data = serializer.validated_data
│               
│               updated_node = service.update_node(
│                   jwt_token=jwt_token,
│                   node=node_to_update,
│                   name=validated_data['name'],
│                   configuration=validated_data['configuration']
│               )
│               
│               response_serializer = NodeSerializer(updated_node)
│               return Response(response_serializer.data)
│       
│           def delete(self, request, pk):
│               service = NodeService()
│               node_to_delete = self.get_object(pk)
│               service.delete_node(node_to_delete)
│               return Response(status=status.HTTP_204_NO_CONTENT)
│       
│       # --- INTERNAL WEBHOOK VIEW ---
│       
│       
│       """
│       class IsInternalServicePermission(permissions.BasePermission):
│           def has_permission(self, request, view):
│               # In a real system, this would be more robust (e.g., shared secret header).
│               return request.user and request.user.is_authenticated
│       
│       class ResourceDeletionHookAPIView(APIView):
│           permission_classes = [IsInternalServicePermission]
│       
│           @transaction.atomic
│           def post(self, request):
│               resource_type = request.data.get('resource_type')
│               resource_id = request.data.get('resource_id')
│       
│               if not resource_type or not resource_id:
│                   return Response({"error": "Missing 'resource_type' or 'resource_id'."}, status=status.HTTP_400_BAD_REQUEST)
│       
│               updated_count = 0
│               message = ""
│       
│               if resource_type == 'model':
│                   nodes_to_update = Node.objects.select_for_update().filter(
│                       configuration__model_config__model_id=resource_id
│                   ).exclude(status=NodeStatus.INACTIVE)
│                   updated_count = nodes_to_update.update(status=NodeStatus.INACTIVE)
│                   message = f"Inactivated {updated_count} nodes."
│               elif resource_type == 'tool':
│                   # SQLite-compatible workaround
│                   print("Checking tool configurations...")
│                   candidate_nodes = Node.objects.select_for_update().filter(
│                       status__in=[NodeStatus.ACTIVE, NodeStatus.ALTERED],
│                       configuration__has_key='tool_config',
│                       configuration__tool_config__has_key='tool_ids'
│                   )
│                   nodes_to_process = []
│                   print(f"Found {candidate_nodes.count()} candidate nodes.")
│                   for node in candidate_nodes:
│                       tool_ids = node.configuration.get('tool_config', {}).get('tool_ids', [])
│                       if isinstance(tool_ids, list) and resource_id in tool_ids:
│                           nodes_to_process.append(node)
│                   print(f"Nodes to process: {len(nodes_to_process)}")
│                   if nodes_to_process:
│                       for node in nodes_to_process:
│                           node.configuration['tool_config']['tool_ids'].remove(resource_id)
│                           node.status = NodeStatus.ALTERED
│                           node.save()
│                       updated_count = len(nodes_to_process)
│                   message = f"Removed tool and altered {updated_count} nodes."
│               else:
│                    return Response({"error": f"Unknown resource_type: {resource_type}"}, status=status.HTTP_400_BAD_REQUEST)
│               print(f"Processed {updated_count} nodes for resource type '{resource_type}' with ID '{resource_id}'.")
│               return Response(
│                   {"message": f"Processed deletion for {resource_type}:{resource_id}. {message}"},
│                   status=status.HTTP_200_OK
│               )
│       """
│   ]
├───nodes_internals
│   __init__.py
│   [
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class NodesInternalsConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'nodes_internals'
│       
│   ]
│   clients.py
│   [
│       # in nodes/clients.py
│       
│       import httpx
│       import os
│       from django.core.exceptions import ImproperlyConfigured
│       from rest_framework.exceptions import PermissionDenied, ValidationError, NotFound
│       
│       class BaseServiceClient:
│           def __init__(self, service_name: str, env_var_name: str):
│               self.service_name = service_name
│               base_url = os.getenv(env_var_name)
│               if not base_url:
│                   raise ImproperlyConfigured(f"{env_var_name} is not set in the environment.")
│               self.client = httpx.Client(base_url=base_url, timeout=10.0)
│       
│           def _handle_response(self, response: httpx.Response):
│               """
│               A centralized function to interpret HTTP responses from other services
│               and raise appropriate DRF exceptions. This version is robust and can
│               handle both dictionary and list-based error responses.
│               """
│               if 200 <= response.status_code < 300:
│                   return response.json() if response.content else None
│               
│               # --- THE FIX IS HERE ---
│               try:
│                   error_data = response.json()
│               except Exception:
│                   # If the response isn't valid JSON, use the reason phrase.
│                   error_data = response.reason_phrase
│       
│               error_message = f"Error from {self.service_name}"
│               if isinstance(error_data, dict):
│                   # Handle standard DRF error format: {"detail": "..."} or {"error": "..."}
│                   error_message = error_data.get("detail", error_data.get("error", str(error_data)))
│               elif isinstance(error_data, list):
│                   # Handle DRF validation error format: ["Error message."]
│                   error_message = ". ".join(str(item) for item in error_data)
│               elif isinstance(error_data, str):
│                   error_message = error_data
│                   
│               # Raise the appropriate exception with the formatted message.
│               if response.status_code == 403:
│                   raise PermissionDenied(error_message)
│               elif response.status_code == 404:
│                   raise NotFound(error_message)
│               elif response.status_code == 400:
│                   raise ValidationError(error_message)
│               else:
│                   # For 5xx errors or other unexpected codes.
│                   response.raise_for_status()
│       
│       
│       # --- The rest of the client classes (ProjectServiceClient, ModelServiceClient) ---
│       # --- remain exactly the same. No changes are needed there. ---
│       
│       class ProjectServiceClient(BaseServiceClient):
│           def __init__(self):
│               super().__init__("Project Service", "PROJECT_SERVICE_URL")
│       
│           def authorize_user(self, jwt_token: str, project_id: str):
│               headers = {"Authorization": f"Bearer {jwt_token}",
│                          "Host": "localhost"  }
│               internal_path = f"/ms2/internal/v1/projects/{project_id}/authorize"
│               response = self.client.get(internal_path, headers=headers)
│               self._handle_response(response)
│       
│       
│       class ModelServiceClient(BaseServiceClient):
│           def __init__(self):
│               super().__init__("Model Service", "MODEL_SERVICE_URL")
│       
│           def validate_model(self, jwt_token: str, model_id: str):
│               headers = {"Authorization": f"Bearer {jwt_token}",
│                          "Host": "localhost"  }
│               internal_path = f"/ms3/internal/v1/models/{model_id}/validate"
│               response = self.client.get(internal_path, headers=headers)
│               self._handle_response(response)
│       
│           def get_model_capabilities(self, jwt_token: str, model_id: str) -> list:
│               """
│               Fetches the capabilities for a given model_id from the Model Service.
│               """
│               print(f"Fetchinggggg capabilities for model ID: {model_id}")
│               headers = {"Authorization": f"Bearer {jwt_token}"}
│               internal_path = f"/ms3/internal/v1/models/{model_id}/capabilities/"
│               response = self.client.get(internal_path, headers=headers)
│               print(f"Response status code: {response.status_code}")
│               data = self._handle_response(response) # This will raise exceptions on failure
│               return data.get("capabilities", [])
│       
│       
│       class ToolServiceClient(BaseServiceClient):
│           def __init__(self):
│               super().__init__("Tool Service", "TOOL_SERVICE_URL")
│       
│           def validate_tools(self, jwt_token: str, tool_ids: list[str]):
│               """
│               Calls the Tool Service's internal validation endpoint to check
│               if the user has permission to use the given tool IDs.
│               """
│               headers = {"Authorization": f"Bearer {jwt_token}"}
│               payload = {"tool_ids": tool_ids}
│               # The endpoint path must match the one in MS7's internal_urls.py
│               internal_path = "/ms7/internal/v1/tools/validate/"
│               
│               response = self.client.post(internal_path, headers=headers, json=payload)
│               
│               # The _handle_response method will raise PermissionDenied on 403, etc.
│               self._handle_response(response)
│   ]
│   ├───generated
│   │   __init__.py
│   │   [
│   │       
│   │   ]
│   │   node_pb2.py
│   │   [
│   │       # -*- coding: utf-8 -*-
│   │       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│   │       # NO CHECKED-IN PROTOBUF GENCODE
│   │       # source: node.proto
│   │       # Protobuf Python Version: 6.31.1
│   │       """Generated protocol buffer code."""
│   │       from google.protobuf import descriptor as _descriptor
│   │       from google.protobuf import descriptor_pool as _descriptor_pool
│   │       from google.protobuf import runtime_version as _runtime_version
│   │       from google.protobuf import symbol_database as _symbol_database
│   │       from google.protobuf.internal import builder as _builder
│   │       _runtime_version.ValidateProtobufRuntimeVersion(
│   │           _runtime_version.Domain.PUBLIC,
│   │           6,
│   │           31,
│   │           1,
│   │           '',
│   │           'node.proto'
│   │       )
│   │       # @@protoc_insertion_point(imports)
│   │       
│   │       _sym_db = _symbol_database.Default()
│   │       
│   │       
│   │       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│   │       
│   │       
│   │       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\nnode.proto\x12\x04node\x1a\x1cgoogle/protobuf/struct.proto\"9\n\x15GetNodeDetailsRequest\x12\x0f\n\x07node_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"\x98\x01\n\x16GetNodeDetailsResponse\x12\n\n\x02id\x18\x01 \x01(\t\x12\x12\n\nproject_id\x18\x02 \x01(\t\x12\x10\n\x08owner_id\x18\x03 \x01(\t\x12\x0c\n\x04name\x18\x04 \x01(\t\x12.\n\rconfiguration\x18\x05 \x01(\x0b\x32\x17.google.protobuf.Struct\x12\x0e\n\x06status\x18\x06 \x01(\t2Z\n\x0bNodeService\x12K\n\x0eGetNodeDetails\x12\x1b.node.GetNodeDetailsRequest\x1a\x1c.node.GetNodeDetailsResponseb\x06proto3')
│   │       
│   │       _globals = globals()
│   │       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│   │       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'node_pb2', _globals)
│   │       if not _descriptor._USE_C_DESCRIPTORS:
│   │         DESCRIPTOR._loaded_options = None
│   │         _globals['_GETNODEDETAILSREQUEST']._serialized_start=50
│   │         _globals['_GETNODEDETAILSREQUEST']._serialized_end=107
│   │         _globals['_GETNODEDETAILSRESPONSE']._serialized_start=110
│   │         _globals['_GETNODEDETAILSRESPONSE']._serialized_end=262
│   │         _globals['_NODESERVICE']._serialized_start=264
│   │         _globals['_NODESERVICE']._serialized_end=354
│   │       # @@protoc_insertion_point(module_scope)
│   │       
│   │   ]
│   │   node_pb2_grpc.py
│   │   [
│   │       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│   │       """Client and server classes corresponding to protobuf-defined services."""
│   │       import grpc
│   │       import warnings
│   │       
│   │       from . import node_pb2 as node__pb2
│   │       
│   │       GRPC_GENERATED_VERSION = '1.74.0'
│   │       GRPC_VERSION = grpc.__version__
│   │       _version_not_supported = False
│   │       
│   │       try:
│   │           from grpc._utilities import first_version_is_lower
│   │           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│   │       except ImportError:
│   │           _version_not_supported = True
│   │       
│   │       if _version_not_supported:
│   │           raise RuntimeError(
│   │               f'The grpc package installed is at version {GRPC_VERSION},'
│   │               + f' but the generated code in node_pb2_grpc.py depends on'
│   │               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│   │               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│   │               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│   │           )
│   │       
│   │       
│   │       class NodeServiceStub(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def __init__(self, channel):
│   │               """Constructor.
│   │       
│   │               Args:
│   │                   channel: A grpc.Channel.
│   │               """
│   │               self.GetNodeDetails = channel.unary_unary(
│   │                       '/node.NodeService/GetNodeDetails',
│   │                       request_serializer=node__pb2.GetNodeDetailsRequest.SerializeToString,
│   │                       response_deserializer=node__pb2.GetNodeDetailsResponse.FromString,
│   │                       _registered_method=True)
│   │       
│   │       
│   │       class NodeServiceServicer(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           def GetNodeDetails(self, request, context):
│   │               """Authorizes and retrieves the full details of a node for the Inference Service.
│   │               """
│   │               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│   │               context.set_details('Method not implemented!')
│   │               raise NotImplementedError('Method not implemented!')
│   │       
│   │       
│   │       def add_NodeServiceServicer_to_server(servicer, server):
│   │           rpc_method_handlers = {
│   │                   'GetNodeDetails': grpc.unary_unary_rpc_method_handler(
│   │                           servicer.GetNodeDetails,
│   │                           request_deserializer=node__pb2.GetNodeDetailsRequest.FromString,
│   │                           response_serializer=node__pb2.GetNodeDetailsResponse.SerializeToString,
│   │                   ),
│   │           }
│   │           generic_handler = grpc.method_handlers_generic_handler(
│   │                   'node.NodeService', rpc_method_handlers)
│   │           server.add_generic_rpc_handlers((generic_handler,))
│   │           server.add_registered_method_handlers('node.NodeService', rpc_method_handlers)
│   │       
│   │       
│   │        # This class is part of an EXPERIMENTAL API.
│   │       class NodeService(object):
│   │           """Missing associated documentation comment in .proto file."""
│   │       
│   │           @staticmethod
│   │           def GetNodeDetails(request,
│   │                   target,
│   │                   options=(),
│   │                   channel_credentials=None,
│   │                   call_credentials=None,
│   │                   insecure=False,
│   │                   compression=None,
│   │                   wait_for_ready=None,
│   │                   timeout=None,
│   │                   metadata=None):
│   │               return grpc.experimental.unary_unary(
│   │                   request,
│   │                   target,
│   │                   '/node.NodeService/GetNodeDetails',
│   │                   node__pb2.GetNodeDetailsRequest.SerializeToString,
│   │                   node__pb2.GetNodeDetailsResponse.FromString,
│   │                   options,
│   │                   channel_credentials,
│   │                   insecure,
│   │                   call_credentials,
│   │                   compression,
│   │                   wait_for_ready,
│   │                   timeout,
│   │                   metadata,
│   │                   _registered_method=True)
│   │       
│   │   ]
│   ├───management
│   │   └───commands
│   │       generate_protos.py
│   │       [
│   │           import os
│   │           import subprocess
│   │           import fileinput
│   │           from django.core.management.base import BaseCommand
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Generates Python gRPC code from .proto files.'
│   │               requires_system_checks = []
│   │           
│   │               def handle(self, *args, **options):
│   │                   proto_path = 'nodes_internals/protos'
│   │                   output_path = 'nodes_internals/generated' # Place in a sub-package
│   │                   
│   │                   os.makedirs(output_path, exist_ok=True)
│   │                   open(os.path.join(output_path, '__init__.py'), 'a').close()
│   │           
│   │                   proto_files = [f for f in os.listdir(proto_path) if f.endswith('.proto')]
│   │                   command = [
│   │                       'python', '-m', 'grpc_tools.protoc',
│   │                       f'--proto_path={proto_path}',
│   │                       f'--python_out={output_path}',
│   │                       f'--grpc_python_out={output_path}',
│   │                   ] + proto_files
│   │           
│   │                   try:
│   │                       subprocess.run(command, check=True)
│   │                       self.stdout.write(self.style.SUCCESS('Successfully generated gRPC Python stubs.'))
│   │                       for proto_file in proto_files:
│   │                           base_name = proto_file.replace('.proto', '')
│   │                           grpc_file_path = os.path.join(output_path, f'{base_name}_pb2_grpc.py')
│   │                           with fileinput.FileInput(grpc_file_path, inplace=True) as file:
│   │                               for line in file:
│   │                                   if line.strip() == f'import {base_name}_pb2 as {base_name}__pb2':
│   │                                       print(f'from . import {base_name}_pb2 as {base_name}__pb2')
│   │                                   else:
│   │                                       print(line, end='')
│   │                       self.stdout.write(self.style.SUCCESS('Imports fixed.'))
│   │                   except subprocess.CalledProcessError as e:
│   │                       self.stderr.write(self.style.ERROR(f'Failed to generate gRPC stubs: {e}'))
│   │       ]
│   │       run_grpc_server.py
│   │       [
│   │           import grpc
│   │           from concurrent import futures
│   │           import time
│   │           from django.core.management.base import BaseCommand
│   │           import django
│   │           import os
│   │           
│   │           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS4.settings')
│   │           django.setup()
│   │           
│   │           from nodes_internals.generated import node_pb2_grpc
│   │           from nodes_internals.servicer import NodeServicer
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Starts the gRPC server for the Node Service'
│   │               def handle(self, *args, **options):
│   │                   server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
│   │                   node_pb2_grpc.add_NodeServiceServicer_to_server(NodeServicer(), server)
│   │                   server.add_insecure_port('[::]:50051')
│   │                   server.start()
│   │                   self.stdout.write(self.style.SUCCESS('Node Service gRPC server started on port 50051.'))
│   │                   try:
│   │                       while True:
│   │                           time.sleep(86400)
│   │                   except KeyboardInterrupt:
│   │                       server.stop(0)
│   │       ]
│   ├───protos
│   │   node.proto
│   │   [
│   │       // MS4/nodes_internals/protos/node.proto
│   │       syntax = "proto3";
│   │       
│   │       import "google/protobuf/struct.proto";
│   │       
│   │       package node;
│   │       
│   │       service NodeService {
│   │         // Authorizes and retrieves the full details of a node for the Inference Service.
│   │         rpc GetNodeDetails(GetNodeDetailsRequest) returns (GetNodeDetailsResponse);
│   │       }
│   │       
│   │       message GetNodeDetailsRequest {
│   │         string node_id = 1;
│   │         string user_id = 2; // User ID from the JWT, used for authorization.
│   │       }
│   │       
│   │       message GetNodeDetailsResponse {
│   │         string id = 1;
│   │         string project_id = 2;
│   │         string owner_id = 3;
│   │         string name = 4;
│   │         google.protobuf.Struct configuration = 5; // The node's JSON configuration.
│   │         string status = 6; // The current status ('draft', 'active', etc.)
│   │       }
│   │   ]
│   servicer.py
│   [
│       # MS4/nodes_internals/servicer.py
│       import grpc
│       from google.protobuf.struct_pb2 import Struct
│       from .generated import node_pb2, node_pb2_grpc
│       from nodes.repository import NodeRepository
│       
│       class NodeServicer(node_pb2_grpc.NodeServiceServicer):
│           def GetNodeDetails(self, request, context):
│               repo = NodeRepository()
│               try:
│                   node_instance = repo.find_by_id(request.node_id)
│                   if not node_instance:
│                       context.set_code(grpc.StatusCode.NOT_FOUND)
│                       context.set_details(f"Node with ID {request.node_id} not found.")
│                       return node_pb2.GetNodeDetailsResponse()
│       
│                   if str(node_instance.owner_id) != request.user_id:
│                       context.set_code(grpc.StatusCode.PERMISSION_DENIED)
│                       context.set_details("User does not have permission to access this node.")
│                       return node_pb2.GetNodeDetailsResponse()
│       
│                   proto_config = Struct()
│                   proto_config.update(node_instance.configuration)
│       
│                   return node_pb2.GetNodeDetailsResponse(
│                       id=str(node_instance.id),
│                       project_id=str(node_instance.project_id),
│                       owner_id=str(node_instance.owner_id),
│                       name=node_instance.name,
│                       configuration=proto_config,
│                       status=node_instance.status
│                   )
│               except Exception as e:
│                   context.set_code(grpc.StatusCode.INTERNAL)
│                   context.set_details(f'An internal error occurred: {e}')
│                   return node_pb2.GetNodeDetailsResponse()
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       asgiref==3.9.1
│       certifi==2025.7.14
│       cffi==1.17.1
│       charset-normalizer==3.4.2
│       cryptography==45.0.5
│       defusedxml==0.7.1
│       Django==5.2.4
│       djangorestframework==3.16.0
│       djangorestframework_simplejwt==5.5.1
│       djoser==2.3.3
│       idna==3.10
│       oauthlib==3.3.1
│       pillow==11.3.0
│       pycparser==2.22
│       PyJWT==2.10.1
│       python3-openid==3.2.0
│       requests==2.32.4
│       requests-oauthlib==2.0.0
│       social-auth-app-django==5.5.1
│       social-auth-core==4.7.0
│       sqlparse==0.5.3
│       tzdata==2025.2
│       urllib3==2.5.0
│       dotenv
│       httpx
│       pika
│       grpcio
│       grpcio-tools
│       protobuf
│       google-api-python-client
│   ]
