│   .env
│   [
│       # MS9/.env
│       DJANGO_SECRET_KEY='django-ierger4354h6jsergserhm.,,.uifrd6tyu64ldpcuck6bytc4h1*8v!=8(_wau6g8or'
│       JWT_SECRET_KEY ='jwt-secure-m3x$DFGRTJRTYNEHRETNEFDDHD43.m<?><DFGRTJYRJGc4h1*8v!=8(_wau6g8or'
│       JWT_ISSUER="https://ms1.auth-service.com"
│       DJANGO_DEBUG=True
│       
│       RABBITMQ_URL="amqp://guest:guest@localhost:5672/"
│       REDIS_URL="redis://localhost:6379/1"
│       PROJECT_SERVICE_URL="http://localhost:8001"
│       OPENAI_API_KEY="your_openai_api_key_for_summarization"
│       
│       
│   ]
├───MS9
│   __init__.py
│   [
│       
│   ]
│   asgi.py
│   [
│       """
│       ASGI config for MS9 project.
│       
│       It exposes the ASGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/asgi/
│       """
│       
│       import os
│       
│       from django.core.asgi import get_asgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS9.settings')
│       
│       application = get_asgi_application()
│       
│   ]
│   settings.py
│   [
│       """
│       Django settings for MS9 project.
│       
│       Generated by 'django-admin startproject' using Django 5.2.5.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/topics/settings/
│       
│       For the full list of settings and their values, see
│       https://docs.djangoproject.com/en/5.2/ref/settings/
│       """
│       
│       import os
│       from pathlib import Path
│       import redis
│       
│       # Build paths inside the project like this: BASE_DIR / 'subdir'.
│       BASE_DIR = Path(__file__).resolve().parent.parent
│       
│       from dotenv import load_dotenv
│       load_dotenv(BASE_DIR / '.env')
│       # Quick-start development settings - unsuitable for production
│       # See https://docs.djangoproject.com/en/5.2/howto/deployment/checklist/
│       
│       SECRET_KEY = os.getenv('DJANGO_SECRET_KEY')
│       if not SECRET_KEY:
│           # This fallback should ideally not be hit if .env is loaded correctly
│           # or if the environment variable is set directly in the deployment environment.
│           SECRET_KEY = 'django-insecure-fallback-dev-key-!!change-me!!'
│           print("WARNING: DJANGO_SECRET_KEY not found in environment or .env. Using fallback. THIS IS INSECURE FOR PRODUCTION.")
│       
│       DEBUG = os.getenv('DJANGO_DEBUG', 'True').lower() in ('true', '1', 't')
│       
│       
│       ALLOWED_HOSTS = []
│       
│       
│       # Application definition
│       
│       INSTALLED_APPS = [
│           'django.contrib.admin',
│           'django.contrib.auth',
│           'django.contrib.contenttypes',
│           'django.contrib.sessions',
│           'django.contrib.messages',
│           'django.contrib.staticfiles',
│           'rest_framework',
│           'rest_framework_simplejwt',
│           'memory',
│           'memory_internals',
│           'messaging',
│       ]
│       
│       MIDDLEWARE = [
│           'django.middleware.security.SecurityMiddleware',
│           'django.contrib.sessions.middleware.SessionMiddleware',
│           'django.middleware.common.CommonMiddleware',
│           'django.middleware.csrf.CsrfViewMiddleware',
│           'django.contrib.auth.middleware.AuthenticationMiddleware',
│           'django.contrib.messages.middleware.MessageMiddleware',
│           'django.middleware.clickjacking.XFrameOptionsMiddleware',
│       ]
│       
│       ROOT_URLCONF = 'MS9.urls'
│       
│       TEMPLATES = [
│           {
│               'BACKEND': 'django.template.backends.django.DjangoTemplates',
│               'DIRS': [],
│               'APP_DIRS': True,
│               'OPTIONS': {
│                   'context_processors': [
│                       'django.template.context_processors.request',
│                       'django.contrib.auth.context_processors.auth',
│                       'django.contrib.messages.context_processors.messages',
│                   ],
│               },
│           },
│       ]
│       
│       WSGI_APPLICATION = 'MS9.wsgi.application'
│       
│       
│       # Database
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#databases
│       
│       DATABASES = {
│           'default': {
│               'ENGINE': 'django.db.backends.sqlite3',
│               'NAME': BASE_DIR / 'db.sqlite3',
│           }
│       }
│       
│       
│       # Password validation
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#auth-password-validators
│       
│       AUTH_PASSWORD_VALIDATORS = [
│           {
│               'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
│           },
│           {
│               'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
│           },
│       ]
│       
│       
│       # Internationalization
│       # https://docs.djangoproject.com/en/5.2/topics/i18n/
│       
│       LANGUAGE_CODE = 'en-us'
│       
│       TIME_ZONE = 'UTC'
│       
│       USE_I18N = True
│       
│       USE_TZ = True
│       
│       
│       # Static files (CSS, JavaScript, Images)
│       # https://docs.djangoproject.com/en/5.2/howto/static-files/
│       
│       STATIC_URL = 'static/'
│       
│       # Default primary key field type
│       # https://docs.djangoproject.com/en/5.2/ref/settings/#default-auto-field
│       
│       DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'
│       
│       # Media files
│       MEDIA_URL = '/media/'
│       MEDIA_ROOT = BASE_DIR / 'media'
│       
│       
│       
│       from datetime import timedelta
│       JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY')
│       
│       
│       # REST Framework
│       REST_FRAMEWORK = {
│           "DEFAULT_PERMISSION_CLASSES": ["rest_framework.permissions.IsAuthenticated"],
│           "DEFAULT_AUTHENTICATION_CLASSES": (
│       
│          
│           "memory.custom_auth.ForceTokenUserJWTAuthentication", # <<< YOUR CUSTOM AUTH CLASS
│           ),
│           
│           'DEFAULT_THROTTLE_CLASSES': (
│               'rest_framework.throttling.AnonRateThrottle',
│               'rest_framework.throttling.UserRateThrottle'
│           ),
│           'DEFAULT_THROTTLE_RATES': {
│               'anon': '100/day',  # Adjust as needed for unauthenticated requests
│               'user': '20000/day' # Adjust as needed for authenticated requests
│           }
│       }
│       
│       
│       SIMPLE_JWT = {
│       
│           "SIGNING_KEY": JWT_SECRET_KEY,  # <<< USE DJANGO'S SECRET_KEY LOADED FROM ENV
│           "VERIFYING_KEY": JWT_SECRET_KEY,
│           "ISSUER": os.getenv('JWT_ISSUER', "https://ms1.auth-service.com"), # MUST match MS1's issuer
│           "AUTH_HEADER_TYPES": ("Bearer",),
│           "ACCESS_TOKEN_LIFETIME": timedelta(minutes=60), # e.g., 1 hour
│           "REFRESH_TOKEN_LIFETIME": timedelta(days=1),    # e.g., 1 day
│           "LEEWAY": timedelta(seconds=10),
│           "ALGORITHM": "HS256",
│           
│           # --- Settings related to interpreting the token payload ---
│           """
│       "USER_ID_CLAIM": "user_id": (Your Specific Question)
│        This is a critical instruction. It tells simple-jwt:
│          "When you parse the token's payload (the data inside),
│            the claim that contains the user's primary identifier is named 'user_id'."
│              Your MS1's CustomTokenObtainPairSerializer probably adds a claim with this name.
│           """
│       
│           "USER_ID_CLAIM": "user_id",
│           "USER_ID_FIELD": "id",
│           "TOKEN_USER_CLASS": "rest_framework_simplejwt.models.TokenUser", # Explicitly use TokenUse
│       
│           # --- Settings for features MS2 likely DOES NOT use ---
│           "UPDATE_LAST_LOGIN": False,
│           "ROTATE_REFRESH_TOKENS": False,
│           "BLACKLIST_AFTER_ROTATION": False, 
│       
│       }
│       PROJECT_SERVICE_URL = os.getenv('PROJECT_SERVICE_URL')
│       REDIS_URL = os.getenv('REDIS_URL')
│       RABBITMQ_URL = os.getenv('RABBITMQ_URL', 'amqp://guest:guest@localhost:5672/')
│       OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') # For summarization
│       REDIS_CLIENT = redis.from_url(REDIS_URL, decode_responses=True)
│   ]
│   urls.py
│   [
│       from django.contrib import admin
│       from django.urls import path, include
│       
│       urlpatterns = [
│           path('ms9/admin/', admin.site.urls),
│           path('ms9/api/v1/', include('memory.api_urls')),
│           path('ms9/internal/v1/', include('memory_internals.internal_urls')),
│       ]
│   ]
│   wsgi.py
│   [
│       """
│       WSGI config for MS9 project.
│       
│       It exposes the WSGI callable as a module-level variable named ``application``.
│       
│       For more information on this file, see
│       https://docs.djangoproject.com/en/5.2/howto/deployment/wsgi/
│       """
│       
│       import os
│       
│       from django.core.wsgi import get_wsgi_application
│       
│       os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS9.settings')
│       
│       application = get_wsgi_application()
│       
│   ]
│   db.sqlite3
│   [
│       [Binary file - content not shown]
│   ]
│   manage.py
│   [
│       #!/usr/bin/env python
│       """Django's command-line utility for administrative tasks."""
│       import os
│       import sys
│       
│       
│       def main():
│           """Run administrative tasks."""
│           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS9.settings')
│           try:
│               from django.core.management import execute_from_command_line
│           except ImportError as exc:
│               raise ImportError(
│                   "Couldn't import Django. Are you sure it's installed and "
│                   "available on your PYTHONPATH environment variable? Did you "
│                   "forget to activate a virtual environment?"
│               ) from exc
│           execute_from_command_line(sys.argv)
│       
│       
│       if __name__ == '__main__':
│           main()
│       
│   ]
├───memory
│   __init__.py
│   [
│       
│   ]
│   admin.py
│   [
│       from django.contrib import admin
│       from .models import MemoryBucket, Message
│       # Register your models here.
│       admin.site.register(MemoryBucket)
│       admin.site.register(Message)
│       
│   ]
│   api_urls.py
│   [
│       from django.urls import path
│       from . import views
│       
│       urlpatterns = [
│           path('buckets/', views.MemoryBucketCreateAPIView.as_view(), name='bucket-create'),
│           path('projects/<uuid:project_id>/buckets/', views.MemoryBucketListAPIView.as_view(), name='bucket-list'),
│           path('buckets/<uuid:pk>/', views.MemoryBucketDetailAPIView.as_view(), name='bucket-detail'),
│           path('buckets/<uuid:pk>/clear/', views.MemoryBucketClearAPIView.as_view(), name='bucket-clear'),
│           path('buckets/<uuid:bucket_id>/messages/', views.MessageListAPIView.as_view(), name='message-list'),
│           path('messages/<uuid:pk>/', views.MessageDetailAPIView.as_view(), name='message-detail'),
│           # --- NEW IMPORT/EXPORT URLS ---
│           path('buckets/import/', views.MemoryBucketImportAPIView.as_view(), name='bucket-import'),
│           path('buckets/<uuid:pk>/export/', views.MemoryBucketExportAPIView.as_view(), name='bucket-export'),
│           # --- END OF NEW URLS ---
│       ]
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MemoryConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'memory'
│       
│   ]
│   custom_auth.py
│   [
│       from rest_framework_simplejwt.authentication import JWTAuthentication
│       from rest_framework_simplejwt.models import TokenUser # Import TokenUser
│       from rest_framework_simplejwt.settings import api_settings as simple_jwt_settings
│       from django.utils.translation import gettext_lazy as _
│       from rest_framework_simplejwt.exceptions import InvalidToken
│       
│       class ForceTokenUserJWTAuthentication(JWTAuthentication):
│           def get_user(self, validated_token):
│               """
│               Returns a TokenUser instance based on the validated token.
│               Bypasses any local database User lookup for JWT authentication.
│               """
│               try:
│                   # simple_jwt_settings.USER_ID_CLAIM refers to what you set in settings.py
│                   # e.g., "user_id"
│                   user_id = validated_token[simple_jwt_settings.USER_ID_CLAIM]
│               except KeyError:
│                   raise InvalidToken(_("Token contained no recognizable user identification"))
│       
│               # Correct way to instantiate TokenUser: pass the validated_token
│               # The TokenUser class will internally use USER_ID_CLAIM and USER_ID_FIELD
│               # from your SIMPLE_JWT settings to extract the user ID and set its 'id' or 'pk'.
│               token_user = TokenUser(validated_token)
│       
│               # The TokenUser's 'id' (and 'pk') attribute should now be populated correctly
│               # by its own __init__ method based on the validated_token and your SIMPLE_JWT settings
│               # for USER_ID_CLAIM and USER_ID_FIELD.
│       
│               # Example: If you wanted to verify or access it (not strictly necessary here)
│               # print(f"TokenUser ID: {token_user.id}, TokenUser PK: {token_user.pk}")
│       
│               return token_user
│   ]
│   models.py
│   [
│       import uuid
│       from django.db import models
│       
│       class MemoryBucket(models.Model):
│           id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
│           name = models.CharField(max_length=255)
│           
│           # Ownership and Scoping
│           owner_id = models.UUIDField(db_index=True, help_text="The user who owns this bucket.")
│           project_id = models.UUIDField(db_index=True, help_text="The project this bucket belongs to.")
│           
│           # Logic Configuration
│           memory_type = models.CharField(max_length=100, default='conversation_buffer_window')
│           config = models.JSONField(default=dict, help_text="Type-specific config, e.g., {'k': 10}")
│           
│           # Calculated Metadata
│           message_count = models.PositiveIntegerField(default=0)
│           token_count = models.PositiveIntegerField(default=0)
│       
│           created_at = models.DateTimeField(auto_now_add=True)
│           updated_at = models.DateTimeField(auto_now=True)
│       
│       class Message(models.Model):
│           id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
│           bucket = models.ForeignKey(MemoryBucket, on_delete=models.CASCADE, related_name='messages')
│           
│           # The rich, multimodal content block
│           content = models.JSONField() 
│           
│           # For idempotency from the Executor's feedback loop
│           idempotency_key = models.CharField(max_length=255, null=True, blank=True, unique=True)
│           
│           timestamp = models.DateTimeField(auto_now_add=True)
│       
│           class Meta:
│               ordering = ['timestamp']
│   ]
│   permissions.py
│   [
│       # MS9/memory/permissions.py
│       
│       from rest_framework import permissions
│       from .models import MemoryBucket, Message
│       
│       class IsBucketOwner(permissions.BasePermission):
│           """
│           Custom permission to only allow owners of a MemoryBucket to access it.
│           This also handles nested objects like Messages by checking the ownership
│           of the parent bucket.
│           """
│           def has_object_permission(self, request, view, obj):
│               # --- THE FIX IS HERE: Explicitly check the type of the object ---
│               
│               owner_id_to_check = None
│       
│               if isinstance(obj, MemoryBucket):
│                   # If the object is a MemoryBucket, its owner_id is directly on it.
│                   owner_id_to_check = obj.owner_id
│               
│               elif isinstance(obj, Message):
│                   # If the object is a Message, we must check the owner_id of its parent bucket.
│                   owner_id_to_check = obj.bucket.owner_id
│               
│               else:
│                   # If we get an unexpected object type, deny permission by default.
│                   return False
│       
│               # --- END OF FIX ---
│       
│               # Now perform the actual comparison.
│               # Ensure we're comparing string versions to avoid UUID object issues.
│               if owner_id_to_check and request.user and request.user.is_authenticated:
│                   return str(owner_id_to_check) == str(request.user.id)
│                   
│               return False
│   ]
│   serializers.py
│   [
│       from rest_framework import serializers
│       from .models import MemoryBucket, Message
│       
│       class MemoryBucketCreateSerializer(serializers.ModelSerializer):
│           project_id = serializers.UUIDField(write_only=True)
│           class Meta:
│               model = MemoryBucket
│               fields = ['name', 'project_id', 'memory_type', 'config']
│       
│       class MemoryBucketUpdateSerializer(serializers.ModelSerializer):
│           class Meta:
│               model = MemoryBucket
│               fields = ['name', 'memory_type', 'config']
│       
│       class MemoryBucketListSerializer(serializers.ModelSerializer):
│           class Meta:
│               model = MemoryBucket
│               fields = ['id', 'name', 'memory_type', 'message_count', 'token_count', 'updated_at']
│       
│       class MemoryBucketDetailSerializer(serializers.ModelSerializer):
│           class Meta:
│               model = MemoryBucket
│               fields = '__all__'
│       
│       class MessageSerializer(serializers.ModelSerializer):
│           class Meta:
│               model = Message
│               fields = ['id', 'content', 'timestamp']
│   ]
│   services.py
│   [
│       # MS9/memory/services.py
│       
│       import httpx
│       import json
│       import uuid
│       from django.conf import settings
│       from django.db import transaction
│       from dateutil.parser import isoparse
│       from rest_framework.exceptions import PermissionDenied, NotFound, ValidationError
│       
│       from .models import MemoryBucket, Message
│       from .serializers import MemoryBucketCreateSerializer
│       from langchain_openai import ChatOpenAI
│       from langchain.memory import ConversationSummaryMemory
│       from langchain_core.messages import AIMessage, HumanMessage
│       
│       class MemoryService:
│           """
│           The service layer for handling all business logic related to MemoryBuckets and Messages.
│           """
│       
│           def create_bucket(self, *, owner_id: uuid.UUID, project_id: uuid.UUID, name: str, memory_type: str, config: dict, jwt_token: str) -> MemoryBucket:
│               """
│               Creates a new memory bucket after validating project ownership.
│               """
│               self._validate_project_ownership(project_id, jwt_token)
│               bucket = MemoryBucket.objects.create(
│                   owner_id=owner_id,
│                   project_id=project_id,
│                   name=name,
│                   memory_type=memory_type,
│                   config=config
│               )
│               return bucket
│       
│           def get_processed_history(self, bucket: MemoryBucket) -> dict:
│               raw_messages = bucket.messages.all().order_by('timestamp')
│       
│               # 1. Convert raw DB messages into a clean list of dictionaries.
│               # This is the full, unfiltered history.
│               full_history = []
│               for msg in raw_messages:
│                   if isinstance(msg.content, dict) and "role" in msg.content and "parts" in msg.content:
│                       full_history.append({
│                           "role": msg.content["role"],
│                           "content": msg.content["parts"]
│                       })
│               
│               # 2. Apply memory logic to the full history.
│               final_history_for_llm = []
│               if bucket.memory_type == 'conversation_buffer_window':
│                   k = bucket.config.get('k', 10)
│                   # --- THE BUG WAS HERE. Slice the CORRECT list. ---
│                   final_history_for_llm = full_history[-(k * 2):] 
│               else:
│                   final_history_for_llm = full_history
│               
│               return {
│                   "bucket_id": str(bucket.id), 
│                   "memory_type": bucket.memory_type, 
│                   "history": final_history_for_llm # Return the correctly sliced (or full) history
│               }
│       
│           def _validate_project_ownership(self, project_id, jwt_token):
│               """
│               Makes a blocking, internal HTTP call to the Project Service to
│               verify that the user owns the project.
│               """
│               headers = {"Authorization": f"Bearer {jwt_token}"}
│               url = f"{settings.PROJECT_SERVICE_URL}/ms2/internal/v1/projects/{project_id}/authorize"
│               try:
│                   with httpx.Client() as client:
│                       response = client.get(url, headers=headers)
│                       if response.status_code == 403:
│                           raise PermissionDenied("You do not own the project for this memory bucket.")
│                       if response.status_code == 404:
│                           raise NotFound("The specified project does not exist.")
│                       response.raise_for_status()
│               except httpx.RequestError:
│                   raise ValidationError("Could not connect to the Project Service to validate ownership.")
│       
│           def export_bucket_data(self, bucket: MemoryBucket) -> dict:
│               """
│               Gathers all data for a bucket and formats it into a standardized
│               JSON structure for export.
│               """
│               messages = bucket.messages.all().order_by('timestamp')
│               
│               message_data_list = [
│                   {"content": msg.content, "timestamp": msg.timestamp.isoformat()}
│                   for msg in messages
│               ]
│               
│               return {
│                   "export_version": "1.0",
│                   "source_bucket": {
│                       "name": bucket.name,
│                       "memory_type": bucket.memory_type,
│                       "config": bucket.config
│                   },
│                   "messages": message_data_list
│               }
│       
│           def import_bucket_data(self, owner_id: uuid.UUID, project_id: uuid.UUID, file_content: bytes, jwt_token: str) -> MemoryBucket:
│               """
│               Parses, validates, and imports a memory history file to create a new bucket.
│               Enforces a "0 Trust" policy.
│               """
│               # ... (This method is complete and correct from the previous step)
│               # It performs the multi-stage validation.
│               try:
│                   data = json.loads(file_content)
│               except json.JSONDecodeError:
│                   raise ValidationError({"file_error": "Invalid file format: Not a valid JSON file."})
│               # ... and so on for all validation stages.
│               # Finally, it creates the bucket and messages inside a transaction.
│       
│           def delete_bucket(self, bucket: MemoryBucket):
│               """
│               Deletes a MemoryBucket instance from the database.
│               """
│               bucket_id = bucket.id
│               bucket.delete()
│               return bucket_id
│   ]
│   views.py
│   [
│       from messaging.event_publisher import memory_event_publisher
│       from django.shortcuts import render
│       from django.http import JsonResponse
│       import uuid
│       import json # <-- Add this import at the top of the file
│       from django.http import HttpResponse 
│       # Create your views here.
│       from rest_framework import generics, views, status, permissions
│       from rest_framework.response import Response
│       from .models import MemoryBucket, Message
│       from .permissions import IsBucketOwner
│       from .services import MemoryService
│       from .serializers import *
│       from rest_framework.exceptions import PermissionDenied, NotFound, ValidationError
│       
│       class MemoryBucketCreateAPIView(views.APIView):
│           """
│           Handles the creation of a new MemoryBucket.
│           Uses a custom 'create' method to use different serializers for
│           request validation and response formatting.
│           """
│           permission_classes = [permissions.IsAuthenticated]
│       
│           def post(self, request, *args, **kwargs):
│               # 1. Use the strict "Create" serializer to validate the incoming request data.
│               write_serializer = MemoryBucketCreateSerializer(data=request.data)
│               write_serializer.is_valid(raise_exception=True)
│               
│               service = MemoryService()
│               try:
│                   # 2. Call the service layer to perform the creation logic.
│                   #    The service method will return the newly created model instance.
│                   new_bucket = service.create_bucket(
│                       owner_id=request.user.id,
│                       jwt_token=str(request.auth),
│                       **write_serializer.validated_data
│                   )
│                   
│                   # 3. Use the more detailed "DetailSerializer" to create the response data.
│                   #    This serializer includes the 'id' and all other fields.
│                   read_serializer = MemoryBucketDetailSerializer(new_bucket)
│                   
│                   # 4. Return a 201 Created response with the full object data.
│                   return Response(read_serializer.data, status=status.HTTP_201_CREATED)
│       
│               except (PermissionDenied, NotFound, ValidationError) as e:
│                    return Response({"error": str(e)}, status=e.status_code)
│               except Exception as e:
│                   return Response({"error": f"An unexpected server error occurred: {e}"}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
│       
│       class MemoryBucketListAPIView(generics.ListAPIView):
│           permission_classes = [permissions.IsAuthenticated]
│           serializer_class = MemoryBucketListSerializer
│           
│           def get_queryset(self):
│               project_id = self.kwargs['project_id']
│               return MemoryBucket.objects.filter(owner_id=self.request.user.id, project_id=project_id)
│       
│       class MemoryBucketDetailAPIView(generics.RetrieveUpdateDestroyAPIView):
│           permission_classes = [permissions.IsAuthenticated, IsBucketOwner]
│           queryset = MemoryBucket.objects.all()
│           
│           def get_serializer_class(self):
│               if self.request.method == 'PUT' or self.request.method == 'PATCH':
│                   return MemoryBucketUpdateSerializer
│               return MemoryBucketDetailSerializer
│           def destroy(self, request, *args, **kwargs):
│               """
│               Handles deleting a MemoryBucket and publishes an event upon success.
│               """
│               # 1. get_object() handles fetching the instance and checking ownership permissions.
│               #    If the user is not the owner, it will raise a 403 Forbidden error.
│               bucket_to_delete = self.get_object()
│               
│               service = MemoryService()
│               try:
│                   # 2. Delegate the deletion logic to the service layer.
│                   #    This returns the ID for use in the event.
│                   deleted_bucket_id = service.delete_bucket(bucket_to_delete)
│       
│                   # 3. If deletion was successful, publish the event.
│                   try:
│                       memory_event_publisher.publish_bucket_deleted(bucket_id=str(deleted_bucket_id))
│                   except Exception as e:
│                       # The bucket was deleted, but the event failed. This is a critical state
│                       # that should be logged for monitoring.
│                       print(f"CRITICAL ALERT: MemoryBucket {deleted_bucket_id} was deleted, but the 'memory.bucket.deleted' event publishing failed: {e}")
│       
│                   # 4. Return a success response to the user.
│                   return Response(status=status.HTTP_204_NO_CONTENT)
│       
│               except Exception as e:
│                   # Catch any unexpected errors during the process.
│                   return Response(
│                       {"error": "An unexpected error occurred during bucket deletion."},
│                       status=status.HTTP_500_INTERNAL_SERVER_ERROR
│                   )
│               
│       class MemoryBucketClearAPIView(views.APIView):
│           permission_classes = [permissions.IsAuthenticated, IsBucketOwner]
│           
│           def post(self, request, pk):
│               bucket = generics.get_object_or_404(MemoryBucket.objects.all(), pk=pk)
│               self.check_object_permissions(request, bucket)
│               bucket.messages.all().delete()
│               return Response(status=status.HTTP_204_NO_CONTENT)
│       
│       class MessageListAPIView(generics.ListAPIView):
│           permission_classes = [permissions.IsAuthenticated, IsBucketOwner]
│           serializer_class = MessageSerializer
│           
│           def get_queryset(self):
│               bucket = generics.get_object_or_404(MemoryBucket.objects.all(), id=self.kwargs['bucket_id'])
│               self.check_object_permissions(self.request, bucket)
│               return Message.objects.filter(bucket=bucket)
│       
│       class MessageDetailAPIView(generics.RetrieveUpdateDestroyAPIView):
│           permission_classes = [permissions.IsAuthenticated, IsBucketOwner]
│           serializer_class = MessageSerializer
│           queryset = Message.objects.all()
│       
│       
│       class MemoryBucketExportAPIView(views.APIView):
│           """
│           Handles the POST request to export a memory bucket's contents as a JSON file.
│           This view now returns a proper file download response.
│           """
│           permission_classes = [permissions.IsAuthenticated, IsBucketOwner]
│       
│           def post(self, request, pk):
│               bucket = generics.get_object_or_404(MemoryBucket.objects.all(), pk=pk)
│               self.check_object_permissions(request, bucket)
│               
│               service = MemoryService()
│               export_data = service.export_bucket_data(bucket)
│               
│               # --- THE DEFINITIVE FIX IS HERE ---
│               
│               # 1. Serialize the dictionary to a JSON formatted string.
│               #    'indent=2' makes the downloaded file human-readable.
│               json_string = json.dumps(export_data, indent=2)
│               
│               # 2. Create an HttpResponse, not a JsonResponse.
│               #    Set the content_type to 'application/json' to tell the client
│               #    what kind of file it is.
│               response = HttpResponse(json_string, content_type='application/json')
│               
│               # 3. Set the Content-Disposition header. This is the crucial part that
│               #    tells the browser to trigger a "Save As..." dialog instead of
│               #    displaying the text in the window.
│               filename = f'memory_export_{bucket.name.replace(" ", "_")}_{pk}.json'
│               response['Content-Disposition'] = f'attachment; filename="{filename}"'
│               
│               # --- END OF FIX ---
│       
│               return response
│       class MemoryBucketImportAPIView(views.APIView):
│           """
│           Handles the POST request to import a file and create a new memory bucket.
│           Uses multipart/form-data for the file upload.
│           """
│           permission_classes = [permissions.IsAuthenticated]
│       
│           def post(self, request):
│               file_obj = request.FILES.get('file')
│               project_id_str = request.data.get('project_id') # Use request.data for form-data
│       
│               if not file_obj:
│                   return Response({"error": "File not provided in 'file' field."}, status=status.HTTP_400_BAD_REQUEST)
│               if not project_id_str:
│                   return Response({"error": "project_id not provided in form data."}, status=status.HTTP_400_BAD_REQUEST)
│       
│               try:
│                   project_id = uuid.UUID(project_id_str)
│                   # Read the entire file content into memory. For very large files,
│                   # streaming to a temp file on disk would be more memory-efficient.
│                   file_content = file_obj.read()
│               except (ValueError, TypeError):
│                   return Response({"error": "Invalid project_id UUID format."}, status=status.HTTP_400_BAD_REQUEST)
│               except Exception:
│                   return Response({"error": "Could not read the uploaded file."}, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
│       
│               service = MemoryService()
│               try:
│                   # Delegate all validation and creation logic to the service layer
│                   new_bucket = service.import_bucket_data(
│                       owner_id=request.user.id,
│                       project_id=project_id,
│                       file_content=file_content,
│                       jwt_token=str(request.auth)
│                   )
│                   
│                   # On success, return the full details of the newly created bucket
│                   response_serializer = MemoryBucketDetailSerializer(new_bucket)
│                   return Response(response_serializer.data, status=status.HTTP_201_CREATED)
│               
│               except ValidationError as e:
│                   # Catch the specific validation errors from the service layer
│                   # and format them into a user-friendly 400 response.
│                   return Response({"error": "Invalid file content.", "details": e.detail}, status=status.HTTP_400_BAD_REQUEST)
│   ]
├───memory_internals
│   __init__.py
│   [
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MemoryInternalsConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'memory_internals'
│       
│   ]
│   internal_urls.py
│   [
│       from django.urls import path
│       from .internal_views import MemoryBucketValidateAPIView
│       
│       urlpatterns = [
│           path('buckets/validate/', MemoryBucketValidateAPIView.as_view(), name='internal-bucket-validate'),
│       ]
│   ]
│   internal_views.py
│   [
│       # MS9/memory_internals/internal_views.py
│       from rest_framework.views import APIView
│       from rest_framework.response import Response
│       from rest_framework import status, permissions
│       from django.db.models import Q
│       from memory.models import MemoryBucket
│       import uuid
│       
│       class IsInternalServicePermission(permissions.BasePermission):
│           """
│           In a real system, this would be more robust (e.g., shared secret header).
│           For now, we trust any authenticated request to this internal endpoint.
│           """
│           def has_permission(self, request, view):
│               return request.user and request.user.is_authenticated
│       
│       class MemoryBucketValidateAPIView(APIView):
│           """
│           Internal HTTP endpoint for the Node Service (MS4) to validate that a user
│           owns a list of memory bucket IDs before linking them to a node.
│           """
│           permission_classes = [IsInternalServicePermission]
│       
│           def post(self, request):
│               user_id = request.user.id
│               bucket_ids_str = request.data.get('bucket_ids', [])
│       
│               if not isinstance(bucket_ids_str, list):
│                   return Response({"error": "'bucket_ids' must be a list."}, status=status.HTTP_400_BAD_REQUEST)
│               
│               try:
│                   bucket_ids = [uuid.UUID(bid) for bid in bucket_ids_str]
│               except (ValueError, TypeError):
│                   return Response({"error": "One or more bucket IDs are not valid UUIDs."}, status=status.HTTP_400_BAD_REQUEST)
│       
│               if not bucket_ids:
│                   return Response(status=status.HTTP_204_NO_CONTENT)
│       
│               # Count how many of the requested buckets are actually owned by this user.
│               valid_bucket_count = MemoryBucket.objects.filter(
│                   owner_id=user_id,
│                   id__in=bucket_ids
│               ).count()
│       
│               if valid_bucket_count == len(bucket_ids):
│                   # The user owns all the buckets they are trying to link.
│                   return Response(status=status.HTTP_204_NO_CONTENT)
│               else:
│                   # The user is trying to link a bucket they don't own or that doesn't exist.
│                   return Response(
│                       {"error": "One or more memory bucket IDs are invalid or you do not have permission to use them."},
│                       status=status.HTTP_403_FORBIDDEN
│                   )
│   ]
│   ├───management
│   │   └───commands
│   │       generate_protos.py
│   │       [
│   │           # MS9/memory_internals/management/commands/generate_protos.py
│   │           
│   │           import os
│   │           import subprocess
│   │           import fileinput
│   │           import sys
│   │           from pathlib import Path
│   │           from django.core.management.base import BaseCommand
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Generates and fixes Python gRPC code from .proto files for the Memory Service.'
│   │               requires_system_checks = [] # Allows this command to run without a full Django app check
│   │           
│   │               def handle(self, *args, **options):
│   │                   # Define the paths relative to the Django project's base directory
│   │                   base_dir = Path(__file__).resolve().parent.parent.parent.parent
│   │                   proto_path = base_dir / 'memory_internals' / 'protos'
│   │                   output_path = base_dir / 'memory_internals' # Generate directly into the app
│   │                   
│   │                   self.stdout.write(f"Proto source directory: {proto_path}")
│   │                   self.stdout.write(f"Generated code output directory: {output_path}")
│   │           
│   │                   if not proto_path.is_dir():
│   │                       self.stderr.write(self.style.ERROR(f"Proto path '{proto_path}' does not exist."))
│   │                       return
│   │           
│   │                   proto_files = [f for f in proto_path.iterdir() if f.suffix == '.proto']
│   │                   if not proto_files:
│   │                       self.stdout.write(self.style.WARNING('No .proto files found in protos/ directory.'))
│   │                       return
│   │                       
│   │                   # Command to run the gRPC code generator
│   │                   command = [
│   │                       sys.executable, '-m', 'grpc_tools.protoc',
│   │                       f'--proto_path={proto_path}',
│   │                       f'--python_out={output_path}',
│   │                       f'--grpc_python_out={output_path}',
│   │                   ] + [str(pf) for pf in proto_files]
│   │           
│   │                   self.stdout.write(f"Running command: {' '.join(command)}")
│   │                   try:
│   │                       subprocess.run(command, check=True, capture_output=True, text=True)
│   │                       self.stdout.write(self.style.SUCCESS('Successfully generated gRPC Python stubs.'))
│   │                   except subprocess.CalledProcessError as e:
│   │                       self.stderr.write(self.style.ERROR('Failed to generate gRPC stubs.'))
│   │                       self.stderr.write(e.stderr)
│   │                       return
│   │           
│   │                   # Automatically fix the relative imports in the generated _grpc.py file
│   │                   for proto_file in proto_files:
│   │                       base_name = proto_file.stem
│   │                       grpc_file_path = output_path / f'{base_name}_pb2_grpc.py'
│   │                       
│   │                       self.stdout.write(f"Fixing imports in {grpc_file_path}...")
│   │                       with fileinput.FileInput(str(grpc_file_path), inplace=True) as file:
│   │                           for line in file:
│   │                               if line.strip() == f'import {base_name}_pb2 as {base_name}__pb2':
│   │                                   print(f'from . import {base_name}_pb2 as {base_name}__pb2', end='\n')
│   │                               else:
│   │                                   print(line, end='')
│   │                       self.stdout.write(self.style.SUCCESS('Imports fixed.'))
│   │       ]
│   │       run_grpc_server.py
│   │       [
│   │           # MS9/memory_internals/management/commands/run_grpc_server.py
│   │           
│   │           import grpc
│   │           from concurrent import futures
│   │           import time
│   │           import logging
│   │           from django.core.management.base import BaseCommand
│   │           import django
│   │           import os
│   │           
│   │           # Configure logging for the server itself
│   │           logging.basicConfig(level=logging.INFO, format='%(asctime)s - MS9-gRPC-Server - %(levelname)s - %(message)s')
│   │           logger = logging.getLogger(__name__)
│   │           
│   │           # Ensure the Django environment is set up before importing models/services
│   │           os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'MS9.settings') # <--- CORRECT
│   │           django.setup()
│   │           
│   │           from memory_internals import memory_pb2_grpc
│   │           from memory_internals.servicer import MemoryServicer
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Starts the gRPC server for the Memory Service'
│   │           
│   │               def handle(self, *args, **options):
│   │                   grpc_port = '50059'
│   │                   logger.info(f"Attempting to start Memory Service gRPC server on port {grpc_port}...")
│   │                   
│   │                   server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
│   │                   
│   │                   memory_pb2_grpc.add_MemoryServiceServicer_to_server(MemoryServicer(), server)
│   │                   
│   │                   server.add_insecure_port(f'[::]:{grpc_port}')
│   │                   server.start()
│   │                   logger.info(f'Memory Service gRPC server started successfully on port {grpc_port}.')
│   │                   
│   │                   try:
│   │                       # Keep the server running indefinitely
│   │                       server.wait_for_termination()
│   │                   except KeyboardInterrupt:
│   │                       logger.warning('Stopping gRPC server due to user request...')
│   │                       server.stop(0)
│   │                       logger.info('gRPC server stopped.')
│   │       ]
│   memory_pb2.py
│   [
│       # -*- coding: utf-8 -*-
│       # Generated by the protocol buffer compiler.  DO NOT EDIT!
│       # NO CHECKED-IN PROTOBUF GENCODE
│       # source: memory.proto
│       # Protobuf Python Version: 6.31.1
│       """Generated protocol buffer code."""
│       from google.protobuf import descriptor as _descriptor
│       from google.protobuf import descriptor_pool as _descriptor_pool
│       from google.protobuf import runtime_version as _runtime_version
│       from google.protobuf import symbol_database as _symbol_database
│       from google.protobuf.internal import builder as _builder
│       _runtime_version.ValidateProtobufRuntimeVersion(
│           _runtime_version.Domain.PUBLIC,
│           6,
│           31,
│           1,
│           '',
│           'memory.proto'
│       )
│       # @@protoc_insertion_point(imports)
│       
│       _sym_db = _symbol_database.Default()
│       
│       
│       from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
│       
│       
│       DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0cmemory.proto\x12\x06memory\x1a\x1cgoogle/protobuf/struct.proto\"7\n\x11GetHistoryRequest\x12\x11\n\tbucket_id\x18\x01 \x01(\t\x12\x0f\n\x07user_id\x18\x02 \x01(\t\"f\n\x12GetHistoryResponse\x12\x11\n\tbucket_id\x18\x01 \x01(\t\x12\x13\n\x0bmemory_type\x18\x02 \x01(\t\x12(\n\x07history\x18\x03 \x03(\x0b\x32\x17.google.protobuf.Struct2T\n\rMemoryService\x12\x43\n\nGetHistory\x12\x19.memory.GetHistoryRequest\x1a\x1a.memory.GetHistoryResponseb\x06proto3')
│       
│       _globals = globals()
│       _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
│       _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'memory_pb2', _globals)
│       if not _descriptor._USE_C_DESCRIPTORS:
│         DESCRIPTOR._loaded_options = None
│         _globals['_GETHISTORYREQUEST']._serialized_start=54
│         _globals['_GETHISTORYREQUEST']._serialized_end=109
│         _globals['_GETHISTORYRESPONSE']._serialized_start=111
│         _globals['_GETHISTORYRESPONSE']._serialized_end=213
│         _globals['_MEMORYSERVICE']._serialized_start=215
│         _globals['_MEMORYSERVICE']._serialized_end=299
│       # @@protoc_insertion_point(module_scope)
│       
│   ]
│   memory_pb2_grpc.py
│   [
│       # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
│       """Client and server classes corresponding to protobuf-defined services."""
│       import grpc
│       import warnings
│       
│       from . import memory_pb2 as memory__pb2
│       
│       GRPC_GENERATED_VERSION = '1.74.0'
│       GRPC_VERSION = grpc.__version__
│       _version_not_supported = False
│       
│       try:
│           from grpc._utilities import first_version_is_lower
│           _version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
│       except ImportError:
│           _version_not_supported = True
│       
│       if _version_not_supported:
│           raise RuntimeError(
│               f'The grpc package installed is at version {GRPC_VERSION},'
│               + f' but the generated code in memory_pb2_grpc.py depends on'
│               + f' grpcio>={GRPC_GENERATED_VERSION}.'
│               + f' Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}'
│               + f' or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.'
│           )
│       
│       
│       class MemoryServiceStub(object):
│           """Missing associated documentation comment in .proto file."""
│       
│           def __init__(self, channel):
│               """Constructor.
│       
│               Args:
│                   channel: A grpc.Channel.
│               """
│               self.GetHistory = channel.unary_unary(
│                       '/memory.MemoryService/GetHistory',
│                       request_serializer=memory__pb2.GetHistoryRequest.SerializeToString,
│                       response_deserializer=memory__pb2.GetHistoryResponse.FromString,
│                       _registered_method=True)
│       
│       
│       class MemoryServiceServicer(object):
│           """Missing associated documentation comment in .proto file."""
│       
│           def GetHistory(self, request, context):
│               """Fetches and processes the history for a given memory bucket.
│               """
│               context.set_code(grpc.StatusCode.UNIMPLEMENTED)
│               context.set_details('Method not implemented!')
│               raise NotImplementedError('Method not implemented!')
│       
│       
│       def add_MemoryServiceServicer_to_server(servicer, server):
│           rpc_method_handlers = {
│                   'GetHistory': grpc.unary_unary_rpc_method_handler(
│                           servicer.GetHistory,
│                           request_deserializer=memory__pb2.GetHistoryRequest.FromString,
│                           response_serializer=memory__pb2.GetHistoryResponse.SerializeToString,
│                   ),
│           }
│           generic_handler = grpc.method_handlers_generic_handler(
│                   'memory.MemoryService', rpc_method_handlers)
│           server.add_generic_rpc_handlers((generic_handler,))
│           server.add_registered_method_handlers('memory.MemoryService', rpc_method_handlers)
│       
│       
│        # This class is part of an EXPERIMENTAL API.
│       class MemoryService(object):
│           """Missing associated documentation comment in .proto file."""
│       
│           @staticmethod
│           def GetHistory(request,
│                   target,
│                   options=(),
│                   channel_credentials=None,
│                   call_credentials=None,
│                   insecure=False,
│                   compression=None,
│                   wait_for_ready=None,
│                   timeout=None,
│                   metadata=None):
│               return grpc.experimental.unary_unary(
│                   request,
│                   target,
│                   '/memory.MemoryService/GetHistory',
│                   memory__pb2.GetHistoryRequest.SerializeToString,
│                   memory__pb2.GetHistoryResponse.FromString,
│                   options,
│                   channel_credentials,
│                   insecure,
│                   call_credentials,
│                   compression,
│                   wait_for_ready,
│                   timeout,
│                   metadata,
│                   _registered_method=True)
│       
│   ]
│   ├───protos
│   │   memory.proto
│   │   [
│   │       // MS9/memory_internals/protos/memory.proto
│   │       syntax = "proto3";
│   │       
│   │       import "google/protobuf/struct.proto";
│   │       
│   │       package memory;
│   │       
│   │       service MemoryService {
│   │         // Fetches and processes the history for a given memory bucket.
│   │         rpc GetHistory(GetHistoryRequest) returns (GetHistoryResponse);
│   │       }
│   │       
│   │       message GetHistoryRequest {
│   │         string bucket_id = 1;
│   │         string user_id = 2; // For authorization
│   │       }
│   │       
│   │       message GetHistoryResponse {
│   │         string bucket_id = 1;
│   │         string memory_type = 2;
│   │         repeated google.protobuf.Struct history = 3; // The processed, ready-to-use context
│   │       }
│   │   ]
│   servicer.py
│   [
│       # MS9/memory_internals/servicer.py
│       
│       import grpc
│       import uuid
│       import logging
│       from google.protobuf.struct_pb2 import Struct
│       from rest_framework.exceptions import PermissionDenied, NotFound
│       
│       from memory.models import MemoryBucket
│       from memory.services import MemoryService
│       from . import memory_pb2, memory_pb2_grpc
│       
│       logging.basicConfig(level=logging.INFO, format='%(asctime)s - MS9-gRPC - %(levelname)s - %(message)s')
│       logger = logging.getLogger(__name__)
│       
│       class MemoryServicer(memory_pb2_grpc.MemoryServiceServicer):
│           def GetHistory(self, request, context):
│               logger.info(f"--- gRPC GetHistory Request Received ---")
│               logger.info(f"    Bucket ID: {request.bucket_id}")
│               logger.info(f"    User ID: {request.user_id}")
│       
│               try:
│                   # 1. Authorize the request
│                   bucket = MemoryBucket.objects.get(id=request.bucket_id)
│                   if str(bucket.owner_id) != request.user_id:
│                       raise PermissionDenied("User does not own this bucket.")
│                   
│                   # 2. Delegate to the service layer to get the processed data
│                   service = MemoryService()
│                   processed_data = service.get_processed_history(bucket)
│                   
│                   # 3. Convert the Python list of dicts into a list of Protobuf Structs
│                   history_as_structs = []
│                   if processed_data and processed_data.get("history"):
│                       for history_item in processed_data["history"]:
│                           proto_struct = Struct()
│                           proto_struct.update(history_item)
│                           history_as_structs.append(proto_struct)
│       
│                   logger.info(f"Success! Returning {len(history_as_structs)} history items for bucket {request.bucket_id}.")
│                   
│                   # 4. Return the final gRPC response using the CORRECTLY CONVERTED list
│                   return memory_pb2.GetHistoryResponse(
│                       bucket_id=processed_data['bucket_id'],
│                       memory_type=processed_data['memory_type'],
│                       history=history_as_structs # <-- This is the crucial line
│                   )
│                   
│               except MemoryBucket.DoesNotExist:
│                   logger.warning(f"NOT FOUND: Memory bucket '{request.bucket_id}' does not exist.")
│                   context.set_code(grpc.StatusCode.NOT_FOUND)
│                   context.set_details("Memory bucket not found.")
│                   return memory_pb2.GetHistoryResponse()
│               except PermissionDenied as e:
│                   logger.warning(f"PERMISSION DENIED for bucket '{request.bucket_id}': {e}")
│                   context.set_code(grpc.StatusCode.PERMISSION_DENIED)
│                   context.set_details(str(e))
│                   return memory_pb2.GetHistoryResponse()
│               except Exception as e:
│                   logger.error(f"INTERNAL ERROR during GetHistory for bucket '{request.bucket_id}'", exc_info=True)
│                   context.set_code(grpc.StatusCode.INTERNAL)
│                   context.set_details("An internal error occurred in the Memory Service.")
│                   return memory_pb2.GetHistoryResponse()
│   ]
├───messaging
│   __init__.py
│   [
│       
│   ]
│   apps.py
│   [
│       from django.apps import AppConfig
│       
│       
│       class MessagingConfig(AppConfig):
│           default_auto_field = 'django.db.models.BigAutoField'
│           name = 'messaging'
│       
│   ]
│   event_publisher.py
│   [
│       # MS9/messaging/event_publisher.py
│       from .rabbitmq_client import rabbitmq_client
│       
│       class MemoryEventPublisher:
│           def publish_bucket_deleted(self, bucket_id: str):
│               rabbitmq_client.publish(
│                   exchange_name='resource_events',
│                   routing_key='memory.bucket.deleted',
│                   body={"bucket_id": bucket_id}
│               )
│           
│           def publish_project_cleanup_confirmation(self, project_id: str):
│               rabbitmq_client.publish(
│                   exchange_name='project_events',
│                   routing_key='resource.for_project.deleted.MemoryService',
│                   body={"project_id": project_id, "service_name": "MemoryService"}
│               )
│       
│       memory_event_publisher = MemoryEventPublisher()
│   ]
│   ├───management
│   │   └───commands
│   │       run_context_update_worker.py
│   │       [
│   │           # MS9/messaging/management/commands/run_context_update_worker.py
│   │           
│   │           import pika
│   │           import json
│   │           import time
│   │           import logging
│   │           from django.core.management.base import BaseCommand
│   │           from django.conf import settings
│   │           from django.db import transaction, IntegrityError
│   │           from memory.models import Message, MemoryBucket
│   │           
│   │           # Configure logging for this worker
│   │           logging.basicConfig(level=logging.INFO, format='%(asctime)s - MS9-UpdateWorker - %(levelname)s - %(message)s')
│   │           logger = logging.getLogger(__name__)
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Listens for memory context updates from the Inference Executor.'
│   │           
│   │               def handle(self, *args, **options):
│   │                   rabbitmq_url = settings.RABBITMQ_URL
│   │                   self.stdout.write(self.style.SUCCESS("--- Memory Context Update Worker ---"))
│   │                   self.stdout.write(f"Connecting to RabbitMQ at {rabbitmq_url}...")
│   │                   
│   │                   while True:
│   │                       try:
│   │                           connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
│   │                           channel = connection.channel()
│   │           
│   │                           exchange_name = 'memory_exchange'
│   │                           self.stdout.write(f"Declaring exchange: '{exchange_name}' (type=topic, durable=True)")
│   │                           channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│   │                           
│   │                           queue_name = 'memory_context_update_queue'
│   │                           self.stdout.write(f"Declaring queue: '{queue_name}' (durable=True)")
│   │                           channel.queue_declare(queue=queue_name, durable=True)
│   │                           
│   │                           routing_key = 'memory.context.update'
│   │                           self.stdout.write(f"Binding queue '{queue_name}' to exchange '{exchange_name}' with key '{routing_key}'")
│   │                           channel.queue_bind(exchange=exchange_name, queue=queue_name, routing_key=routing_key)
│   │                           
│   │                           self.stdout.write(self.style.SUCCESS('\n [*] Worker is now waiting for memory update messages.'))
│   │                           channel.basic_consume(queue=queue_name, on_message_callback=self.callback)
│   │                           channel.start_consuming()
│   │           
│   │                       except pika.exceptions.AMQPConnectionError as e:
│   │                           self.stderr.write(self.style.ERROR(f'Connection to RabbitMQ failed: {e}. Retrying in 5 seconds...'))
│   │                           time.sleep(5)
│   │                       except KeyboardInterrupt:
│   │                           self.stdout.write(self.style.WARNING('\nWorker stopped by user.'))
│   │                           break
│   │           
│   │               def callback(self, ch, method, properties, body):
│   │                   logger.info(f"\n--- Context Update Event Received ---")
│   │                   try:
│   │                       payload = json.loads(body)
│   │                       idempotency_key = payload.get("idempotency_key")
│   │                       bucket_id = payload.get("memory_bucket_id")
│   │                       
│   │                       logger.info(f"    Job ID (Idempotency Key): {idempotency_key}")
│   │                       logger.info(f"    Target Bucket ID: {bucket_id}")
│   │           
│   │                       if not idempotency_key or not bucket_id:
│   │                           logger.error("Message missing key data. Discarding.")
│   │                           ch.basic_ack(delivery_tag=method.delivery_tag)
│   │                           return
│   │           
│   │                       try:
│   │                           with transaction.atomic():
│   │                               # The idempotency check remains the same.
│   │                               if Message.objects.filter(idempotency_key=idempotency_key).exists():
│   │                                   logger.warning(f"    DUPLICATE job '{idempotency_key}' detected in DB. Discarding message.")
│   │                                   ch.basic_ack(delivery_tag=method.delivery_tag)
│   │                                   logger.info(f"--- Event Processing Finished ---")
│   │                                   return
│   │           
│   │                               logger.info(f"    New job. Proceeding to update memory.")
│   │                               messages_to_add_raw = payload.get("messages_to_add", [])
│   │                               if not messages_to_add_raw:
│   │                                   logger.warning("    Payload contained no messages to add. Nothing to do.")
│   │                                   ch.basic_ack(delivery_tag=method.delivery_tag)
│   │                                   logger.info(f"--- Event Processing Finished ---")
│   │                                   return
│   │           
│   │                               bucket = MemoryBucket.objects.get(id=bucket_id)
│   │                               
│   │                               # --- THE DEFINITIVE FIX IS HERE ---
│   │                               messages_to_create = []
│   │                               for i, msg_data in enumerate(messages_to_add_raw):
│   │                                   # Apply the idempotency key ONLY to the first message in the batch.
│   │                                   # All subsequent messages for the same job will have a null key.
│   │                                   current_key = idempotency_key if i == 0 else None
│   │                                   
│   │                                   messages_to_create.append(
│   │                                       Message(
│   │                                           bucket=bucket,
│   │                                           content=msg_data,
│   │                                           idempotency_key=current_key
│   │                                       )
│   │                                   )
│   │                               # --- END OF FIX ---
│   │                                   
│   │                               Message.objects.bulk_create(messages_to_create)
│   │                               
│   │                               # This logic should be outside the loop
│   │                               bucket.message_count = bucket.messages.count()
│   │                               bucket.save(update_fields=['message_count', 'updated_at'])
│   │                               
│   │                               logger.info(f"    SUCCESS: Added {len(messages_to_create)} messages to bucket {bucket_id}.")
│   │           
│   │                       except MemoryBucket.DoesNotExist:
│   │                           logger.error(f"    FAILURE: Memory bucket '{bucket_id}' not found. Cannot save messages.")
│   │                       except IntegrityError:
│   │                           logger.warning(f"    DUPLICATE job '{idempotency_key}' detected by DB constraint during race condition. Discarding message.")
│   │                       
│   │                   except Exception as e:
│   │                       logger.error(f"An unexpected error occurred: {e}", exc_info=True)
│   │                   
│   │                   logger.info(f"--- Event Processing Finished ---")
│   │                   ch.basic_ack(delivery_tag=method.delivery_tag)
│   │       ]
│   │       run_project_cleanup_worker.py
│   │       [
│   │           # MS9/messaging/management/commands/run_project_cleanup_worker.py
│   │           import pika
│   │           import json
│   │           import time
│   │           from django.core.management.base import BaseCommand
│   │           from django.conf import settings
│   │           from memory.models import MemoryBucket
│   │           from messaging.event_publisher import memory_event_publisher
│   │           
│   │           class Command(BaseCommand):
│   │               help = 'Listens for project deletion events to clean up memory buckets.'
│   │           
│   │               def handle(self, *args, **options):
│   │                   rabbitmq_url = settings.RABBITMQ_URL
│   │                   while True:
│   │                       try:
│   │                           connection = pika.BlockingConnection(pika.URLParameters(rabbitmq_url))
│   │                           channel = connection.channel()
│   │                           channel.exchange_declare(exchange='project_events', exchange_type='topic', durable=True)
│   │                           queue_name = 'memory_project_cleanup_queue'
│   │                           channel.queue_declare(queue=queue_name, durable=True)
│   │                           channel.queue_bind(exchange='project_events', queue=queue_name, routing_key='project.deletion.initiated')
│   │                           
│   │                           self.stdout.write(self.style.SUCCESS(' [*] Memory project cleanup worker is waiting for messages.'))
│   │                           channel.basic_consume(queue=queue_name, on_message_callback=self.callback)
│   │                           channel.start_consuming()
│   │                       except pika.exceptions.AMQPConnectionError:
│   │                           self.stderr.write(self.style.ERROR('Connection to RabbitMQ failed. Retrying...'))
│   │                           time.sleep(5)
│   │                       except KeyboardInterrupt:
│   │                           self.stdout.write(self.style.WARNING('Worker stopped.'))
│   │                           break
│   │           
│   │               def callback(self, ch, method, properties, body):
│   │                   try:
│   │                       payload = json.loads(body)
│   │                       project_id = payload.get('project_id')
│   │                       if project_id:
│   │                           deleted_count, _ = MemoryBucket.objects.filter(project_id=project_id).delete()
│   │                           self.stdout.write(self.style.SUCCESS(f"Deleted {deleted_count} memory buckets for project {project_id}."))
│   │                           
│   │                           # Publish confirmation back to the project saga
│   │                           memory_event_publisher.publish_project_cleanup_confirmation(project_id)
│   │                   except Exception as e:
│   │                       self.stderr.write(self.style.ERROR(f"Error during project cleanup: {e}"))
│   │                       # In production, nack and DLQ
│   │                   
│   │                   ch.basic_ack(delivery_tag=method.delivery_tag)
│   │       ]
│   rabbitmq_client.py
│   [
│       # project service/messaging/rabbitmq_client.py
│       
│       import pika
│       import json
│       import time
│       from django.conf import settings
│       import threading
│       
│       class RabbitMQClient:
│           """
│           A thread-safe RabbitMQ client that ensures one connection per thread.
│           This prevents connection sharing issues between the main web server thread
│           and background worker threads.
│           """
│           _thread_local = threading.local()
│       
│           def _get_connection(self):
│               """Gets or creates a connection for the current thread."""
│               if not hasattr(self._thread_local, 'connection') or self._thread_local.connection.is_closed:
│                   print(f"Thread {threading.get_ident()}: No active connection. Connecting to RabbitMQ...")
│                   try:
│                       params = pika.URLParameters(settings.RABBITMQ_URL) # In prod, use settings.RABBITMQ_URL
│                       self._thread_local.connection = pika.BlockingConnection(params)
│                       print(f"Thread {threading.get_ident()}: Connection successful.")
│                   except pika.exceptions.AMQPConnectionError as e:
│                       print(f"CRITICAL: Failed to connect to RabbitMQ: {e}")
│                       # In a real app, this should raise an exception or have a retry mechanism.
│                       raise
│               return self._thread_local.connection
│       
│           def publish(self, exchange_name, routing_key, body):
│               """Publishes a message, ensuring a valid connection and channel."""
│               try:
│                   connection = self._get_connection()
│                   channel = connection.channel()
│                   
│                   # Declare exchanges to ensure they exist. This is idempotent.
│                   channel.exchange_declare(exchange=exchange_name, exchange_type='topic', durable=True)
│       
│                   message_body = json.dumps(body, default=str)
│                   
│                   channel.basic_publish(
│                       exchange=exchange_name,
│                       routing_key=routing_key,
│                       body=message_body,
│                       properties=pika.BasicProperties(
│                           content_type='application/json',
│                           delivery_mode=pika.DeliveryMode.Persistent,
│                       )
│                   )
│                   print(f" [x] Sent '{routing_key}':'{message_body}'")
│               except pika.exceptions.AMQPError as e:
│                   print(f"Error publishing message: {e}. Connection may be closed. It will be reopened on next call.")
│                   # Invalidate the connection so it's recreated on the next call
│                   if hasattr(self._thread_local, 'connection'):
│                       self._thread_local.connection.close()
│                   raise # Re-raise the exception so the caller knows the publish failed
│       
│       # Create a single, globally accessible instance.
│       # The instance itself is shared, but the connection it manages is thread-local.
│       rabbitmq_client = RabbitMQClient()
│   ]
│   project meta gen.py
│   [
│       import os
│       import mimetypes
│       import glob
│       import re
│       
│       def get_next_sequence_number():
│           """Find the next available sequence number for the output file."""
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           pattern = os.path.join(script_dir, "project_structure_*.txt")
│           existing_files = glob.glob(pattern)
│           
│           if not existing_files:
│               return 1
│           
│           # Extract sequence numbers from existing files
│           sequence_numbers = []
│           for file_path in existing_files:
│               basename = os.path.basename(file_path)
│               match = re.search(r'project_structure_(\d+)\.txt', basename)
│               if match:
│                   sequence_numbers.append(int(match.group(1)))
│           
│           if not sequence_numbers:
│               return 1
│           
│           # Return the next number in sequence
│           return max(sequence_numbers) + 1
│       
│       def generate_project_structure():
│           """Generate a text file containing the project structure with file contents."""
│           # Get the absolute path of the script's directory
│           script_dir = os.path.abspath(os.path.dirname(__file__))
│           # Change to that directory to ensure we're working only there
│           os.chdir(script_dir)
│           
│           # Generate a unique filename with sequence number
│           seq_num = get_next_sequence_number()
│           output_file = os.path.join(script_dir, f"project_structure_{seq_num}.txt")
│           
│           with open(output_file, 'w', encoding='utf-8', errors='replace') as f:
│               # Get items in the script directory only, excluding specified patterns
│               items = get_directory_items(script_dir, output_file)
│               
│               # Process each item at root level
│               for i, item in enumerate(items):
│                   is_last = i == len(items) - 1
│                   
│                   if os.path.isdir(os.path.join(script_dir, item)):
│                       # It's a directory
│                       if is_last:
│                           f.write(f"└───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "    ", output_file, script_dir)
│                       else:
│                           f.write(f"├───{item}\n")
│                           process_directory(os.path.join(script_dir, item), f, "│   ", output_file, script_dir)
│                   else:
│                       # It's a file - at root level, format as in the example
│                       f.write(f"│   {item}\n")
│                       # Include file content
│                       content = read_file_content(os.path.join(script_dir, item))
│                       f.write(f"│   [\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           f.write(f"│       {line}\n")
│                       f.write(f"│   ]\n")
│           
│           print(f"Project structure has been written to {output_file}")
│       
│       def should_exclude(item_path):
│           """Check if an item should be excluded based on patterns."""
│           # Exclude __pycache__ directories
│           if os.path.isdir(item_path) and "__pycache__" in item_path:
│               return True
│           
│           # Exclude migrations directories
│           if os.path.isdir(item_path) and "migrations" in item_path:
│               return True
│           
│           # Exclude .pyc files
│           if item_path.endswith('.pyc'):
│               return True
│           
│           # Exclude all project_structure files
│           if os.path.basename(item_path).startswith("project_structure_") and item_path.endswith(".txt"):
│               return True
│           
│           return False
│       
│       def get_directory_items(dir_path, output_file):
│           """Get sorted list of items in a directory, excluding the output file and specified patterns."""
│           # Get absolute path to output file to exclude it
│           abs_output_path = os.path.abspath(output_file)
│           
│           try:
│               # List directory contents
│               items = sorted(os.listdir(dir_path))
│               
│               # Filter out the output file itself and items matching exclude patterns
│               filtered_items = []
│               for item in items:
│                   item_path = os.path.join(dir_path, item)
│                   
│                   # Skip the output file
│                   if os.path.abspath(item_path) == abs_output_path:
│                       continue
│                       
│                   # Skip symlinks that might point outside
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   # Skip items matching exclude patterns
│                   if should_exclude(item_path):
│                       continue
│                       
│                   filtered_items.append(item)
│               
│               return filtered_items
│           except Exception as e:
│               print(f"Error listing directory {dir_path}: {e}")
│               return []
│       
│       def is_binary_file(file_path):
│           """Determine if a file is binary or text."""
│           # Initialize mimetypes
│           if not mimetypes.inited:
│               mimetypes.init()
│           
│           # Check by mime type first
│           mime_type, _ = mimetypes.guess_type(file_path)
│           if mime_type and not mime_type.startswith(('text/', 'application/json', 'application/xml', 'application/javascript')):
│               return True
│               
│           # Fallback: check for null bytes
│           try:
│               with open(file_path, 'rb') as f:
│                   chunk = f.read(4096)
│                   return b'\0' in chunk
│           except Exception:
│               return True  # If we can't read it, assume binary
│       
│       def read_file_content(file_path, max_length=500000):
│           """Read content from a file, handling binary files and errors."""
│           try:
│               # Check if binary
│               if is_binary_file(file_path):
│                   return "[Binary file - content not shown]"
│                   
│               # Read text file
│               with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
│                   content = f.read(max_length + 1)
│                   
│               # Handle truncation
│               if len(content) > max_length:
│                   content = content[:max_length] + "... [truncated]"
│                   
│               # Return raw content without escaping special characters
│               return content
│           except Exception as e:
│               return f"[Error reading file: {str(e)}]"
│       
│       def process_directory(dir_path, file_obj, indent, output_file, script_dir):
│           """Recursively process a directory and write its structure to the file."""
│           # Safety check - ensure we're still within the script directory
│           rel_path = os.path.relpath(dir_path, script_dir)
│           if rel_path.startswith('..') or rel_path == '.':
│               return  # Don't process if it's outside our script directory
│           
│           try:
│               # List directory contents
│               items = get_directory_items(dir_path, output_file)
│               
│               # Process each item
│               for i, item in enumerate(items):
│                   item_path = os.path.join(dir_path, item)
│                   is_last = i == len(items) - 1
│                   
│                   # Safety check - don't follow symlinks or items outside our script directory
│                   if os.path.islink(item_path):
│                       continue
│                       
│                   rel_path = os.path.relpath(item_path, script_dir)
│                   if rel_path.startswith('..'):
│                       continue
│                   
│                   if os.path.isdir(item_path):
│                       # It's a directory
│                       if is_last:
│                           file_obj.write(f"{indent}└───{item}\n")
│                           process_directory(item_path, file_obj, indent + "    ", output_file, script_dir)
│                       else:
│                           file_obj.write(f"{indent}├───{item}\n")
│                           process_directory(item_path, file_obj, indent + "│   ", output_file, script_dir)
│                   else:
│                       # It's a file
│                       file_obj.write(f"{indent}{item}\n")
│                       # Include file content
│                       content = read_file_content(item_path)
│                       file_obj.write(f"{indent}[\n")
│                       content_lines = content.split('\n')
│                       for line in content_lines:
│                           file_obj.write(f"{indent}    {line}\n")
│                       file_obj.write(f"{indent}]\n")
│           except PermissionError:
│               file_obj.write(f"{indent}[Permission denied]\n")
│           except Exception as e:
│               file_obj.write(f"{indent}[Error: {str(e)}]\n")
│       
│       if __name__ == "__main__":
│           generate_project_structure()
│   ]
│   requirements.txt
│   [
│       asgiref==3.9.1
│       certifi==2025.8.3
│       cffi==1.17.1
│       charset-normalizer==3.4.3
│       cryptography==45.0.6
│       defusedxml==0.7.1
│       Django==5.2.5
│       djangorestframework==3.16.1
│       djangorestframework_simplejwt==5.5.1
│       idna==3.10
│       oauthlib==3.3.1
│       pycparser==2.22
│       PyJWT==2.10.1
│       python3-openid==3.2.0
│       requests==2.32.5
│       requests-oauthlib==2.0.0
│       social-auth-app-django==5.5.1
│       social-auth-core==4.7.0
│       sqlparse==0.5.3
│       tzdata==2025.2
│       urllib3==2.5.0
│       Django
│       djangorestframework
│       djangorestframework-simplejwt
│       python-dotenv
│       pika
│       psycopg2-binary # Recommended for production, works with SQLite too
│       gunicorn
│       
│       # gRPC
│       grpcio
│       grpcio-tools
│       protobuf
│       google-api-python-client
│       
│       # LangChain and AI
│       langchain
│       langchain-openai # Needed for ConversationSummaryMemory
│       tiktoken # For token counting
│       
│       # Other
│       httpx
│       redis # For idempotency checks
│   ]
